{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "HPO-RayTune-heston.ipynb",
   "provenance": [
    {
     "file_id": "1kmsV85Gt5eOXATkYBL_ao-haR7l48J15",
     "timestamp": 1649343856016
    }
   ],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyP72AX+IvTN1ckSp5a/jiJ5"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameters Optimization with ASHA\n",
    "\n",
    "- Dataset used synthetic heston least square montecarlo options\n"
   ],
   "metadata": {
    "id": "pbyC8ZtoVB0v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U ray[tune]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrP2nHqZY3py",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649348432520,
     "user_tz": -120,
     "elapsed": 26221,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     }
    },
    "outputId": "0a9323fe-8771-4e17-95fc-f70743e026cc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting ray[tune]\n",
      "  Downloading ray-1.11.0-cp37-cp37m-manylinux2014_x86_64.whl (52.7 MB)\n",
      "\u001B[K     |████████████████████████████████| 52.7 MB 120 kB/s \n",
      "\u001B[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (7.1.2)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (4.3.3)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.21.5)\n",
      "Collecting grpcio<=1.43.0,>=1.28.1\n",
      "  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001B[K     |████████████████████████████████| 4.1 MB 32.7 MB/s \n",
      "\u001B[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (21.4.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.13)\n",
      "Collecting redis>=3.5.0\n",
      "  Downloading redis-4.2.2-py3-none-any.whl (226 kB)\n",
      "\u001B[K     |████████████████████████████████| 226 kB 38.2 MB/s \n",
      "\u001B[?25hRequirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.17.3)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.8.9)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (2.23.0)\n",
      "Collecting tensorboardX>=1.9\n",
      "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
      "\u001B[K     |████████████████████████████████| 125 kB 38.7 MB/s \n",
      "\u001B[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.3.5)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray[tune]) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (4.11.3)\n",
      "Collecting async-timeout>=4.0.2\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (3.10.0.2)\n",
      "Collecting deprecated>=1.2.3\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (21.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray[tune]) (1.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray[tune]) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.4->redis>=3.5.0->ray[tune]) (3.0.7)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (0.18.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2021.10.8)\n",
      "Installing collected packages: deprecated, async-timeout, redis, grpcio, tensorboardX, ray\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.44.0\n",
      "    Uninstalling grpcio-1.44.0:\n",
      "      Successfully uninstalled grpcio-1.44.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001B[0m\n",
      "Successfully installed async-timeout-4.0.2 deprecated-1.2.13 grpcio-1.43.0 ray-1.11.0 redis-4.2.2 tensorboardX-2.5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title **Imports**\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ],
   "metadata": {
    "id": "LYboMxKfYqnx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "synthetic_calls_path = '../data/heston_mc_synthetic_calls_tot.csv'\n",
    "synthetic_puts_path = '../data/heston_mc_synthetic_puts_tot.csv'\n",
    "checkpoint_dir = '../models/opt_checkpoints'"
   ],
   "metadata": {
    "id": "WtpL-EgSdtZz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df"
   ],
   "metadata": {
    "id": "lBqrK3kAd1T2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class OptDataset(Dataset):\n",
    "\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X[idx], self.y[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)"
   ],
   "metadata": {
    "id": "4_wf988bH-Ax"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocessing(df:pd.DataFrame):\n",
    "  df = pd.get_dummies(df, prefix='', prefix_sep='')\n",
    "  input_sc = StandardScaler()\n",
    "  output_sc = StandardScaler()\n",
    "  input_data = input_sc.fit_transform(df.drop('Option Price', axis=1))\n",
    "  output_data = output_sc.fit_transform(df['Option Price'].values.reshape(-1, 1))\n",
    "\n",
    "  return input_data, output_data"
   ],
   "metadata": {
    "id": "7atNF6Bte6ie"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_data():\n",
    "  synthetic_calls = pd.read_csv(synthetic_calls_path, index_col=0)\n",
    "  synthetic_puts = pd.read_csv(synthetic_puts_path, index_col=0)\n",
    "\n",
    "  synthetic_calls = reduce_mem_usage(synthetic_calls)\n",
    "  synthetic_puts = reduce_mem_usage(synthetic_puts)\n",
    "  \n",
    "  synthetic_options = pd.concat([synthetic_calls, synthetic_puts], ignore_index=True)\n",
    "  synthetic_options = shuffle(synthetic_options, random_state=0)\n",
    "  \n",
    "  input_data, output_data = preprocessing(synthetic_options)\n",
    "\n",
    "  train_size = 0.8\n",
    "  val_size = 0.1\n",
    "\n",
    "  last_train_idx = int(np.round(len(input_data) * train_size))\n",
    "  last_val_idx = last_train_idx + int(np.round(len(input_data) * val_size))\n",
    "\n",
    "  X_train = Variable(torch.Tensor(input_data[0:last_train_idx]))\n",
    "  X_val = Variable(torch.Tensor(input_data[last_train_idx:last_val_idx]))\n",
    "  X_test = Variable(torch.Tensor(input_data[last_val_idx:]))\n",
    "\n",
    "  y_train = Variable(torch.Tensor(output_data[0:last_train_idx]))\n",
    "  y_val = Variable(torch.Tensor(output_data[last_train_idx:last_val_idx]))\n",
    "  y_test = Variable(torch.Tensor(output_data[last_val_idx:]))\n",
    "\n",
    "  return OptDataset(X_train, y_train), OptDataset(X_val, y_val), OptDataset(X_test, y_test)"
   ],
   "metadata": {
    "id": "_zVo4hXFc_V3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configurable Model\n",
    "\n",
    "FCN model with residual blocks, with the following configurable parameters:\n",
    "\n",
    "- Number of layers\n",
    "- Activation function: $[ReLU, LeakyReLU, ELU]$\n",
    "- Hidden layer size"
   ],
   "metadata": {
    "id": "26eqJpyzE8Kp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "device = 'cuda:0' if CUDA else 'cpu'"
   ],
   "metadata": {
    "id": "GlucHMsYZdNO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, module):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.module = module\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.module(x) + x"
   ],
   "metadata": {
    "id": "Z-Gpgg6PZbHj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class HiddenLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, layer_size, act_fn):\n",
    "      super(HiddenLayer, self).__init__()\n",
    "      \n",
    "      if act_fn == 'ReLU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.ReLU())\n",
    "      elif act_fn == 'LeakyReLU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.LeakyReLU())\n",
    "      elif act_fn == 'ELU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.ELU())\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.layer(x)"
   ],
   "metadata": {
    "id": "Jl5SWFIzZW73"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, output_size, hidden_size, num_layers, act_fn):\n",
    "    super(Net, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.output_size = output_size\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if act_fn == 'ReLU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ReLU())\n",
    "    elif act_fn == 'LeakyReLU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.LeakyReLU())\n",
    "    elif act_fn == 'ELU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ELU())\n",
    "\n",
    "    self.hidden_layers_list = []\n",
    "\n",
    "    for i in range(num_layers // 2):\n",
    "      self.hidden_layers_list.append(\n",
    "          ResBlock(\n",
    "            nn.Sequential(\n",
    "                HiddenLayer(self.hidden_size, act_fn),\n",
    "                HiddenLayer(self.hidden_size, act_fn)\n",
    "            )\n",
    "        )\n",
    "      )\n",
    "\n",
    "    self.hidden_layers = nn.Sequential(*self.hidden_layers_list)\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        self.initial_layer,\n",
    "        self.hidden_layers,\n",
    "        nn.Linear(self.hidden_size, self.output_size)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.net(x)"
   ],
   "metadata": {
    "id": "17o15dfwYuJJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def init_xuniform(m):\n",
    "  if isinstance(m, nn.Linear):\n",
    "    torch.nn.init.xavier_uniform_(m.weight)\n",
    "    m.bias.data.fill_(0.01)"
   ],
   "metadata": {
    "id": "DjbcKxECdWmL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_size = 11\n",
    "output_size = 1\n",
    "number_of_samples = 20"
   ],
   "metadata": {
    "id": "vdlcCMa4bWey"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_cifar(config, checkpoint_dir=None):\n",
    "    net = Net(input_size, output_size, config['hidden_size'], config['num_layers'], config['act_fn'])\n",
    "    net = net.to(device)\n",
    "    net.apply(init_xuniform)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    trainset, valset, testset = load_data()\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        valset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "\n",
    "    for epoch in range(25):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 50 == 49:  # print every 50 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps))\n",
    "    print(\"Finished Training\")"
   ],
   "metadata": {
    "id": "RYWtIqSlZmFn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def test_accuracy(net, loss_fn):\n",
    "    trainset, valset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            X, y = data\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            out = net(X)\n",
    "            loss = loss_fn(out, y.to(device))\n",
    "            losses.append(loss.cpu().item())\n",
    "\n",
    "    return np.array(losses).mean()"
   ],
   "metadata": {
    "id": "GhmuCc7gJjd3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apart from the networks actitectural parameters, we would like to find the optimal **learning rate** and **batch size**. "
   ],
   "metadata": {
    "id": "lva0gjNhVzkQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    \"hidden_size\": tune.choice([400, 600, 800]),\n",
    "    \"num_layers\": tune.choice([4, 6, 8]),\n",
    "    \"act_fn\": tune.choice([\"ReLU\", \"LeakyReLU\", \"ELU\"]),\n",
    "    \"lr\": tune.loguniform(5e-5, 1e-1),\n",
    "    \"batch_size\": tune.choice([512, 1024, 2048])\n",
    "}"
   ],
   "metadata": {
    "id": "ypLLRjrGLLBB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=30,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)"
   ],
   "metadata": {
    "id": "Mq9OFACTL6dD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])"
   ],
   "metadata": {
    "id": "00WpLw19M2rm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "result = tune.run(\n",
    "        partial(train_cifar, checkpoint_dir=checkpoint_dir),\n",
    "        resources_per_trial={\"cpu\": 2, \"gpu\": 1},\n",
    "        config=config,\n",
    "        num_samples=number_of_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "19aCOuzb9aVvCd1jxvzXokk9ZOgK-d9SV"
    },
    "id": "70MNOcvoM8-F",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649351220834,
     "user_tz": -120,
     "elapsed": 2728139,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     }
    },
    "outputId": "7e368c34-01e2-4ff9-9593-1cff66b40ba7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Output hidden; open in https://colab.research.google.com to view."
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "res_df = result.results_df\n",
    "res_df"
   ],
   "metadata": {
    "id": "3dnnyDPrXfrK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "res_df.to_csv('../results/asha_heston_results.csv')"
   ],
   "metadata": {
    "id": "ABIaNuKLXsD3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qmHIrt2PNy7d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649351220837,
     "user_tz": -120,
     "elapsed": 154,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     }
    },
    "outputId": "b75aa78e-8313-4403-ffc2-5fb76eb8d569"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best trial config: {'hidden_size': 600, 'num_layers': 6, 'act_fn': 'ReLU', 'lr': 0.001213210423986924, 'batch_size': 2048}\n",
      "Best trial final validation loss: 0.004382969217840582\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "best_trained_model = Net(\n",
    "    input_size, \n",
    "    output_size, \n",
    "    best_trial.config['hidden_size'], \n",
    "    best_trial.config['num_layers'], \n",
    "    best_trial.config['act_fn'])"
   ],
   "metadata": {
    "id": "FFn3JWeDN8bl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "best_trained_model = best_trained_model.to(device)\n",
    "best_checkpoint_dir = best_trial.checkpoint.value\n",
    "model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "test_acc = test_accuracy(best_trained_model, loss_fn)\n",
    "print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PrrtG1q5OMIY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649351291595,
     "user_tz": -120,
     "elapsed": 70776,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     }
    },
    "outputId": "378c65fc-5230-4b1b-d0e2-1c80b3712c8d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best trial test set accuracy: 0.004320884846653493\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [Pytorch Tutorial](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)\n",
    "- [ASHA Paper](https://openreview.net/pdf?id=S1MAriC5F7)"
   ],
   "metadata": {
    "id": "uuJtRkISEV8Z"
   }
  }
 ]
}