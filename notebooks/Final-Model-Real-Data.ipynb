{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ju-WUaXnY5cr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Models Testing on Heston data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1650891681415,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "hH3-v8y-AuXg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650891681417,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "QXEzFQ3iAyj_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891681418,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "gsH02HCWA0gC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "options_path = '../data/real_options_tot.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1650891681421,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "jT4n95T6A4S6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3413,
     "status": "ok",
     "timestamp": 1650891684820,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "7H8lEQPCA5IS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "options_df = pd.read_csv(options_path, index_col=0)\n",
    "options_df = reduce_mem_usage(options_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1650891684821,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "9TKRgvuOA8pC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "options_df = shuffle(options_df, random_state=0)\n",
    "options_df = options_df.reset_index()\n",
    "options_df['r'] = options_df['r'] / 100\n",
    "options_df = options_df.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1650891684822,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "3HXliAsq132p",
    "outputId": "09bc2eb1-d20d-4471-ed50-5fa0eba727fb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             contractSymbol        lastTradeDate  strike   lastPrice  \\\n0       AMZN220916P03900000  2022-04-21 15:42:43  3900.0  871.000000   \n1       MSFT230317P00395000  2022-04-29 13:43:21   395.0  108.875000   \n2         FB220916P00335000  2022-04-27 13:30:14   335.0  159.875000   \n3       AAPL220527C00165000  2022-04-25 19:59:50   165.0    6.148438   \n4       TSLA240621P00350000  2022-04-01 19:12:48   350.0   28.500000   \n...                     ...                  ...     ...         ...   \n374723  TSLA220819C00650000  2022-04-19 16:10:28   650.0  398.250000   \n374724  TSLA220513C01070000  2022-04-19 19:44:18  1070.0   39.000000   \n374725  TSLA220520P00580000  2022-05-04 19:53:41   580.0    0.830078   \n374726  MSFT220916C00185000  2022-05-02 17:52:36   185.0   96.625000   \n374727  TSLA220520C00695000  2022-03-25 14:16:05   695.0  319.750000   \n\n                bid          ask     change  percentChange  volume  \\\n0       1118.000000  1127.000000   0.000000       0.000000     1.0   \n1        133.875000   137.000000   0.000000       0.000000     1.0   \n2        111.812500   112.437500   0.000000       0.000000    10.0   \n3          5.949219     6.199219   0.399902       6.957031   708.0   \n4         27.500000    36.500000   0.000000       0.000000     1.0   \n...             ...          ...        ...            ...     ...   \n374723   393.000000   396.500000  33.812500       9.273438     1.0   \n374724    37.843750    42.187500   2.439453       6.671875    83.0   \n374725     0.660156     0.859863  -1.080078     -56.531250   271.0   \n374726    97.375000   100.875000   0.000000       0.000000     2.0   \n374727     0.000000     0.000000   0.000000       0.000000     1.0   \n\n        openInterest  ...  contractSize  currency  type  expiryDate  \\\n0              127.0  ...       REGULAR       USD   put  2022-09-16   \n1              126.0  ...       REGULAR       USD   put  2023-03-17   \n2             1896.0  ...       REGULAR       USD   put  2022-09-16   \n3             1466.0  ...       REGULAR       USD  call  2022-05-27   \n4                3.0  ...       REGULAR       USD   put  2024-06-21   \n...              ...  ...           ...       ...   ...         ...   \n374723          54.0  ...       REGULAR       USD  call  2022-08-19   \n374724          22.0  ...       REGULAR       USD  call  2022-05-13   \n374725         590.0  ...       REGULAR       USD   put  2022-05-20   \n374726        2600.0  ...       REGULAR       USD  call  2022-09-16   \n374727           0.0  ...       REGULAR       USD  call  2022-05-20   \n\n       downloadDate     close     hv_21  moneyness       tau         r  \n0        2022-04-26  2788.000  0.380371   1.398438  0.392822  0.008331  \n1        2022-05-11   260.500  0.438721   1.516602  0.851562  0.009628  \n2        2022-05-04   223.375  0.789551   1.500000  0.370850  0.008331  \n3        2022-04-25   162.875  0.253662   0.987305  0.087891  0.008430  \n4        2022-04-04  1145.000  0.581055   0.305664  2.222656  0.006832  \n...             ...       ...       ...        ...       ...       ...  \n374723   2022-04-19  1028.000  0.574219   1.581055  0.335205  0.008202  \n374724   2022-04-19  1028.000  0.574219   0.960938  0.065918  0.008202  \n374725   2022-05-04   952.500  0.653809   0.608887  0.043945  0.008331  \n374726   2022-05-03   281.750  0.407227   1.523438  0.373535  0.008812  \n374727   2022-03-31  1078.000  0.601562   1.550781  0.137329  0.005249  \n\n[374728 rows x 22 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contractSymbol</th>\n      <th>lastTradeDate</th>\n      <th>strike</th>\n      <th>lastPrice</th>\n      <th>bid</th>\n      <th>ask</th>\n      <th>change</th>\n      <th>percentChange</th>\n      <th>volume</th>\n      <th>openInterest</th>\n      <th>...</th>\n      <th>contractSize</th>\n      <th>currency</th>\n      <th>type</th>\n      <th>expiryDate</th>\n      <th>downloadDate</th>\n      <th>close</th>\n      <th>hv_21</th>\n      <th>moneyness</th>\n      <th>tau</th>\n      <th>r</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AMZN220916P03900000</td>\n      <td>2022-04-21 15:42:43</td>\n      <td>3900.0</td>\n      <td>871.000000</td>\n      <td>1118.000000</td>\n      <td>1127.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>127.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>put</td>\n      <td>2022-09-16</td>\n      <td>2022-04-26</td>\n      <td>2788.000</td>\n      <td>0.380371</td>\n      <td>1.398438</td>\n      <td>0.392822</td>\n      <td>0.008331</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MSFT230317P00395000</td>\n      <td>2022-04-29 13:43:21</td>\n      <td>395.0</td>\n      <td>108.875000</td>\n      <td>133.875000</td>\n      <td>137.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>126.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>put</td>\n      <td>2023-03-17</td>\n      <td>2022-05-11</td>\n      <td>260.500</td>\n      <td>0.438721</td>\n      <td>1.516602</td>\n      <td>0.851562</td>\n      <td>0.009628</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>FB220916P00335000</td>\n      <td>2022-04-27 13:30:14</td>\n      <td>335.0</td>\n      <td>159.875000</td>\n      <td>111.812500</td>\n      <td>112.437500</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>10.0</td>\n      <td>1896.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>put</td>\n      <td>2022-09-16</td>\n      <td>2022-05-04</td>\n      <td>223.375</td>\n      <td>0.789551</td>\n      <td>1.500000</td>\n      <td>0.370850</td>\n      <td>0.008331</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AAPL220527C00165000</td>\n      <td>2022-04-25 19:59:50</td>\n      <td>165.0</td>\n      <td>6.148438</td>\n      <td>5.949219</td>\n      <td>6.199219</td>\n      <td>0.399902</td>\n      <td>6.957031</td>\n      <td>708.0</td>\n      <td>1466.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>call</td>\n      <td>2022-05-27</td>\n      <td>2022-04-25</td>\n      <td>162.875</td>\n      <td>0.253662</td>\n      <td>0.987305</td>\n      <td>0.087891</td>\n      <td>0.008430</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TSLA240621P00350000</td>\n      <td>2022-04-01 19:12:48</td>\n      <td>350.0</td>\n      <td>28.500000</td>\n      <td>27.500000</td>\n      <td>36.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>put</td>\n      <td>2024-06-21</td>\n      <td>2022-04-04</td>\n      <td>1145.000</td>\n      <td>0.581055</td>\n      <td>0.305664</td>\n      <td>2.222656</td>\n      <td>0.006832</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>374723</th>\n      <td>TSLA220819C00650000</td>\n      <td>2022-04-19 16:10:28</td>\n      <td>650.0</td>\n      <td>398.250000</td>\n      <td>393.000000</td>\n      <td>396.500000</td>\n      <td>33.812500</td>\n      <td>9.273438</td>\n      <td>1.0</td>\n      <td>54.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>call</td>\n      <td>2022-08-19</td>\n      <td>2022-04-19</td>\n      <td>1028.000</td>\n      <td>0.574219</td>\n      <td>1.581055</td>\n      <td>0.335205</td>\n      <td>0.008202</td>\n    </tr>\n    <tr>\n      <th>374724</th>\n      <td>TSLA220513C01070000</td>\n      <td>2022-04-19 19:44:18</td>\n      <td>1070.0</td>\n      <td>39.000000</td>\n      <td>37.843750</td>\n      <td>42.187500</td>\n      <td>2.439453</td>\n      <td>6.671875</td>\n      <td>83.0</td>\n      <td>22.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>call</td>\n      <td>2022-05-13</td>\n      <td>2022-04-19</td>\n      <td>1028.000</td>\n      <td>0.574219</td>\n      <td>0.960938</td>\n      <td>0.065918</td>\n      <td>0.008202</td>\n    </tr>\n    <tr>\n      <th>374725</th>\n      <td>TSLA220520P00580000</td>\n      <td>2022-05-04 19:53:41</td>\n      <td>580.0</td>\n      <td>0.830078</td>\n      <td>0.660156</td>\n      <td>0.859863</td>\n      <td>-1.080078</td>\n      <td>-56.531250</td>\n      <td>271.0</td>\n      <td>590.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>put</td>\n      <td>2022-05-20</td>\n      <td>2022-05-04</td>\n      <td>952.500</td>\n      <td>0.653809</td>\n      <td>0.608887</td>\n      <td>0.043945</td>\n      <td>0.008331</td>\n    </tr>\n    <tr>\n      <th>374726</th>\n      <td>MSFT220916C00185000</td>\n      <td>2022-05-02 17:52:36</td>\n      <td>185.0</td>\n      <td>96.625000</td>\n      <td>97.375000</td>\n      <td>100.875000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.0</td>\n      <td>2600.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>call</td>\n      <td>2022-09-16</td>\n      <td>2022-05-03</td>\n      <td>281.750</td>\n      <td>0.407227</td>\n      <td>1.523438</td>\n      <td>0.373535</td>\n      <td>0.008812</td>\n    </tr>\n    <tr>\n      <th>374727</th>\n      <td>TSLA220520C00695000</td>\n      <td>2022-03-25 14:16:05</td>\n      <td>695.0</td>\n      <td>319.750000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>call</td>\n      <td>2022-05-20</td>\n      <td>2022-03-31</td>\n      <td>1078.000</td>\n      <td>0.601562</td>\n      <td>1.550781</td>\n      <td>0.137329</td>\n      <td>0.005249</td>\n    </tr>\n  </tbody>\n</table>\n<p>374728 rows × 22 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pn28_RUMBAFH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1650891685322,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "32oPe6XUBCTF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop = ['impliedVolatility',\n",
    "                  'inTheMoney',\n",
    "                  'change',\n",
    "                  'percentChange',\n",
    "                  'bid',\n",
    "                  'ask',\n",
    "                  'volume',\n",
    "                  'openInterest',\n",
    "                  'contractSymbol',\n",
    "                  'lastTradeDate',\n",
    "                  'contractSize',\n",
    "                  'currency',\n",
    "                  'expiryDate',\n",
    "                  'downloadDate']\n",
    "options_df = options_df.drop(cols_to_drop, axis=1)\n",
    "options_df = pd.get_dummies(options_df, prefix='', prefix_sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1650891686346,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "_Tlc7k7xBDPV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_sc = StandardScaler()\n",
    "output_sc = StandardScaler()\n",
    "input_data = input_sc.fit_transform(options_df.drop(['lastPrice'], axis=1))\n",
    "output_data = output_sc.fit_transform(options_df['lastPrice'].values.reshape(-1, 1))\n",
    "\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "\n",
    "last_train_idx = int(np.round(len(input_data) * train_size))\n",
    "last_val_idx = last_train_idx + int(np.round(len(input_data) * val_size))\n",
    "\n",
    "X_train = input_data[0:last_train_idx]\n",
    "X_val = input_data[last_train_idx:last_val_idx]\n",
    "X_test = input_data[last_val_idx:]\n",
    "\n",
    "y_train = output_data[0:last_train_idx]\n",
    "y_val = output_data[last_train_idx:last_val_idx]\n",
    "y_test = output_data[last_val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650891686347,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "7nEEoQGvvDpL",
    "outputId": "88b4d863-d037-439b-e625-5ebb52ad41ef",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['strike', 'lastPrice', 'close', 'hv_21', 'moneyness', 'tau', 'r',\n       'call', 'put'],\n      dtype='object')"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cols = options_df.columns\n",
    "df_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1650891686349,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "IwMVtvfABIhR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = Variable(torch.Tensor(X_train))\n",
    "X_val = Variable(torch.Tensor(X_val))\n",
    "X_test = Variable(torch.Tensor(X_test))\n",
    "\n",
    "y_train = Variable(torch.Tensor(y_train))\n",
    "y_val = Variable(torch.Tensor(y_val))\n",
    "y_test = Variable(torch.Tensor(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ra2l5P1nBVCz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891686350,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "xTLMLFoSBWDy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "device = 'cuda:0' if CUDA else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1650891686352,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "CQ9Gl3bUBWsj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, module):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.module = module\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.module(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891686353,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "YL3SicQPBYq0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, layer_size, act_fn):\n",
    "      super(HiddenLayer, self).__init__()\n",
    "      \n",
    "      if act_fn == 'ReLU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.ReLU())\n",
    "      elif act_fn == 'LeakyReLU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.LeakyReLU())\n",
    "      elif act_fn == 'ELU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.ELU())\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1650891686792,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "lHUFGf9xBawS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, output_size, hidden_size, num_layers, act_fn):\n",
    "    super(Net, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.output_size = output_size\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if act_fn == 'ReLU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ReLU())\n",
    "    elif act_fn == 'LeakyReLU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.LeakyReLU())\n",
    "    elif act_fn == 'ELU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ELU())\n",
    "\n",
    "    self.hidden_layers_list = []\n",
    "\n",
    "    for i in range(num_layers // 2):\n",
    "      self.hidden_layers_list.append(\n",
    "          ResBlock(\n",
    "            nn.Sequential(\n",
    "                HiddenLayer(self.hidden_size, act_fn),\n",
    "                HiddenLayer(self.hidden_size, act_fn)\n",
    "            )\n",
    "        )\n",
    "      )\n",
    "\n",
    "    self.hidden_layers = nn.Sequential(*self.hidden_layers_list)\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        self.initial_layer,\n",
    "        self.hidden_layers,\n",
    "        nn.Linear(self.hidden_size, self.output_size)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650891686793,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "zcq_lQrHBdH8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(m, init_m: str):\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_uniform(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.uniform_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_normal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.normal_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_xuniform(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.xavier_uniform_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_xnormal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.xavier_normal_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  if init_m == 'uniform':\n",
    "    m.apply(init_uniform)\n",
    "  elif init_m == 'normal':\n",
    "    m.apply(init_normal)\n",
    "  elif init_m == 'xaiver uniform':\n",
    "    m.apply(init_xuniform)\n",
    "  elif init_m == 'xavier normal':\n",
    "    m.apply(init_xnormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuCpyycNCKEZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650891686794,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "YbCOnCSNCL25",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 8\n",
    "output_size = 1\n",
    "num_layers = 6\n",
    "hidden_size = 600\n",
    "batch_size = 1162\n",
    "epochs = 2000\n",
    "lr = 0.000193\n",
    "init_method = 'xaiver uniform'\n",
    "act_fn = 'ELU'\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers, act_fn)\n",
    "init_weights(model, init_method)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 7351,
     "status": "ok",
     "timestamp": 1650891694138,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "QqZbxrppvDpZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1650891694142,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "Q-9T0GArCMgp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class OptDataset(Dataset):\n",
    "\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X[idx], self.y[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6-hCH2ivDpb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Losses Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1650891694144,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "mVaO8TruHW4M",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def MAPELoss(output, target):\n",
    "  return torch.mean(torch.abs((target - output) / target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1650891694149,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "tVLW5dHhvDpe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, X_val, y_val):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X_val, y_val), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = loss_fn(out, batch_labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    print('\\nVal set: Average loss: {:.8f}\\n'.format(\n",
    "                losses.mean()))\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68eF5_EovDpf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Early Stopping class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650891694150,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "n2So2ffSvDpg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code took form: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, \n",
    "                 patience=10, \n",
    "                 verbose=False, \n",
    "                 delta=0, \n",
    "                 path='../models/final_heston_model.chkpt',\n",
    "                 trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL_xmnKXvDph",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1650891694152,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "DrBBTGKJvDph",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val\n",
    "):\n",
    "\n",
    "  training_losses = []\n",
    "  validation_losses = []\n",
    "\n",
    "  early_stopping = EarlyStopping(patience=20)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "\n",
    "    for batch, batch_labels in DataLoader(OptDataset(X_train, y_train), batch_size=batch_size):\n",
    "      out = model(batch.to(device))\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss = loss_fn(out, batch_labels.to(device))\n",
    "      epoch_losses.append(loss.item())\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if i > 0 and i % 50 == 0:\n",
    "        avg_loss = total_loss / 50\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| Epoch {:3d} | {:5d}/{:5d} batches | lr {:2.5f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.8f}'.format(\n",
    "              epoch, i, len(X_train) // batch_size+1, lr, elapsed * 1000 / 50,\n",
    "              avg_loss))\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "      i += 1\n",
    "\n",
    "    training_losses.append(np.array(epoch_losses).mean())\n",
    "    val_loss = evaluate(model, loss_fn, X_val, y_val)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Stopping at Epoch: {epoch}\")\n",
    "        break\n",
    "\n",
    "  return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2525796,
     "status": "ok",
     "timestamp": 1650894219916,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "hBETWoCfvDpj",
    "outputId": "30acf99f-3a1e-4d87-d1ff-13068ebe4cd4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 |    50/  258 batches | lr 0.00019 | ms/batch 29.99 | loss 0.50392869\n",
      "| Epoch   0 |   100/  258 batches | lr 0.00019 | ms/batch 21.66 | loss 0.19444944\n",
      "| Epoch   0 |   150/  258 batches | lr 0.00019 | ms/batch 21.39 | loss 0.14266609\n",
      "| Epoch   0 |   200/  258 batches | lr 0.00019 | ms/batch 19.36 | loss 0.12785340\n",
      "| Epoch   0 |   250/  258 batches | lr 0.00019 | ms/batch 21.50 | loss 0.12628896\n",
      "\n",
      "Val set: Average loss: 0.12812898\n",
      "\n",
      "| Epoch   1 |    50/  258 batches | lr 0.00019 | ms/batch 20.06 | loss 0.13621995\n",
      "| Epoch   1 |   100/  258 batches | lr 0.00019 | ms/batch 21.26 | loss 0.13220316\n",
      "| Epoch   1 |   150/  258 batches | lr 0.00019 | ms/batch 21.37 | loss 0.12807747\n",
      "| Epoch   1 |   200/  258 batches | lr 0.00019 | ms/batch 19.46 | loss 0.12076801\n",
      "| Epoch   1 |   250/  258 batches | lr 0.00019 | ms/batch 23.01 | loss 0.11903761\n",
      "\n",
      "Val set: Average loss: 0.12094337\n",
      "\n",
      "| Epoch   2 |    50/  258 batches | lr 0.00019 | ms/batch 23.71 | loss 0.13974048\n",
      "| Epoch   2 |   100/  258 batches | lr 0.00019 | ms/batch 19.51 | loss 0.14746277\n",
      "| Epoch   2 |   150/  258 batches | lr 0.00019 | ms/batch 21.23 | loss 0.12350071\n",
      "| Epoch   2 |   200/  258 batches | lr 0.00019 | ms/batch 21.19 | loss 0.11546819\n",
      "| Epoch   2 |   250/  258 batches | lr 0.00019 | ms/batch 22.14 | loss 0.11550495\n",
      "\n",
      "Val set: Average loss: 0.11511689\n",
      "\n",
      "| Epoch   3 |    50/  258 batches | lr 0.00019 | ms/batch 22.61 | loss 0.12540054\n",
      "| Epoch   3 |   100/  258 batches | lr 0.00019 | ms/batch 21.55 | loss 0.12376935\n",
      "| Epoch   3 |   150/  258 batches | lr 0.00019 | ms/batch 19.75 | loss 0.12033229\n",
      "| Epoch   3 |   200/  258 batches | lr 0.00019 | ms/batch 22.67 | loss 0.11284113\n",
      "| Epoch   3 |   250/  258 batches | lr 0.00019 | ms/batch 22.54 | loss 0.11448020\n",
      "\n",
      "Val set: Average loss: 0.11139477\n",
      "\n",
      "| Epoch   4 |    50/  258 batches | lr 0.00019 | ms/batch 23.47 | loss 0.12080415\n",
      "| Epoch   4 |   100/  258 batches | lr 0.00019 | ms/batch 22.75 | loss 0.11987751\n",
      "| Epoch   4 |   150/  258 batches | lr 0.00019 | ms/batch 21.70 | loss 0.11964405\n",
      "| Epoch   4 |   200/  258 batches | lr 0.00019 | ms/batch 19.65 | loss 0.11087319\n",
      "| Epoch   4 |   250/  258 batches | lr 0.00019 | ms/batch 22.30 | loss 0.11204751\n",
      "\n",
      "Val set: Average loss: 0.10578950\n",
      "\n",
      "| Epoch   5 |    50/  258 batches | lr 0.00019 | ms/batch 22.41 | loss 0.11776692\n",
      "| Epoch   5 |   100/  258 batches | lr 0.00019 | ms/batch 23.48 | loss 0.11587323\n",
      "| Epoch   5 |   150/  258 batches | lr 0.00019 | ms/batch 22.04 | loss 0.11630596\n",
      "| Epoch   5 |   200/  258 batches | lr 0.00019 | ms/batch 22.01 | loss 0.10820648\n",
      "| Epoch   5 |   250/  258 batches | lr 0.00019 | ms/batch 21.95 | loss 0.11043314\n",
      "\n",
      "Val set: Average loss: 0.10159984\n",
      "\n",
      "| Epoch   6 |    50/  258 batches | lr 0.00019 | ms/batch 22.80 | loss 0.11610948\n",
      "| Epoch   6 |   100/  258 batches | lr 0.00019 | ms/batch 21.92 | loss 0.11137497\n",
      "| Epoch   6 |   150/  258 batches | lr 0.00019 | ms/batch 22.27 | loss 0.11231196\n",
      "| Epoch   6 |   200/  258 batches | lr 0.00019 | ms/batch 20.83 | loss 0.10625024\n",
      "| Epoch   6 |   250/  258 batches | lr 0.00019 | ms/batch 21.37 | loss 0.10762399\n",
      "\n",
      "Val set: Average loss: 0.09935049\n",
      "\n",
      "| Epoch   7 |    50/  258 batches | lr 0.00019 | ms/batch 22.32 | loss 0.11460068\n",
      "| Epoch   7 |   100/  258 batches | lr 0.00019 | ms/batch 22.06 | loss 0.10909105\n",
      "| Epoch   7 |   150/  258 batches | lr 0.00019 | ms/batch 21.88 | loss 0.10917303\n",
      "| Epoch   7 |   200/  258 batches | lr 0.00019 | ms/batch 21.49 | loss 0.10402139\n",
      "| Epoch   7 |   250/  258 batches | lr 0.00019 | ms/batch 21.66 | loss 0.10436677\n",
      "\n",
      "Val set: Average loss: 0.09732164\n",
      "\n",
      "| Epoch   8 |    50/  258 batches | lr 0.00019 | ms/batch 20.36 | loss 0.11375826\n",
      "| Epoch   8 |   100/  258 batches | lr 0.00019 | ms/batch 22.43 | loss 0.10756559\n",
      "| Epoch   8 |   150/  258 batches | lr 0.00019 | ms/batch 20.18 | loss 0.10688277\n",
      "| Epoch   8 |   200/  258 batches | lr 0.00019 | ms/batch 24.41 | loss 0.10301902\n",
      "| Epoch   8 |   250/  258 batches | lr 0.00019 | ms/batch 21.12 | loss 0.10266204\n",
      "\n",
      "Val set: Average loss: 0.09614174\n",
      "\n",
      "| Epoch   9 |    50/  258 batches | lr 0.00019 | ms/batch 22.07 | loss 0.11151091\n",
      "| Epoch   9 |   100/  258 batches | lr 0.00019 | ms/batch 22.18 | loss 0.10760559\n",
      "| Epoch   9 |   150/  258 batches | lr 0.00019 | ms/batch 20.02 | loss 0.10542052\n",
      "| Epoch   9 |   200/  258 batches | lr 0.00019 | ms/batch 22.34 | loss 0.10276141\n",
      "| Epoch   9 |   250/  258 batches | lr 0.00019 | ms/batch 20.19 | loss 0.10169662\n",
      "\n",
      "Val set: Average loss: 0.09565995\n",
      "\n",
      "| Epoch  10 |    50/  258 batches | lr 0.00019 | ms/batch 23.34 | loss 0.11016496\n",
      "| Epoch  10 |   100/  258 batches | lr 0.00019 | ms/batch 21.29 | loss 0.10753988\n",
      "| Epoch  10 |   150/  258 batches | lr 0.00019 | ms/batch 21.03 | loss 0.10506186\n",
      "| Epoch  10 |   200/  258 batches | lr 0.00019 | ms/batch 22.77 | loss 0.10272393\n",
      "| Epoch  10 |   250/  258 batches | lr 0.00019 | ms/batch 21.18 | loss 0.10083873\n",
      "\n",
      "Val set: Average loss: 0.09412529\n",
      "\n",
      "| Epoch  11 |    50/  258 batches | lr 0.00019 | ms/batch 21.09 | loss 0.10779062\n",
      "| Epoch  11 |   100/  258 batches | lr 0.00019 | ms/batch 24.34 | loss 0.10590660\n",
      "| Epoch  11 |   150/  258 batches | lr 0.00019 | ms/batch 19.83 | loss 0.10399327\n",
      "| Epoch  11 |   200/  258 batches | lr 0.00019 | ms/batch 21.23 | loss 0.10232070\n",
      "| Epoch  11 |   250/  258 batches | lr 0.00019 | ms/batch 21.47 | loss 0.10022503\n",
      "\n",
      "Val set: Average loss: 0.09375224\n",
      "\n",
      "| Epoch  12 |    50/  258 batches | lr 0.00019 | ms/batch 22.14 | loss 0.10669746\n",
      "| Epoch  12 |   100/  258 batches | lr 0.00019 | ms/batch 21.78 | loss 0.10485039\n",
      "| Epoch  12 |   150/  258 batches | lr 0.00019 | ms/batch 19.35 | loss 0.10318514\n",
      "| Epoch  12 |   200/  258 batches | lr 0.00019 | ms/batch 21.19 | loss 0.10129466\n",
      "| Epoch  12 |   250/  258 batches | lr 0.00019 | ms/batch 21.73 | loss 0.09944276\n",
      "\n",
      "Val set: Average loss: 0.09352735\n",
      "\n",
      "| Epoch  13 |    50/  258 batches | lr 0.00019 | ms/batch 21.07 | loss 0.10581070\n",
      "| Epoch  13 |   100/  258 batches | lr 0.00019 | ms/batch 22.33 | loss 0.10398752\n",
      "| Epoch  13 |   150/  258 batches | lr 0.00019 | ms/batch 22.73 | loss 0.10250000\n",
      "| Epoch  13 |   200/  258 batches | lr 0.00019 | ms/batch 25.52 | loss 0.10032408\n",
      "| Epoch  13 |   250/  258 batches | lr 0.00019 | ms/batch 22.27 | loss 0.09871742\n",
      "\n",
      "Val set: Average loss: 0.09336410\n",
      "\n",
      "| Epoch  14 |    50/  258 batches | lr 0.00019 | ms/batch 25.15 | loss 0.10495173\n",
      "| Epoch  14 |   100/  258 batches | lr 0.00019 | ms/batch 24.39 | loss 0.10326682\n",
      "| Epoch  14 |   150/  258 batches | lr 0.00019 | ms/batch 31.58 | loss 0.10178289\n",
      "| Epoch  14 |   200/  258 batches | lr 0.00019 | ms/batch 30.22 | loss 0.09953146\n",
      "| Epoch  14 |   250/  258 batches | lr 0.00019 | ms/batch 26.24 | loss 0.09809433\n",
      "\n",
      "Val set: Average loss: 0.09323070\n",
      "\n",
      "| Epoch  15 |    50/  258 batches | lr 0.00019 | ms/batch 32.38 | loss 0.10415964\n",
      "| Epoch  15 |   100/  258 batches | lr 0.00019 | ms/batch 26.26 | loss 0.10259583\n",
      "| Epoch  15 |   150/  258 batches | lr 0.00019 | ms/batch 27.02 | loss 0.10102307\n",
      "| Epoch  15 |   200/  258 batches | lr 0.00019 | ms/batch 25.38 | loss 0.09883897\n",
      "| Epoch  15 |   250/  258 batches | lr 0.00019 | ms/batch 30.87 | loss 0.09751468\n",
      "\n",
      "Val set: Average loss: 0.09313590\n",
      "\n",
      "| Epoch  16 |    50/  258 batches | lr 0.00019 | ms/batch 31.59 | loss 0.10344086\n",
      "| Epoch  16 |   100/  258 batches | lr 0.00019 | ms/batch 34.74 | loss 0.10192744\n",
      "| Epoch  16 |   150/  258 batches | lr 0.00019 | ms/batch 32.12 | loss 0.10021742\n",
      "| Epoch  16 |   200/  258 batches | lr 0.00019 | ms/batch 34.50 | loss 0.09820378\n",
      "| Epoch  16 |   250/  258 batches | lr 0.00019 | ms/batch 39.10 | loss 0.09696114\n",
      "\n",
      "Val set: Average loss: 0.09305749\n",
      "\n",
      "| Epoch  17 |    50/  258 batches | lr 0.00019 | ms/batch 24.53 | loss 0.10279887\n",
      "| Epoch  17 |   100/  258 batches | lr 0.00019 | ms/batch 30.43 | loss 0.10120196\n",
      "| Epoch  17 |   150/  258 batches | lr 0.00019 | ms/batch 26.76 | loss 0.09939513\n",
      "| Epoch  17 |   200/  258 batches | lr 0.00019 | ms/batch 26.83 | loss 0.09759809\n",
      "| Epoch  17 |   250/  258 batches | lr 0.00019 | ms/batch 29.68 | loss 0.09639138\n",
      "\n",
      "Val set: Average loss: 0.09295619\n",
      "\n",
      "| Epoch  18 |    50/  258 batches | lr 0.00019 | ms/batch 24.11 | loss 0.10216533\n",
      "| Epoch  18 |   100/  258 batches | lr 0.00019 | ms/batch 23.41 | loss 0.10042274\n",
      "| Epoch  18 |   150/  258 batches | lr 0.00019 | ms/batch 22.19 | loss 0.09855952\n",
      "| Epoch  18 |   200/  258 batches | lr 0.00019 | ms/batch 23.45 | loss 0.09702432\n",
      "| Epoch  18 |   250/  258 batches | lr 0.00019 | ms/batch 22.16 | loss 0.09583683\n",
      "\n",
      "Val set: Average loss: 0.09272834\n",
      "\n",
      "| Epoch  19 |    50/  258 batches | lr 0.00019 | ms/batch 22.45 | loss 0.10160628\n",
      "| Epoch  19 |   100/  258 batches | lr 0.00019 | ms/batch 22.76 | loss 0.09954030\n",
      "| Epoch  19 |   150/  258 batches | lr 0.00019 | ms/batch 22.27 | loss 0.09774676\n",
      "| Epoch  19 |   200/  258 batches | lr 0.00019 | ms/batch 21.94 | loss 0.09645966\n",
      "| Epoch  19 |   250/  258 batches | lr 0.00019 | ms/batch 21.99 | loss 0.09525920\n",
      "\n",
      "Val set: Average loss: 0.09237496\n",
      "\n",
      "| Epoch  20 |    50/  258 batches | lr 0.00019 | ms/batch 20.58 | loss 0.10109206\n",
      "| Epoch  20 |   100/  258 batches | lr 0.00019 | ms/batch 22.16 | loss 0.09867635\n",
      "| Epoch  20 |   150/  258 batches | lr 0.00019 | ms/batch 20.34 | loss 0.09695005\n",
      "| Epoch  20 |   200/  258 batches | lr 0.00019 | ms/batch 21.79 | loss 0.09585917\n",
      "| Epoch  20 |   250/  258 batches | lr 0.00019 | ms/batch 21.74 | loss 0.09465899\n",
      "\n",
      "Val set: Average loss: 0.09193127\n",
      "\n",
      "| Epoch  21 |    50/  258 batches | lr 0.00019 | ms/batch 25.60 | loss 0.10058720\n",
      "| Epoch  21 |   100/  258 batches | lr 0.00019 | ms/batch 20.51 | loss 0.09793471\n",
      "| Epoch  21 |   150/  258 batches | lr 0.00019 | ms/batch 22.60 | loss 0.09615381\n",
      "| Epoch  21 |   200/  258 batches | lr 0.00019 | ms/batch 23.40 | loss 0.09521353\n",
      "| Epoch  21 |   250/  258 batches | lr 0.00019 | ms/batch 22.35 | loss 0.09407023\n",
      "\n",
      "Val set: Average loss: 0.09141970\n",
      "\n",
      "| Epoch  22 |    50/  258 batches | lr 0.00019 | ms/batch 22.66 | loss 0.10005777\n",
      "| Epoch  22 |   100/  258 batches | lr 0.00019 | ms/batch 22.12 | loss 0.09725725\n",
      "| Epoch  22 |   150/  258 batches | lr 0.00019 | ms/batch 22.19 | loss 0.09532387\n",
      "| Epoch  22 |   200/  258 batches | lr 0.00019 | ms/batch 22.58 | loss 0.09456729\n",
      "| Epoch  22 |   250/  258 batches | lr 0.00019 | ms/batch 20.64 | loss 0.09350123\n",
      "\n",
      "Val set: Average loss: 0.09082418\n",
      "\n",
      "| Epoch  23 |    50/  258 batches | lr 0.00019 | ms/batch 23.90 | loss 0.09944515\n",
      "| Epoch  23 |   100/  258 batches | lr 0.00019 | ms/batch 22.16 | loss 0.09654505\n",
      "| Epoch  23 |   150/  258 batches | lr 0.00019 | ms/batch 21.86 | loss 0.09433739\n",
      "| Epoch  23 |   200/  258 batches | lr 0.00019 | ms/batch 19.87 | loss 0.09396370\n",
      "| Epoch  23 |   250/  258 batches | lr 0.00019 | ms/batch 21.90 | loss 0.09286280\n",
      "\n",
      "Val set: Average loss: 0.08999808\n",
      "\n",
      "| Epoch  24 |    50/  258 batches | lr 0.00019 | ms/batch 22.19 | loss 0.09862678\n",
      "| Epoch  24 |   100/  258 batches | lr 0.00019 | ms/batch 21.85 | loss 0.09565410\n",
      "| Epoch  24 |   150/  258 batches | lr 0.00019 | ms/batch 21.60 | loss 0.09321603\n",
      "| Epoch  24 |   200/  258 batches | lr 0.00019 | ms/batch 21.94 | loss 0.09330097\n",
      "| Epoch  24 |   250/  258 batches | lr 0.00019 | ms/batch 21.59 | loss 0.09201666\n",
      "\n",
      "Val set: Average loss: 0.08880324\n",
      "\n",
      "| Epoch  25 |    50/  258 batches | lr 0.00019 | ms/batch 21.33 | loss 0.09738342\n",
      "| Epoch  25 |   100/  258 batches | lr 0.00019 | ms/batch 20.91 | loss 0.09456854\n",
      "| Epoch  25 |   150/  258 batches | lr 0.00019 | ms/batch 19.23 | loss 0.09194406\n",
      "| Epoch  25 |   200/  258 batches | lr 0.00019 | ms/batch 22.25 | loss 0.09250085\n",
      "| Epoch  25 |   250/  258 batches | lr 0.00019 | ms/batch 21.07 | loss 0.09085497\n",
      "\n",
      "Val set: Average loss: 0.08741898\n",
      "\n",
      "| Epoch  26 |    50/  258 batches | lr 0.00019 | ms/batch 21.55 | loss 0.09577778\n",
      "| Epoch  26 |   100/  258 batches | lr 0.00019 | ms/batch 20.89 | loss 0.09362891\n",
      "| Epoch  26 |   150/  258 batches | lr 0.00019 | ms/batch 19.46 | loss 0.09038628\n",
      "| Epoch  26 |   200/  258 batches | lr 0.00019 | ms/batch 20.90 | loss 0.09151312\n",
      "| Epoch  26 |   250/  258 batches | lr 0.00019 | ms/batch 20.88 | loss 0.08936261\n",
      "\n",
      "Val set: Average loss: 0.08580302\n",
      "\n",
      "| Epoch  27 |    50/  258 batches | lr 0.00019 | ms/batch 21.90 | loss 0.09403714\n",
      "| Epoch  27 |   100/  258 batches | lr 0.00019 | ms/batch 21.95 | loss 0.09278244\n",
      "| Epoch  27 |   150/  258 batches | lr 0.00019 | ms/batch 21.73 | loss 0.08824549\n",
      "| Epoch  27 |   200/  258 batches | lr 0.00019 | ms/batch 21.27 | loss 0.08994789\n",
      "| Epoch  27 |   250/  258 batches | lr 0.00019 | ms/batch 21.92 | loss 0.08768873\n",
      "\n",
      "Val set: Average loss: 0.08360262\n",
      "\n",
      "| Epoch  28 |    50/  258 batches | lr 0.00019 | ms/batch 21.57 | loss 0.09225268\n",
      "| Epoch  28 |   100/  258 batches | lr 0.00019 | ms/batch 20.44 | loss 0.09165582\n",
      "| Epoch  28 |   150/  258 batches | lr 0.00019 | ms/batch 22.46 | loss 0.08585493\n",
      "| Epoch  28 |   200/  258 batches | lr 0.00019 | ms/batch 22.51 | loss 0.08785606\n",
      "| Epoch  28 |   250/  258 batches | lr 0.00019 | ms/batch 23.47 | loss 0.08608710\n",
      "\n",
      "Val set: Average loss: 0.08180319\n",
      "\n",
      "| Epoch  29 |    50/  258 batches | lr 0.00019 | ms/batch 24.91 | loss 0.09088934\n",
      "| Epoch  29 |   100/  258 batches | lr 0.00019 | ms/batch 23.75 | loss 0.08964977\n",
      "| Epoch  29 |   150/  258 batches | lr 0.00019 | ms/batch 23.36 | loss 0.08383640\n",
      "| Epoch  29 |   200/  258 batches | lr 0.00019 | ms/batch 22.19 | loss 0.08621268\n",
      "| Epoch  29 |   250/  258 batches | lr 0.00019 | ms/batch 23.15 | loss 0.08442976\n",
      "\n",
      "Val set: Average loss: 0.08171734\n",
      "\n",
      "| Epoch  30 |    50/  258 batches | lr 0.00019 | ms/batch 22.09 | loss 0.08920911\n",
      "| Epoch  30 |   100/  258 batches | lr 0.00019 | ms/batch 19.61 | loss 0.08881544\n",
      "| Epoch  30 |   150/  258 batches | lr 0.00019 | ms/batch 21.55 | loss 0.08231445\n",
      "| Epoch  30 |   200/  258 batches | lr 0.00019 | ms/batch 21.45 | loss 0.08439550\n",
      "| Epoch  30 |   250/  258 batches | lr 0.00019 | ms/batch 21.43 | loss 0.08318230\n",
      "\n",
      "Val set: Average loss: 0.08275500\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  31 |    50/  258 batches | lr 0.00019 | ms/batch 21.82 | loss 0.08822208\n",
      "| Epoch  31 |   100/  258 batches | lr 0.00019 | ms/batch 21.46 | loss 0.08790262\n",
      "| Epoch  31 |   150/  258 batches | lr 0.00019 | ms/batch 21.38 | loss 0.08168623\n",
      "| Epoch  31 |   200/  258 batches | lr 0.00019 | ms/batch 21.44 | loss 0.08270820\n",
      "| Epoch  31 |   250/  258 batches | lr 0.00019 | ms/batch 19.56 | loss 0.08249087\n",
      "\n",
      "Val set: Average loss: 0.08093565\n",
      "\n",
      "| Epoch  32 |    50/  258 batches | lr 0.00019 | ms/batch 21.68 | loss 0.08747243\n",
      "| Epoch  32 |   100/  258 batches | lr 0.00019 | ms/batch 21.87 | loss 0.08611370\n",
      "| Epoch  32 |   150/  258 batches | lr 0.00019 | ms/batch 20.07 | loss 0.08065289\n",
      "| Epoch  32 |   200/  258 batches | lr 0.00019 | ms/batch 20.34 | loss 0.08149003\n",
      "| Epoch  32 |   250/  258 batches | lr 0.00019 | ms/batch 22.01 | loss 0.08169507\n",
      "\n",
      "Val set: Average loss: 0.07910596\n",
      "\n",
      "| Epoch  33 |    50/  258 batches | lr 0.00019 | ms/batch 19.85 | loss 0.08649450\n",
      "| Epoch  33 |   100/  258 batches | lr 0.00019 | ms/batch 21.44 | loss 0.08487800\n",
      "| Epoch  33 |   150/  258 batches | lr 0.00019 | ms/batch 21.63 | loss 0.08011674\n",
      "| Epoch  33 |   200/  258 batches | lr 0.00019 | ms/batch 19.57 | loss 0.08045992\n",
      "| Epoch  33 |   250/  258 batches | lr 0.00019 | ms/batch 21.30 | loss 0.08104291\n",
      "\n",
      "Val set: Average loss: 0.07748700\n",
      "\n",
      "| Epoch  34 |    50/  258 batches | lr 0.00019 | ms/batch 21.77 | loss 0.08589487\n",
      "| Epoch  34 |   100/  258 batches | lr 0.00019 | ms/batch 21.51 | loss 0.08425271\n",
      "| Epoch  34 |   150/  258 batches | lr 0.00019 | ms/batch 19.55 | loss 0.08005354\n",
      "| Epoch  34 |   200/  258 batches | lr 0.00019 | ms/batch 21.36 | loss 0.07964629\n",
      "| Epoch  34 |   250/  258 batches | lr 0.00019 | ms/batch 21.57 | loss 0.08042654\n",
      "\n",
      "Val set: Average loss: 0.07704754\n",
      "\n",
      "| Epoch  35 |    50/  258 batches | lr 0.00019 | ms/batch 19.95 | loss 0.08531833\n",
      "| Epoch  35 |   100/  258 batches | lr 0.00019 | ms/batch 21.67 | loss 0.08368152\n",
      "| Epoch  35 |   150/  258 batches | lr 0.00019 | ms/batch 21.35 | loss 0.07948879\n",
      "| Epoch  35 |   200/  258 batches | lr 0.00019 | ms/batch 21.49 | loss 0.07910379\n",
      "| Epoch  35 |   250/  258 batches | lr 0.00019 | ms/batch 21.58 | loss 0.07980950\n",
      "\n",
      "Val set: Average loss: 0.07746787\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  36 |    50/  258 batches | lr 0.00019 | ms/batch 22.36 | loss 0.08481357\n",
      "| Epoch  36 |   100/  258 batches | lr 0.00019 | ms/batch 19.66 | loss 0.08283271\n",
      "| Epoch  36 |   150/  258 batches | lr 0.00019 | ms/batch 23.47 | loss 0.07886365\n",
      "| Epoch  36 |   200/  258 batches | lr 0.00019 | ms/batch 19.85 | loss 0.07889817\n",
      "| Epoch  36 |   250/  258 batches | lr 0.00019 | ms/batch 19.63 | loss 0.07945527\n",
      "\n",
      "Val set: Average loss: 0.07720713\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  37 |    50/  258 batches | lr 0.00019 | ms/batch 21.84 | loss 0.08456197\n",
      "| Epoch  37 |   100/  258 batches | lr 0.00019 | ms/batch 21.49 | loss 0.08210655\n",
      "| Epoch  37 |   150/  258 batches | lr 0.00019 | ms/batch 23.83 | loss 0.07822547\n",
      "| Epoch  37 |   200/  258 batches | lr 0.00019 | ms/batch 22.61 | loss 0.07898236\n",
      "| Epoch  37 |   250/  258 batches | lr 0.00019 | ms/batch 22.44 | loss 0.07914201\n",
      "\n",
      "Val set: Average loss: 0.07602084\n",
      "\n",
      "| Epoch  38 |    50/  258 batches | lr 0.00019 | ms/batch 20.80 | loss 0.08412221\n",
      "| Epoch  38 |   100/  258 batches | lr 0.00019 | ms/batch 20.57 | loss 0.08157570\n",
      "| Epoch  38 |   150/  258 batches | lr 0.00019 | ms/batch 22.43 | loss 0.07763087\n",
      "| Epoch  38 |   200/  258 batches | lr 0.00019 | ms/batch 22.41 | loss 0.07926668\n",
      "| Epoch  38 |   250/  258 batches | lr 0.00019 | ms/batch 22.10 | loss 0.07859355\n",
      "\n",
      "Val set: Average loss: 0.07451066\n",
      "\n",
      "| Epoch  39 |    50/  258 batches | lr 0.00019 | ms/batch 20.30 | loss 0.08294196\n",
      "| Epoch  39 |   100/  258 batches | lr 0.00019 | ms/batch 21.34 | loss 0.08095345\n",
      "| Epoch  39 |   150/  258 batches | lr 0.00019 | ms/batch 21.20 | loss 0.07714307\n",
      "| Epoch  39 |   200/  258 batches | lr 0.00019 | ms/batch 21.38 | loss 0.07954393\n",
      "| Epoch  39 |   250/  258 batches | lr 0.00019 | ms/batch 19.56 | loss 0.07788315\n",
      "\n",
      "Val set: Average loss: 0.07463690\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  40 |    50/  258 batches | lr 0.00019 | ms/batch 21.64 | loss 0.08187879\n",
      "| Epoch  40 |   100/  258 batches | lr 0.00019 | ms/batch 21.50 | loss 0.08052411\n",
      "| Epoch  40 |   150/  258 batches | lr 0.00019 | ms/batch 21.30 | loss 0.07678232\n",
      "| Epoch  40 |   200/  258 batches | lr 0.00019 | ms/batch 21.47 | loss 0.07967273\n",
      "| Epoch  40 |   250/  258 batches | lr 0.00019 | ms/batch 21.79 | loss 0.07751347\n",
      "\n",
      "Val set: Average loss: 0.07544120\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  41 |    50/  258 batches | lr 0.00019 | ms/batch 21.86 | loss 0.08125187\n",
      "| Epoch  41 |   100/  258 batches | lr 0.00019 | ms/batch 21.32 | loss 0.08017325\n",
      "| Epoch  41 |   150/  258 batches | lr 0.00019 | ms/batch 21.75 | loss 0.07640518\n",
      "| Epoch  41 |   200/  258 batches | lr 0.00019 | ms/batch 19.56 | loss 0.07956143\n",
      "| Epoch  41 |   250/  258 batches | lr 0.00019 | ms/batch 21.43 | loss 0.07727430\n",
      "\n",
      "Val set: Average loss: 0.07665071\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  42 |    50/  258 batches | lr 0.00019 | ms/batch 22.00 | loss 0.08084221\n",
      "| Epoch  42 |   100/  258 batches | lr 0.00019 | ms/batch 23.27 | loss 0.07985429\n",
      "| Epoch  42 |   150/  258 batches | lr 0.00019 | ms/batch 19.69 | loss 0.07604595\n",
      "| Epoch  42 |   200/  258 batches | lr 0.00019 | ms/batch 21.82 | loss 0.07929213\n",
      "| Epoch  42 |   250/  258 batches | lr 0.00019 | ms/batch 21.74 | loss 0.07704129\n",
      "\n",
      "Val set: Average loss: 0.07790926\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  43 |    50/  258 batches | lr 0.00019 | ms/batch 21.71 | loss 0.08048531\n",
      "| Epoch  43 |   100/  258 batches | lr 0.00019 | ms/batch 19.51 | loss 0.07953951\n",
      "| Epoch  43 |   150/  258 batches | lr 0.00019 | ms/batch 23.03 | loss 0.07577091\n",
      "| Epoch  43 |   200/  258 batches | lr 0.00019 | ms/batch 19.55 | loss 0.07899744\n",
      "| Epoch  43 |   250/  258 batches | lr 0.00019 | ms/batch 19.68 | loss 0.07687319\n",
      "\n",
      "Val set: Average loss: 0.07902340\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  44 |    50/  258 batches | lr 0.00019 | ms/batch 21.73 | loss 0.08016069\n",
      "| Epoch  44 |   100/  258 batches | lr 0.00019 | ms/batch 21.17 | loss 0.07926686\n",
      "| Epoch  44 |   150/  258 batches | lr 0.00019 | ms/batch 21.49 | loss 0.07556416\n",
      "| Epoch  44 |   200/  258 batches | lr 0.00019 | ms/batch 19.44 | loss 0.07872207\n",
      "| Epoch  44 |   250/  258 batches | lr 0.00019 | ms/batch 19.53 | loss 0.07673725\n",
      "\n",
      "Val set: Average loss: 0.07996391\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch  45 |    50/  258 batches | lr 0.00019 | ms/batch 21.93 | loss 0.07989797\n",
      "| Epoch  45 |   100/  258 batches | lr 0.00019 | ms/batch 21.38 | loss 0.07906220\n",
      "| Epoch  45 |   150/  258 batches | lr 0.00019 | ms/batch 21.57 | loss 0.07542699\n",
      "| Epoch  45 |   200/  258 batches | lr 0.00019 | ms/batch 21.46 | loss 0.07848520\n",
      "| Epoch  45 |   250/  258 batches | lr 0.00019 | ms/batch 21.53 | loss 0.07660352\n",
      "\n",
      "Val set: Average loss: 0.08054867\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch  46 |    50/  258 batches | lr 0.00019 | ms/batch 21.70 | loss 0.07970192\n",
      "| Epoch  46 |   100/  258 batches | lr 0.00019 | ms/batch 21.20 | loss 0.07889187\n",
      "| Epoch  46 |   150/  258 batches | lr 0.00019 | ms/batch 21.39 | loss 0.07534959\n",
      "| Epoch  46 |   200/  258 batches | lr 0.00019 | ms/batch 21.52 | loss 0.07833146\n",
      "| Epoch  46 |   250/  258 batches | lr 0.00019 | ms/batch 23.29 | loss 0.07647431\n",
      "\n",
      "Val set: Average loss: 0.07954508\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch  47 |    50/  258 batches | lr 0.00019 | ms/batch 22.09 | loss 0.07957227\n",
      "| Epoch  47 |   100/  258 batches | lr 0.00019 | ms/batch 21.34 | loss 0.07883561\n",
      "| Epoch  47 |   150/  258 batches | lr 0.00019 | ms/batch 21.45 | loss 0.07527384\n",
      "| Epoch  47 |   200/  258 batches | lr 0.00019 | ms/batch 21.53 | loss 0.07817637\n",
      "| Epoch  47 |   250/  258 batches | lr 0.00019 | ms/batch 19.70 | loss 0.07638711\n",
      "\n",
      "Val set: Average loss: 0.07609476\n",
      "\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch  48 |    50/  258 batches | lr 0.00019 | ms/batch 21.81 | loss 0.07934755\n",
      "| Epoch  48 |   100/  258 batches | lr 0.00019 | ms/batch 19.47 | loss 0.07904695\n",
      "| Epoch  48 |   150/  258 batches | lr 0.00019 | ms/batch 21.35 | loss 0.07518807\n",
      "| Epoch  48 |   200/  258 batches | lr 0.00019 | ms/batch 21.09 | loss 0.07785459\n",
      "| Epoch  48 |   250/  258 batches | lr 0.00019 | ms/batch 19.80 | loss 0.07624528\n",
      "\n",
      "Val set: Average loss: 0.07409324\n",
      "\n",
      "| Epoch  49 |    50/  258 batches | lr 0.00019 | ms/batch 20.16 | loss 0.07904115\n",
      "| Epoch  49 |   100/  258 batches | lr 0.00019 | ms/batch 21.16 | loss 0.07909053\n",
      "| Epoch  49 |   150/  258 batches | lr 0.00019 | ms/batch 21.63 | loss 0.07502490\n",
      "| Epoch  49 |   200/  258 batches | lr 0.00019 | ms/batch 21.40 | loss 0.07741846\n",
      "| Epoch  49 |   250/  258 batches | lr 0.00019 | ms/batch 19.54 | loss 0.07600195\n",
      "\n",
      "Val set: Average loss: 0.07363961\n",
      "\n",
      "| Epoch  50 |    50/  258 batches | lr 0.00019 | ms/batch 21.89 | loss 0.07876163\n",
      "| Epoch  50 |   100/  258 batches | lr 0.00019 | ms/batch 21.98 | loss 0.07891700\n",
      "| Epoch  50 |   150/  258 batches | lr 0.00019 | ms/batch 20.51 | loss 0.07475743\n",
      "| Epoch  50 |   200/  258 batches | lr 0.00019 | ms/batch 21.38 | loss 0.07702508\n",
      "| Epoch  50 |   250/  258 batches | lr 0.00019 | ms/batch 21.26 | loss 0.07574389\n",
      "\n",
      "Val set: Average loss: 0.07340442\n",
      "\n",
      "| Epoch  51 |    50/  258 batches | lr 0.00019 | ms/batch 22.01 | loss 0.07851850\n",
      "| Epoch  51 |   100/  258 batches | lr 0.00019 | ms/batch 21.27 | loss 0.07862150\n",
      "| Epoch  51 |   150/  258 batches | lr 0.00019 | ms/batch 19.53 | loss 0.07448953\n",
      "| Epoch  51 |   200/  258 batches | lr 0.00019 | ms/batch 21.26 | loss 0.07670913\n",
      "| Epoch  51 |   250/  258 batches | lr 0.00019 | ms/batch 21.45 | loss 0.07550477\n",
      "\n",
      "Val set: Average loss: 0.07317423\n",
      "\n",
      "| Epoch  52 |    50/  258 batches | lr 0.00019 | ms/batch 21.86 | loss 0.07831294\n",
      "| Epoch  52 |   100/  258 batches | lr 0.00019 | ms/batch 21.32 | loss 0.07825809\n",
      "| Epoch  52 |   150/  258 batches | lr 0.00019 | ms/batch 21.34 | loss 0.07426787\n",
      "| Epoch  52 |   200/  258 batches | lr 0.00019 | ms/batch 21.45 | loss 0.07646901\n",
      "| Epoch  52 |   250/  258 batches | lr 0.00019 | ms/batch 21.42 | loss 0.07528303\n",
      "\n",
      "Val set: Average loss: 0.07294340\n",
      "\n",
      "| Epoch  53 |    50/  258 batches | lr 0.00019 | ms/batch 21.72 | loss 0.07812821\n",
      "| Epoch  53 |   100/  258 batches | lr 0.00019 | ms/batch 19.58 | loss 0.07789806\n",
      "| Epoch  53 |   150/  258 batches | lr 0.00019 | ms/batch 21.23 | loss 0.07408786\n",
      "| Epoch  53 |   200/  258 batches | lr 0.00019 | ms/batch 21.39 | loss 0.07627531\n",
      "| Epoch  53 |   250/  258 batches | lr 0.00019 | ms/batch 21.54 | loss 0.07506946\n",
      "\n",
      "Val set: Average loss: 0.07273125\n",
      "\n",
      "| Epoch  54 |    50/  258 batches | lr 0.00019 | ms/batch 19.99 | loss 0.07795657\n",
      "| Epoch  54 |   100/  258 batches | lr 0.00019 | ms/batch 19.57 | loss 0.07755096\n",
      "| Epoch  54 |   150/  258 batches | lr 0.00019 | ms/batch 23.16 | loss 0.07394105\n",
      "| Epoch  54 |   200/  258 batches | lr 0.00019 | ms/batch 19.96 | loss 0.07610306\n",
      "| Epoch  54 |   250/  258 batches | lr 0.00019 | ms/batch 21.39 | loss 0.07485400\n",
      "\n",
      "Val set: Average loss: 0.07253725\n",
      "\n",
      "| Epoch  55 |    50/  258 batches | lr 0.00019 | ms/batch 19.87 | loss 0.07779640\n",
      "| Epoch  55 |   100/  258 batches | lr 0.00019 | ms/batch 21.34 | loss 0.07721074\n",
      "| Epoch  55 |   150/  258 batches | lr 0.00019 | ms/batch 21.42 | loss 0.07380877\n",
      "| Epoch  55 |   200/  258 batches | lr 0.00019 | ms/batch 21.34 | loss 0.07592959\n",
      "| Epoch  55 |   250/  258 batches | lr 0.00019 | ms/batch 21.41 | loss 0.07463207\n",
      "\n",
      "Val set: Average loss: 0.07235619\n",
      "\n",
      "| Epoch  56 |    50/  258 batches | lr 0.00019 | ms/batch 19.92 | loss 0.07765355\n",
      "| Epoch  56 |   100/  258 batches | lr 0.00019 | ms/batch 21.36 | loss 0.07688459\n",
      "| Epoch  56 |   150/  258 batches | lr 0.00019 | ms/batch 21.48 | loss 0.07368034\n",
      "| Epoch  56 |   200/  258 batches | lr 0.00019 | ms/batch 21.29 | loss 0.07574156\n",
      "| Epoch  56 |   250/  258 batches | lr 0.00019 | ms/batch 21.45 | loss 0.07441078\n",
      "\n",
      "Val set: Average loss: 0.07219622\n",
      "\n",
      "| Epoch  57 |    50/  258 batches | lr 0.00019 | ms/batch 22.03 | loss 0.07753673\n",
      "| Epoch  57 |   100/  258 batches | lr 0.00019 | ms/batch 21.62 | loss 0.07656729\n",
      "| Epoch  57 |   150/  258 batches | lr 0.00019 | ms/batch 21.42 | loss 0.07354096\n",
      "| Epoch  57 |   200/  258 batches | lr 0.00019 | ms/batch 21.47 | loss 0.07555476\n",
      "| Epoch  57 |   250/  258 batches | lr 0.00019 | ms/batch 19.54 | loss 0.07419917\n",
      "\n",
      "Val set: Average loss: 0.07205276\n",
      "\n",
      "| Epoch  58 |    50/  258 batches | lr 0.00019 | ms/batch 21.85 | loss 0.07743264\n",
      "| Epoch  58 |   100/  258 batches | lr 0.00019 | ms/batch 21.34 | loss 0.07627416\n",
      "| Epoch  58 |   150/  258 batches | lr 0.00019 | ms/batch 21.40 | loss 0.07339858\n",
      "| Epoch  58 |   200/  258 batches | lr 0.00019 | ms/batch 21.46 | loss 0.07536295\n",
      "| Epoch  58 |   250/  258 batches | lr 0.00019 | ms/batch 21.45 | loss 0.07399818\n",
      "\n",
      "Val set: Average loss: 0.07190644\n",
      "\n",
      "| Epoch  59 |    50/  258 batches | lr 0.00019 | ms/batch 21.89 | loss 0.07734532\n",
      "| Epoch  59 |   100/  258 batches | lr 0.00019 | ms/batch 21.57 | loss 0.07599481\n",
      "| Epoch  59 |   150/  258 batches | lr 0.00019 | ms/batch 21.44 | loss 0.07326056\n",
      "| Epoch  59 |   200/  258 batches | lr 0.00019 | ms/batch 21.51 | loss 0.07516239\n",
      "| Epoch  59 |   250/  258 batches | lr 0.00019 | ms/batch 19.54 | loss 0.07379356\n",
      "\n",
      "Val set: Average loss: 0.07174011\n",
      "\n",
      "| Epoch  60 |    50/  258 batches | lr 0.00019 | ms/batch 21.64 | loss 0.07724680\n",
      "| Epoch  60 |   100/  258 batches | lr 0.00019 | ms/batch 21.41 | loss 0.07573625\n",
      "| Epoch  60 |   150/  258 batches | lr 0.00019 | ms/batch 19.53 | loss 0.07315801\n",
      "| Epoch  60 |   200/  258 batches | lr 0.00019 | ms/batch 21.66 | loss 0.07496023\n",
      "| Epoch  60 |   250/  258 batches | lr 0.00019 | ms/batch 19.45 | loss 0.07359680\n",
      "\n",
      "Val set: Average loss: 0.07156025\n",
      "\n",
      "| Epoch  61 |    50/  258 batches | lr 0.00019 | ms/batch 21.56 | loss 0.07713819\n",
      "| Epoch  61 |   100/  258 batches | lr 0.00019 | ms/batch 21.29 | loss 0.07548468\n",
      "| Epoch  61 |   150/  258 batches | lr 0.00019 | ms/batch 21.39 | loss 0.07307787\n",
      "| Epoch  61 |   200/  258 batches | lr 0.00019 | ms/batch 21.44 | loss 0.07475782\n",
      "| Epoch  61 |   250/  258 batches | lr 0.00019 | ms/batch 21.22 | loss 0.07340053\n",
      "\n",
      "Val set: Average loss: 0.07135000\n",
      "\n",
      "| Epoch  62 |    50/  258 batches | lr 0.00019 | ms/batch 19.95 | loss 0.07700853\n",
      "| Epoch  62 |   100/  258 batches | lr 0.00019 | ms/batch 21.22 | loss 0.07523094\n",
      "| Epoch  62 |   150/  258 batches | lr 0.00019 | ms/batch 19.76 | loss 0.07299881\n",
      "| Epoch  62 |   200/  258 batches | lr 0.00019 | ms/batch 23.29 | loss 0.07457322\n",
      "| Epoch  62 |   250/  258 batches | lr 0.00019 | ms/batch 19.66 | loss 0.07321001\n",
      "\n",
      "Val set: Average loss: 0.07110937\n",
      "\n",
      "| Epoch  63 |    50/  258 batches | lr 0.00019 | ms/batch 22.07 | loss 0.07684949\n",
      "| Epoch  63 |   100/  258 batches | lr 0.00019 | ms/batch 21.22 | loss 0.07498646\n",
      "| Epoch  63 |   150/  258 batches | lr 0.00019 | ms/batch 21.36 | loss 0.07291942\n",
      "| Epoch  63 |   200/  258 batches | lr 0.00019 | ms/batch 19.62 | loss 0.07439586\n",
      "| Epoch  63 |   250/  258 batches | lr 0.00019 | ms/batch 21.42 | loss 0.07302854\n",
      "\n",
      "Val set: Average loss: 0.07085883\n",
      "\n",
      "| Epoch  64 |    50/  258 batches | lr 0.00019 | ms/batch 21.93 | loss 0.07666370\n",
      "| Epoch  64 |   100/  258 batches | lr 0.00019 | ms/batch 19.45 | loss 0.07475368\n",
      "| Epoch  64 |   150/  258 batches | lr 0.00019 | ms/batch 23.18 | loss 0.07284332\n",
      "| Epoch  64 |   200/  258 batches | lr 0.00019 | ms/batch 19.60 | loss 0.07423190\n",
      "| Epoch  64 |   250/  258 batches | lr 0.00019 | ms/batch 19.83 | loss 0.07284969\n",
      "\n",
      "Val set: Average loss: 0.07061302\n",
      "\n",
      "| Epoch  65 |    50/  258 batches | lr 0.00019 | ms/batch 21.69 | loss 0.07646641\n",
      "| Epoch  65 |   100/  258 batches | lr 0.00019 | ms/batch 21.52 | loss 0.07453408\n",
      "| Epoch  65 |   150/  258 batches | lr 0.00019 | ms/batch 19.49 | loss 0.07277341\n",
      "| Epoch  65 |   200/  258 batches | lr 0.00019 | ms/batch 21.39 | loss 0.07405866\n",
      "| Epoch  65 |   250/  258 batches | lr 0.00019 | ms/batch 21.71 | loss 0.07267464\n",
      "\n",
      "Val set: Average loss: 0.07037757\n",
      "\n",
      "| Epoch  66 |    50/  258 batches | lr 0.00019 | ms/batch 22.30 | loss 0.07627088\n",
      "| Epoch  66 |   100/  258 batches | lr 0.00019 | ms/batch 21.34 | loss 0.07432753\n",
      "| Epoch  66 |   150/  258 batches | lr 0.00019 | ms/batch 21.43 | loss 0.07270155\n",
      "| Epoch  66 |   200/  258 batches | lr 0.00019 | ms/batch 19.80 | loss 0.07389843\n",
      "| Epoch  66 |   250/  258 batches | lr 0.00019 | ms/batch 21.57 | loss 0.07250726\n",
      "\n",
      "Val set: Average loss: 0.07016171\n",
      "\n",
      "| Epoch  67 |    50/  258 batches | lr 0.00019 | ms/batch 21.83 | loss 0.07608010\n",
      "| Epoch  67 |   100/  258 batches | lr 0.00019 | ms/batch 21.50 | loss 0.07413005\n",
      "| Epoch  67 |   150/  258 batches | lr 0.00019 | ms/batch 21.43 | loss 0.07262110\n",
      "| Epoch  67 |   200/  258 batches | lr 0.00019 | ms/batch 19.53 | loss 0.07372869\n",
      "| Epoch  67 |   250/  258 batches | lr 0.00019 | ms/batch 21.24 | loss 0.07233470\n",
      "\n",
      "Val set: Average loss: 0.06997701\n",
      "\n",
      "| Epoch  68 |    50/  258 batches | lr 0.00019 | ms/batch 19.95 | loss 0.07590272\n",
      "| Epoch  68 |   100/  258 batches | lr 0.00019 | ms/batch 21.44 | loss 0.07393937\n",
      "| Epoch  68 |   150/  258 batches | lr 0.00019 | ms/batch 21.55 | loss 0.07255050\n",
      "| Epoch  68 |   200/  258 batches | lr 0.00019 | ms/batch 21.32 | loss 0.07355175\n",
      "| Epoch  68 |   250/  258 batches | lr 0.00019 | ms/batch 21.32 | loss 0.07216267\n",
      "\n",
      "Val set: Average loss: 0.06982160\n",
      "\n",
      "| Epoch  69 |    50/  258 batches | lr 0.00019 | ms/batch 20.08 | loss 0.07573951\n",
      "| Epoch  69 |   100/  258 batches | lr 0.00019 | ms/batch 21.63 | loss 0.07374948\n",
      "| Epoch  69 |   150/  258 batches | lr 0.00019 | ms/batch 21.35 | loss 0.07248888\n",
      "| Epoch  69 |   200/  258 batches | lr 0.00019 | ms/batch 21.45 | loss 0.07337799\n",
      "| Epoch  69 |   250/  258 batches | lr 0.00019 | ms/batch 19.66 | loss 0.07199532\n",
      "\n",
      "Val set: Average loss: 0.06968821\n",
      "\n",
      "| Epoch  70 |    50/  258 batches | lr 0.00019 | ms/batch 21.94 | loss 0.07559662\n",
      "| Epoch  70 |   100/  258 batches | lr 0.00019 | ms/batch 21.31 | loss 0.07356343\n",
      "| Epoch  70 |   150/  258 batches | lr 0.00019 | ms/batch 19.46 | loss 0.07242664\n",
      "| Epoch  70 |   200/  258 batches | lr 0.00019 | ms/batch 21.59 | loss 0.07320023\n",
      "| Epoch  70 |   250/  258 batches | lr 0.00019 | ms/batch 21.27 | loss 0.07182907\n",
      "\n",
      "Val set: Average loss: 0.06956846\n",
      "\n",
      "| Epoch  71 |    50/  258 batches | lr 0.00019 | ms/batch 22.01 | loss 0.07546474\n",
      "| Epoch  71 |   100/  258 batches | lr 0.00019 | ms/batch 23.24 | loss 0.07338458\n",
      "| Epoch  71 |   150/  258 batches | lr 0.00019 | ms/batch 19.49 | loss 0.07236596\n",
      "| Epoch  71 |   200/  258 batches | lr 0.00019 | ms/batch 19.65 | loss 0.07300359\n",
      "| Epoch  71 |   250/  258 batches | lr 0.00019 | ms/batch 23.26 | loss 0.07166415\n",
      "\n",
      "Val set: Average loss: 0.06945956\n",
      "\n",
      "| Epoch  72 |    50/  258 batches | lr 0.00019 | ms/batch 21.95 | loss 0.07534556\n",
      "| Epoch  72 |   100/  258 batches | lr 0.00019 | ms/batch 19.50 | loss 0.07321472\n",
      "| Epoch  72 |   150/  258 batches | lr 0.00019 | ms/batch 21.33 | loss 0.07231497\n",
      "| Epoch  72 |   200/  258 batches | lr 0.00019 | ms/batch 21.60 | loss 0.07281106\n",
      "| Epoch  72 |   250/  258 batches | lr 0.00019 | ms/batch 19.51 | loss 0.07150914\n",
      "\n",
      "Val set: Average loss: 0.06936621\n",
      "\n",
      "| Epoch  73 |    50/  258 batches | lr 0.00019 | ms/batch 21.69 | loss 0.07524250\n",
      "| Epoch  73 |   100/  258 batches | lr 0.00019 | ms/batch 21.46 | loss 0.07305460\n",
      "| Epoch  73 |   150/  258 batches | lr 0.00019 | ms/batch 21.34 | loss 0.07227702\n",
      "| Epoch  73 |   200/  258 batches | lr 0.00019 | ms/batch 21.74 | loss 0.07262036\n",
      "| Epoch  73 |   250/  258 batches | lr 0.00019 | ms/batch 19.45 | loss 0.07135514\n",
      "\n",
      "Val set: Average loss: 0.06928475\n",
      "\n",
      "| Epoch  74 |    50/  258 batches | lr 0.00019 | ms/batch 22.50 | loss 0.07513891\n",
      "| Epoch  74 |   100/  258 batches | lr 0.00019 | ms/batch 20.58 | loss 0.07290334\n",
      "| Epoch  74 |   150/  258 batches | lr 0.00019 | ms/batch 21.78 | loss 0.07223542\n",
      "| Epoch  74 |   200/  258 batches | lr 0.00019 | ms/batch 22.09 | loss 0.07242961\n",
      "| Epoch  74 |   250/  258 batches | lr 0.00019 | ms/batch 19.80 | loss 0.07120239\n",
      "\n",
      "Val set: Average loss: 0.06921643\n",
      "\n",
      "| Epoch  75 |    50/  258 batches | lr 0.00019 | ms/batch 22.28 | loss 0.07504175\n",
      "| Epoch  75 |   100/  258 batches | lr 0.00019 | ms/batch 22.07 | loss 0.07275958\n",
      "| Epoch  75 |   150/  258 batches | lr 0.00019 | ms/batch 21.58 | loss 0.07218495\n",
      "| Epoch  75 |   200/  258 batches | lr 0.00019 | ms/batch 21.53 | loss 0.07224486\n",
      "| Epoch  75 |   250/  258 batches | lr 0.00019 | ms/batch 21.59 | loss 0.07105851\n",
      "\n",
      "Val set: Average loss: 0.06914837\n",
      "\n",
      "| Epoch  76 |    50/  258 batches | lr 0.00019 | ms/batch 22.27 | loss 0.07495017\n",
      "| Epoch  76 |   100/  258 batches | lr 0.00019 | ms/batch 20.84 | loss 0.07262346\n",
      "| Epoch  76 |   150/  258 batches | lr 0.00019 | ms/batch 23.09 | loss 0.07212454\n",
      "| Epoch  76 |   200/  258 batches | lr 0.00019 | ms/batch 23.80 | loss 0.07206665\n",
      "| Epoch  76 |   250/  258 batches | lr 0.00019 | ms/batch 22.70 | loss 0.07092503\n",
      "\n",
      "Val set: Average loss: 0.06908766\n",
      "\n",
      "| Epoch  77 |    50/  258 batches | lr 0.00019 | ms/batch 25.52 | loss 0.07486115\n",
      "| Epoch  77 |   100/  258 batches | lr 0.00019 | ms/batch 20.84 | loss 0.07249337\n",
      "| Epoch  77 |   150/  258 batches | lr 0.00019 | ms/batch 24.40 | loss 0.07206071\n",
      "| Epoch  77 |   200/  258 batches | lr 0.00019 | ms/batch 23.46 | loss 0.07187715\n",
      "| Epoch  77 |   250/  258 batches | lr 0.00019 | ms/batch 23.95 | loss 0.07079906\n",
      "\n",
      "Val set: Average loss: 0.06901826\n",
      "\n",
      "| Epoch  78 |    50/  258 batches | lr 0.00019 | ms/batch 22.12 | loss 0.07476622\n",
      "| Epoch  78 |   100/  258 batches | lr 0.00019 | ms/batch 19.35 | loss 0.07236754\n",
      "| Epoch  78 |   150/  258 batches | lr 0.00019 | ms/batch 21.88 | loss 0.07200132\n",
      "| Epoch  78 |   200/  258 batches | lr 0.00019 | ms/batch 21.14 | loss 0.07168914\n",
      "| Epoch  78 |   250/  258 batches | lr 0.00019 | ms/batch 20.95 | loss 0.07067371\n",
      "\n",
      "Val set: Average loss: 0.06893025\n",
      "\n",
      "| Epoch  79 |    50/  258 batches | lr 0.00019 | ms/batch 21.39 | loss 0.07466068\n",
      "| Epoch  79 |   100/  258 batches | lr 0.00019 | ms/batch 21.28 | loss 0.07225609\n",
      "| Epoch  79 |   150/  258 batches | lr 0.00019 | ms/batch 19.77 | loss 0.07195253\n",
      "| Epoch  79 |   200/  258 batches | lr 0.00019 | ms/batch 19.11 | loss 0.07151737\n",
      "| Epoch  79 |   250/  258 batches | lr 0.00019 | ms/batch 20.94 | loss 0.07054769\n",
      "\n",
      "Val set: Average loss: 0.06885986\n",
      "\n",
      "| Epoch  80 |    50/  258 batches | lr 0.00019 | ms/batch 21.46 | loss 0.07455715\n",
      "| Epoch  80 |   100/  258 batches | lr 0.00019 | ms/batch 21.04 | loss 0.07216537\n",
      "| Epoch  80 |   150/  258 batches | lr 0.00019 | ms/batch 21.01 | loss 0.07191870\n",
      "| Epoch  80 |   200/  258 batches | lr 0.00019 | ms/batch 20.98 | loss 0.07136353\n",
      "| Epoch  80 |   250/  258 batches | lr 0.00019 | ms/batch 21.16 | loss 0.07042843\n",
      "\n",
      "Val set: Average loss: 0.06877923\n",
      "\n",
      "| Epoch  81 |    50/  258 batches | lr 0.00019 | ms/batch 23.23 | loss 0.07445013\n",
      "| Epoch  81 |   100/  258 batches | lr 0.00019 | ms/batch 19.27 | loss 0.07210945\n",
      "| Epoch  81 |   150/  258 batches | lr 0.00019 | ms/batch 23.22 | loss 0.07191407\n",
      "| Epoch  81 |   200/  258 batches | lr 0.00019 | ms/batch 19.08 | loss 0.07121041\n",
      "| Epoch  81 |   250/  258 batches | lr 0.00019 | ms/batch 20.99 | loss 0.07032755\n",
      "\n",
      "Val set: Average loss: 0.06861965\n",
      "\n",
      "| Epoch  82 |    50/  258 batches | lr 0.00019 | ms/batch 21.59 | loss 0.07433277\n",
      "| Epoch  82 |   100/  258 batches | lr 0.00019 | ms/batch 19.20 | loss 0.07208798\n",
      "| Epoch  82 |   150/  258 batches | lr 0.00019 | ms/batch 21.03 | loss 0.07190892\n",
      "| Epoch  82 |   200/  258 batches | lr 0.00019 | ms/batch 20.82 | loss 0.07106279\n",
      "| Epoch  82 |   250/  258 batches | lr 0.00019 | ms/batch 21.07 | loss 0.07025262\n",
      "\n",
      "Val set: Average loss: 0.06845046\n",
      "\n",
      "| Epoch  83 |    50/  258 batches | lr 0.00019 | ms/batch 19.64 | loss 0.07422280\n",
      "| Epoch  83 |   100/  258 batches | lr 0.00019 | ms/batch 21.41 | loss 0.07205686\n",
      "| Epoch  83 |   150/  258 batches | lr 0.00019 | ms/batch 22.22 | loss 0.07181094\n",
      "| Epoch  83 |   200/  258 batches | lr 0.00019 | ms/batch 22.82 | loss 0.07090854\n",
      "| Epoch  83 |   250/  258 batches | lr 0.00019 | ms/batch 22.41 | loss 0.07020039\n",
      "\n",
      "Val set: Average loss: 0.06832263\n",
      "\n",
      "| Epoch  84 |    50/  258 batches | lr 0.00019 | ms/batch 23.27 | loss 0.07411181\n",
      "| Epoch  84 |   100/  258 batches | lr 0.00019 | ms/batch 22.16 | loss 0.07199684\n",
      "| Epoch  84 |   150/  258 batches | lr 0.00019 | ms/batch 21.33 | loss 0.07161564\n",
      "| Epoch  84 |   200/  258 batches | lr 0.00019 | ms/batch 21.44 | loss 0.07073969\n",
      "| Epoch  84 |   250/  258 batches | lr 0.00019 | ms/batch 21.37 | loss 0.07015813\n",
      "\n",
      "Val set: Average loss: 0.06820362\n",
      "\n",
      "| Epoch  85 |    50/  258 batches | lr 0.00019 | ms/batch 21.82 | loss 0.07400728\n",
      "| Epoch  85 |   100/  258 batches | lr 0.00019 | ms/batch 21.26 | loss 0.07195149\n",
      "| Epoch  85 |   150/  258 batches | lr 0.00019 | ms/batch 21.60 | loss 0.07137928\n",
      "| Epoch  85 |   200/  258 batches | lr 0.00019 | ms/batch 19.54 | loss 0.07059700\n",
      "| Epoch  85 |   250/  258 batches | lr 0.00019 | ms/batch 21.27 | loss 0.07012728\n",
      "\n",
      "Val set: Average loss: 0.06811980\n",
      "\n",
      "| Epoch  86 |    50/  258 batches | lr 0.00019 | ms/batch 20.03 | loss 0.07392705\n",
      "| Epoch  86 |   100/  258 batches | lr 0.00019 | ms/batch 21.52 | loss 0.07189491\n",
      "| Epoch  86 |   150/  258 batches | lr 0.00019 | ms/batch 21.25 | loss 0.07120507\n",
      "| Epoch  86 |   200/  258 batches | lr 0.00019 | ms/batch 21.37 | loss 0.07048988\n",
      "| Epoch  86 |   250/  258 batches | lr 0.00019 | ms/batch 21.52 | loss 0.07010357\n",
      "\n",
      "Val set: Average loss: 0.06809177\n",
      "\n",
      "| Epoch  87 |    50/  258 batches | lr 0.00019 | ms/batch 20.07 | loss 0.07386429\n",
      "| Epoch  87 |   100/  258 batches | lr 0.00019 | ms/batch 19.64 | loss 0.07183406\n",
      "| Epoch  87 |   150/  258 batches | lr 0.00019 | ms/batch 21.38 | loss 0.07106375\n",
      "| Epoch  87 |   200/  258 batches | lr 0.00019 | ms/batch 19.67 | loss 0.07039423\n",
      "| Epoch  87 |   250/  258 batches | lr 0.00019 | ms/batch 23.44 | loss 0.07008471\n",
      "\n",
      "Val set: Average loss: 0.06811879\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  88 |    50/  258 batches | lr 0.00019 | ms/batch 22.44 | loss 0.07380439\n",
      "| Epoch  88 |   100/  258 batches | lr 0.00019 | ms/batch 21.61 | loss 0.07176071\n",
      "| Epoch  88 |   150/  258 batches | lr 0.00019 | ms/batch 21.80 | loss 0.07092782\n",
      "| Epoch  88 |   200/  258 batches | lr 0.00019 | ms/batch 21.82 | loss 0.07029898\n",
      "| Epoch  88 |   250/  258 batches | lr 0.00019 | ms/batch 21.78 | loss 0.07006382\n",
      "\n",
      "Val set: Average loss: 0.06815846\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  89 |    50/  258 batches | lr 0.00019 | ms/batch 22.15 | loss 0.07373533\n",
      "| Epoch  89 |   100/  258 batches | lr 0.00019 | ms/batch 21.35 | loss 0.07166850\n",
      "| Epoch  89 |   150/  258 batches | lr 0.00019 | ms/batch 21.89 | loss 0.07078142\n",
      "| Epoch  89 |   200/  258 batches | lr 0.00019 | ms/batch 22.04 | loss 0.07019446\n",
      "| Epoch  89 |   250/  258 batches | lr 0.00019 | ms/batch 21.92 | loss 0.07002130\n",
      "\n",
      "Val set: Average loss: 0.06817802\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  90 |    50/  258 batches | lr 0.00019 | ms/batch 23.31 | loss 0.07365501\n",
      "| Epoch  90 |   100/  258 batches | lr 0.00019 | ms/batch 22.87 | loss 0.07156884\n",
      "| Epoch  90 |   150/  258 batches | lr 0.00019 | ms/batch 21.91 | loss 0.07061838\n",
      "| Epoch  90 |   200/  258 batches | lr 0.00019 | ms/batch 22.79 | loss 0.07007137\n",
      "| Epoch  90 |   250/  258 batches | lr 0.00019 | ms/batch 22.02 | loss 0.06995452\n",
      "\n",
      "Val set: Average loss: 0.06814719\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  91 |    50/  258 batches | lr 0.00019 | ms/batch 22.75 | loss 0.07355158\n",
      "| Epoch  91 |   100/  258 batches | lr 0.00019 | ms/batch 21.41 | loss 0.07145360\n",
      "| Epoch  91 |   150/  258 batches | lr 0.00019 | ms/batch 21.37 | loss 0.07043894\n",
      "| Epoch  91 |   200/  258 batches | lr 0.00019 | ms/batch 22.85 | loss 0.06992914\n",
      "| Epoch  91 |   250/  258 batches | lr 0.00019 | ms/batch 21.13 | loss 0.06986758\n",
      "\n",
      "Val set: Average loss: 0.06807156\n",
      "\n",
      "| Epoch  92 |    50/  258 batches | lr 0.00019 | ms/batch 24.23 | loss 0.07342403\n",
      "| Epoch  92 |   100/  258 batches | lr 0.00019 | ms/batch 22.32 | loss 0.07133781\n",
      "| Epoch  92 |   150/  258 batches | lr 0.00019 | ms/batch 23.10 | loss 0.07024666\n",
      "| Epoch  92 |   200/  258 batches | lr 0.00019 | ms/batch 21.87 | loss 0.06979116\n",
      "| Epoch  92 |   250/  258 batches | lr 0.00019 | ms/batch 19.96 | loss 0.06976962\n",
      "\n",
      "Val set: Average loss: 0.06796796\n",
      "\n",
      "| Epoch  93 |    50/  258 batches | lr 0.00019 | ms/batch 21.95 | loss 0.07327794\n",
      "| Epoch  93 |   100/  258 batches | lr 0.00019 | ms/batch 21.95 | loss 0.07122400\n",
      "| Epoch  93 |   150/  258 batches | lr 0.00019 | ms/batch 22.04 | loss 0.07006734\n",
      "| Epoch  93 |   200/  258 batches | lr 0.00019 | ms/batch 22.40 | loss 0.06965594\n",
      "| Epoch  93 |   250/  258 batches | lr 0.00019 | ms/batch 19.88 | loss 0.06966055\n",
      "\n",
      "Val set: Average loss: 0.06784228\n",
      "\n",
      "| Epoch  94 |    50/  258 batches | lr 0.00019 | ms/batch 22.34 | loss 0.07312713\n",
      "| Epoch  94 |   100/  258 batches | lr 0.00019 | ms/batch 22.62 | loss 0.07110692\n",
      "| Epoch  94 |   150/  258 batches | lr 0.00019 | ms/batch 26.71 | loss 0.06988983\n",
      "| Epoch  94 |   200/  258 batches | lr 0.00019 | ms/batch 23.31 | loss 0.06953627\n",
      "| Epoch  94 |   250/  258 batches | lr 0.00019 | ms/batch 22.99 | loss 0.06954414\n",
      "\n",
      "Val set: Average loss: 0.06769245\n",
      "\n",
      "| Epoch  95 |    50/  258 batches | lr 0.00019 | ms/batch 22.63 | loss 0.07296616\n",
      "| Epoch  95 |   100/  258 batches | lr 0.00019 | ms/batch 21.69 | loss 0.07098649\n",
      "| Epoch  95 |   150/  258 batches | lr 0.00019 | ms/batch 22.44 | loss 0.06970894\n",
      "| Epoch  95 |   200/  258 batches | lr 0.00019 | ms/batch 21.67 | loss 0.06942380\n",
      "| Epoch  95 |   250/  258 batches | lr 0.00019 | ms/batch 22.65 | loss 0.06942624\n",
      "\n",
      "Val set: Average loss: 0.06753681\n",
      "\n",
      "| Epoch  96 |    50/  258 batches | lr 0.00019 | ms/batch 22.93 | loss 0.07278886\n",
      "| Epoch  96 |   100/  258 batches | lr 0.00019 | ms/batch 22.40 | loss 0.07087160\n",
      "| Epoch  96 |   150/  258 batches | lr 0.00019 | ms/batch 19.51 | loss 0.06952405\n",
      "| Epoch  96 |   200/  258 batches | lr 0.00019 | ms/batch 21.22 | loss 0.06932153\n",
      "| Epoch  96 |   250/  258 batches | lr 0.00019 | ms/batch 21.29 | loss 0.06930811\n",
      "\n",
      "Val set: Average loss: 0.06739039\n",
      "\n",
      "| Epoch  97 |    50/  258 batches | lr 0.00019 | ms/batch 21.93 | loss 0.07260212\n",
      "| Epoch  97 |   100/  258 batches | lr 0.00019 | ms/batch 21.46 | loss 0.07076509\n",
      "| Epoch  97 |   150/  258 batches | lr 0.00019 | ms/batch 19.63 | loss 0.06931629\n",
      "| Epoch  97 |   200/  258 batches | lr 0.00019 | ms/batch 21.29 | loss 0.06923228\n",
      "| Epoch  97 |   250/  258 batches | lr 0.00019 | ms/batch 21.55 | loss 0.06918135\n",
      "\n",
      "Val set: Average loss: 0.06725276\n",
      "\n",
      "| Epoch  98 |    50/  258 batches | lr 0.00019 | ms/batch 21.81 | loss 0.07242661\n",
      "| Epoch  98 |   100/  258 batches | lr 0.00019 | ms/batch 21.46 | loss 0.07066555\n",
      "| Epoch  98 |   150/  258 batches | lr 0.00019 | ms/batch 21.25 | loss 0.06911226\n",
      "| Epoch  98 |   200/  258 batches | lr 0.00019 | ms/batch 19.60 | loss 0.06916547\n",
      "| Epoch  98 |   250/  258 batches | lr 0.00019 | ms/batch 21.34 | loss 0.06905319\n",
      "\n",
      "Val set: Average loss: 0.06713455\n",
      "\n",
      "| Epoch  99 |    50/  258 batches | lr 0.00019 | ms/batch 21.54 | loss 0.07227517\n",
      "| Epoch  99 |   100/  258 batches | lr 0.00019 | ms/batch 19.39 | loss 0.07057222\n",
      "| Epoch  99 |   150/  258 batches | lr 0.00019 | ms/batch 21.23 | loss 0.06887798\n",
      "| Epoch  99 |   200/  258 batches | lr 0.00019 | ms/batch 21.64 | loss 0.06912175\n",
      "| Epoch  99 |   250/  258 batches | lr 0.00019 | ms/batch 21.43 | loss 0.06892058\n",
      "\n",
      "Val set: Average loss: 0.06705979\n",
      "\n",
      "| Epoch 100 |    50/  258 batches | lr 0.00019 | ms/batch 21.81 | loss 0.07213715\n",
      "| Epoch 100 |   100/  258 batches | lr 0.00019 | ms/batch 21.31 | loss 0.07047984\n",
      "| Epoch 100 |   150/  258 batches | lr 0.00019 | ms/batch 21.51 | loss 0.06859496\n",
      "| Epoch 100 |   200/  258 batches | lr 0.00019 | ms/batch 19.47 | loss 0.06909054\n",
      "| Epoch 100 |   250/  258 batches | lr 0.00019 | ms/batch 21.46 | loss 0.06878200\n",
      "\n",
      "Val set: Average loss: 0.06702439\n",
      "\n",
      "| Epoch 101 |    50/  258 batches | lr 0.00019 | ms/batch 21.81 | loss 0.07200547\n",
      "| Epoch 101 |   100/  258 batches | lr 0.00019 | ms/batch 19.40 | loss 0.07038109\n",
      "| Epoch 101 |   150/  258 batches | lr 0.00019 | ms/batch 21.86 | loss 0.06830081\n",
      "| Epoch 101 |   200/  258 batches | lr 0.00019 | ms/batch 19.41 | loss 0.06903364\n",
      "| Epoch 101 |   250/  258 batches | lr 0.00019 | ms/batch 21.17 | loss 0.06864763\n",
      "\n",
      "Val set: Average loss: 0.06699446\n",
      "\n",
      "| Epoch 102 |    50/  258 batches | lr 0.00019 | ms/batch 21.80 | loss 0.07187247\n",
      "| Epoch 102 |   100/  258 batches | lr 0.00019 | ms/batch 21.50 | loss 0.07028320\n",
      "| Epoch 102 |   150/  258 batches | lr 0.00019 | ms/batch 21.36 | loss 0.06803224\n",
      "| Epoch 102 |   200/  258 batches | lr 0.00019 | ms/batch 21.38 | loss 0.06896219\n",
      "| Epoch 102 |   250/  258 batches | lr 0.00019 | ms/batch 21.56 | loss 0.06851295\n",
      "\n",
      "Val set: Average loss: 0.06695502\n",
      "\n",
      "| Epoch 103 |    50/  258 batches | lr 0.00019 | ms/batch 21.80 | loss 0.07174642\n",
      "| Epoch 103 |   100/  258 batches | lr 0.00019 | ms/batch 19.66 | loss 0.07018583\n",
      "| Epoch 103 |   150/  258 batches | lr 0.00019 | ms/batch 21.58 | loss 0.06780236\n",
      "| Epoch 103 |   200/  258 batches | lr 0.00019 | ms/batch 21.14 | loss 0.06887616\n",
      "| Epoch 103 |   250/  258 batches | lr 0.00019 | ms/batch 21.23 | loss 0.06837467\n",
      "\n",
      "Val set: Average loss: 0.06690679\n",
      "\n",
      "| Epoch 104 |    50/  258 batches | lr 0.00019 | ms/batch 21.64 | loss 0.07163460\n",
      "| Epoch 104 |   100/  258 batches | lr 0.00019 | ms/batch 19.45 | loss 0.07009386\n",
      "| Epoch 104 |   150/  258 batches | lr 0.00019 | ms/batch 21.33 | loss 0.06759975\n",
      "| Epoch 104 |   200/  258 batches | lr 0.00019 | ms/batch 21.34 | loss 0.06879118\n",
      "| Epoch 104 |   250/  258 batches | lr 0.00019 | ms/batch 21.61 | loss 0.06823669\n",
      "\n",
      "Val set: Average loss: 0.06683630\n",
      "\n",
      "| Epoch 105 |    50/  258 batches | lr 0.00019 | ms/batch 21.66 | loss 0.07153473\n",
      "| Epoch 105 |   100/  258 batches | lr 0.00019 | ms/batch 19.58 | loss 0.06999566\n",
      "| Epoch 105 |   150/  258 batches | lr 0.00019 | ms/batch 21.19 | loss 0.06744061\n",
      "| Epoch 105 |   200/  258 batches | lr 0.00019 | ms/batch 21.44 | loss 0.06866893\n",
      "| Epoch 105 |   250/  258 batches | lr 0.00019 | ms/batch 21.24 | loss 0.06810406\n",
      "\n",
      "Val set: Average loss: 0.06675881\n",
      "\n",
      "| Epoch 106 |    50/  258 batches | lr 0.00019 | ms/batch 21.72 | loss 0.07143992\n",
      "| Epoch 106 |   100/  258 batches | lr 0.00019 | ms/batch 19.36 | loss 0.06990027\n",
      "| Epoch 106 |   150/  258 batches | lr 0.00019 | ms/batch 21.39 | loss 0.06731803\n",
      "| Epoch 106 |   200/  258 batches | lr 0.00019 | ms/batch 21.55 | loss 0.06850749\n",
      "| Epoch 106 |   250/  258 batches | lr 0.00019 | ms/batch 21.23 | loss 0.06797501\n",
      "\n",
      "Val set: Average loss: 0.06667834\n",
      "\n",
      "| Epoch 107 |    50/  258 batches | lr 0.00019 | ms/batch 21.78 | loss 0.07135862\n",
      "| Epoch 107 |   100/  258 batches | lr 0.00019 | ms/batch 19.40 | loss 0.06980109\n",
      "| Epoch 107 |   150/  258 batches | lr 0.00019 | ms/batch 21.37 | loss 0.06720061\n",
      "| Epoch 107 |   200/  258 batches | lr 0.00019 | ms/batch 21.24 | loss 0.06833904\n",
      "| Epoch 107 |   250/  258 batches | lr 0.00019 | ms/batch 21.33 | loss 0.06785395\n",
      "\n",
      "Val set: Average loss: 0.06659931\n",
      "\n",
      "| Epoch 108 |    50/  258 batches | lr 0.00019 | ms/batch 21.57 | loss 0.07128570\n",
      "| Epoch 108 |   100/  258 batches | lr 0.00019 | ms/batch 21.51 | loss 0.06970748\n",
      "| Epoch 108 |   150/  258 batches | lr 0.00019 | ms/batch 19.55 | loss 0.06709054\n",
      "| Epoch 108 |   200/  258 batches | lr 0.00019 | ms/batch 21.52 | loss 0.06819075\n",
      "| Epoch 108 |   250/  258 batches | lr 0.00019 | ms/batch 21.25 | loss 0.06773897\n",
      "\n",
      "Val set: Average loss: 0.06653205\n",
      "\n",
      "| Epoch 109 |    50/  258 batches | lr 0.00019 | ms/batch 21.73 | loss 0.07121004\n",
      "| Epoch 109 |   100/  258 batches | lr 0.00019 | ms/batch 21.32 | loss 0.06962378\n",
      "| Epoch 109 |   150/  258 batches | lr 0.00019 | ms/batch 21.29 | loss 0.06699232\n",
      "| Epoch 109 |   200/  258 batches | lr 0.00019 | ms/batch 19.45 | loss 0.06805232\n",
      "| Epoch 109 |   250/  258 batches | lr 0.00019 | ms/batch 21.12 | loss 0.06762215\n",
      "\n",
      "Val set: Average loss: 0.06646902\n",
      "\n",
      "| Epoch 110 |    50/  258 batches | lr 0.00019 | ms/batch 21.67 | loss 0.07112725\n",
      "| Epoch 110 |   100/  258 batches | lr 0.00019 | ms/batch 21.48 | loss 0.06955466\n",
      "| Epoch 110 |   150/  258 batches | lr 0.00019 | ms/batch 19.49 | loss 0.06688798\n",
      "| Epoch 110 |   200/  258 batches | lr 0.00019 | ms/batch 21.29 | loss 0.06795063\n",
      "| Epoch 110 |   250/  258 batches | lr 0.00019 | ms/batch 21.49 | loss 0.06749838\n",
      "\n",
      "Val set: Average loss: 0.06639946\n",
      "\n",
      "| Epoch 111 |    50/  258 batches | lr 0.00019 | ms/batch 19.91 | loss 0.07102536\n",
      "| Epoch 111 |   100/  258 batches | lr 0.00019 | ms/batch 21.18 | loss 0.06949092\n",
      "| Epoch 111 |   150/  258 batches | lr 0.00019 | ms/batch 21.59 | loss 0.06679145\n",
      "| Epoch 111 |   200/  258 batches | lr 0.00019 | ms/batch 21.42 | loss 0.06786025\n",
      "| Epoch 111 |   250/  258 batches | lr 0.00019 | ms/batch 19.71 | loss 0.06737310\n",
      "\n",
      "Val set: Average loss: 0.06634527\n",
      "\n",
      "| Epoch 112 |    50/  258 batches | lr 0.00019 | ms/batch 21.68 | loss 0.07090742\n",
      "| Epoch 112 |   100/  258 batches | lr 0.00019 | ms/batch 19.56 | loss 0.06943943\n",
      "| Epoch 112 |   150/  258 batches | lr 0.00019 | ms/batch 21.05 | loss 0.06670137\n",
      "| Epoch 112 |   200/  258 batches | lr 0.00019 | ms/batch 21.36 | loss 0.06777900\n",
      "| Epoch 112 |   250/  258 batches | lr 0.00019 | ms/batch 19.41 | loss 0.06725129\n",
      "\n",
      "Val set: Average loss: 0.06630526\n",
      "\n",
      "| Epoch 113 |    50/  258 batches | lr 0.00019 | ms/batch 21.59 | loss 0.07077785\n",
      "| Epoch 113 |   100/  258 batches | lr 0.00019 | ms/batch 19.36 | loss 0.06939452\n",
      "| Epoch 113 |   150/  258 batches | lr 0.00019 | ms/batch 23.19 | loss 0.06662471\n",
      "| Epoch 113 |   200/  258 batches | lr 0.00019 | ms/batch 21.33 | loss 0.06770975\n",
      "| Epoch 113 |   250/  258 batches | lr 0.00019 | ms/batch 21.37 | loss 0.06713150\n",
      "\n",
      "Val set: Average loss: 0.06625631\n",
      "\n",
      "| Epoch 114 |    50/  258 batches | lr 0.00019 | ms/batch 21.62 | loss 0.07062841\n",
      "| Epoch 114 |   100/  258 batches | lr 0.00019 | ms/batch 19.51 | loss 0.06935248\n",
      "| Epoch 114 |   150/  258 batches | lr 0.00019 | ms/batch 21.20 | loss 0.06655078\n",
      "| Epoch 114 |   200/  258 batches | lr 0.00019 | ms/batch 21.60 | loss 0.06762962\n",
      "| Epoch 114 |   250/  258 batches | lr 0.00019 | ms/batch 21.32 | loss 0.06701839\n",
      "\n",
      "Val set: Average loss: 0.06617504\n",
      "\n",
      "| Epoch 115 |    50/  258 batches | lr 0.00019 | ms/batch 21.79 | loss 0.07047168\n",
      "| Epoch 115 |   100/  258 batches | lr 0.00019 | ms/batch 21.59 | loss 0.06931673\n",
      "| Epoch 115 |   150/  258 batches | lr 0.00019 | ms/batch 19.79 | loss 0.06648014\n",
      "| Epoch 115 |   200/  258 batches | lr 0.00019 | ms/batch 21.43 | loss 0.06753496\n",
      "| Epoch 115 |   250/  258 batches | lr 0.00019 | ms/batch 19.77 | loss 0.06691024\n",
      "\n",
      "Val set: Average loss: 0.06607258\n",
      "\n",
      "| Epoch 116 |    50/  258 batches | lr 0.00019 | ms/batch 21.98 | loss 0.07034257\n",
      "| Epoch 116 |   100/  258 batches | lr 0.00019 | ms/batch 19.50 | loss 0.06927905\n",
      "| Epoch 116 |   150/  258 batches | lr 0.00019 | ms/batch 23.67 | loss 0.06642186\n",
      "| Epoch 116 |   200/  258 batches | lr 0.00019 | ms/batch 19.66 | loss 0.06742886\n",
      "| Epoch 116 |   250/  258 batches | lr 0.00019 | ms/batch 21.88 | loss 0.06680241\n",
      "\n",
      "Val set: Average loss: 0.06595621\n",
      "\n",
      "| Epoch 117 |    50/  258 batches | lr 0.00019 | ms/batch 21.94 | loss 0.07022115\n",
      "| Epoch 117 |   100/  258 batches | lr 0.00019 | ms/batch 19.44 | loss 0.06922117\n",
      "| Epoch 117 |   150/  258 batches | lr 0.00019 | ms/batch 21.26 | loss 0.06641061\n",
      "| Epoch 117 |   200/  258 batches | lr 0.00019 | ms/batch 21.46 | loss 0.06733561\n",
      "| Epoch 117 |   250/  258 batches | lr 0.00019 | ms/batch 19.62 | loss 0.06665871\n",
      "\n",
      "Val set: Average loss: 0.06586198\n",
      "\n",
      "| Epoch 118 |    50/  258 batches | lr 0.00019 | ms/batch 21.60 | loss 0.07009265\n",
      "| Epoch 118 |   100/  258 batches | lr 0.00019 | ms/batch 19.48 | loss 0.06915871\n",
      "| Epoch 118 |   150/  258 batches | lr 0.00019 | ms/batch 23.23 | loss 0.06635143\n",
      "| Epoch 118 |   200/  258 batches | lr 0.00019 | ms/batch 19.58 | loss 0.06723535\n",
      "| Epoch 118 |   250/  258 batches | lr 0.00019 | ms/batch 21.20 | loss 0.06653874\n",
      "\n",
      "Val set: Average loss: 0.06577768\n",
      "\n",
      "| Epoch 119 |    50/  258 batches | lr 0.00019 | ms/batch 19.91 | loss 0.06996868\n",
      "| Epoch 119 |   100/  258 batches | lr 0.00019 | ms/batch 21.35 | loss 0.06909101\n",
      "| Epoch 119 |   150/  258 batches | lr 0.00019 | ms/batch 21.45 | loss 0.06624082\n",
      "| Epoch 119 |   200/  258 batches | lr 0.00019 | ms/batch 21.40 | loss 0.06714285\n",
      "| Epoch 119 |   250/  258 batches | lr 0.00019 | ms/batch 21.53 | loss 0.06643842\n",
      "\n",
      "Val set: Average loss: 0.06572535\n",
      "\n",
      "| Epoch 120 |    50/  258 batches | lr 0.00019 | ms/batch 19.73 | loss 0.06980163\n",
      "| Epoch 120 |   100/  258 batches | lr 0.00019 | ms/batch 21.38 | loss 0.06899307\n",
      "| Epoch 120 |   150/  258 batches | lr 0.00019 | ms/batch 21.27 | loss 0.06617529\n",
      "| Epoch 120 |   200/  258 batches | lr 0.00019 | ms/batch 21.36 | loss 0.06721519\n",
      "| Epoch 120 |   250/  258 batches | lr 0.00019 | ms/batch 21.17 | loss 0.06631996\n",
      "\n",
      "Val set: Average loss: 0.06572001\n",
      "\n",
      "| Epoch 121 |    50/  258 batches | lr 0.00019 | ms/batch 21.48 | loss 0.06993196\n",
      "| Epoch 121 |   100/  258 batches | lr 0.00019 | ms/batch 21.64 | loss 0.06892127\n",
      "| Epoch 121 |   150/  258 batches | lr 0.00019 | ms/batch 21.19 | loss 0.06614872\n",
      "| Epoch 121 |   200/  258 batches | lr 0.00019 | ms/batch 21.37 | loss 0.06721614\n",
      "| Epoch 121 |   250/  258 batches | lr 0.00019 | ms/batch 19.61 | loss 0.06626825\n",
      "\n",
      "Val set: Average loss: 0.06573303\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 122 |    50/  258 batches | lr 0.00019 | ms/batch 21.99 | loss 0.07090091\n",
      "| Epoch 122 |   100/  258 batches | lr 0.00019 | ms/batch 21.25 | loss 0.06914053\n",
      "| Epoch 122 |   150/  258 batches | lr 0.00019 | ms/batch 21.52 | loss 0.06679302\n",
      "| Epoch 122 |   200/  258 batches | lr 0.00019 | ms/batch 19.51 | loss 0.06689879\n",
      "| Epoch 122 |   250/  258 batches | lr 0.00019 | ms/batch 21.28 | loss 0.06623517\n",
      "\n",
      "Val set: Average loss: 0.06547560\n",
      "\n",
      "| Epoch 123 |    50/  258 batches | lr 0.00019 | ms/batch 19.89 | loss 0.07052886\n",
      "| Epoch 123 |   100/  258 batches | lr 0.00019 | ms/batch 23.07 | loss 0.06878456\n",
      "| Epoch 123 |   150/  258 batches | lr 0.00019 | ms/batch 19.55 | loss 0.06650140\n",
      "| Epoch 123 |   200/  258 batches | lr 0.00019 | ms/batch 21.48 | loss 0.06671342\n",
      "| Epoch 123 |   250/  258 batches | lr 0.00019 | ms/batch 21.64 | loss 0.06615897\n",
      "\n",
      "Val set: Average loss: 0.06529436\n",
      "\n",
      "| Epoch 124 |    50/  258 batches | lr 0.00019 | ms/batch 19.98 | loss 0.07022537\n",
      "| Epoch 124 |   100/  258 batches | lr 0.00019 | ms/batch 21.14 | loss 0.06853698\n",
      "| Epoch 124 |   150/  258 batches | lr 0.00019 | ms/batch 19.48 | loss 0.06577646\n",
      "| Epoch 124 |   200/  258 batches | lr 0.00019 | ms/batch 23.16 | loss 0.06666884\n",
      "| Epoch 124 |   250/  258 batches | lr 0.00019 | ms/batch 19.41 | loss 0.06601067\n",
      "\n",
      "Val set: Average loss: 0.06525854\n",
      "\n",
      "| Epoch 125 |    50/  258 batches | lr 0.00019 | ms/batch 20.08 | loss 0.07014148\n",
      "| Epoch 125 |   100/  258 batches | lr 0.00019 | ms/batch 21.35 | loss 0.06842900\n",
      "| Epoch 125 |   150/  258 batches | lr 0.00019 | ms/batch 21.44 | loss 0.06565261\n",
      "| Epoch 125 |   200/  258 batches | lr 0.00019 | ms/batch 21.66 | loss 0.06654656\n",
      "| Epoch 125 |   250/  258 batches | lr 0.00019 | ms/batch 24.75 | loss 0.06592365\n",
      "\n",
      "Val set: Average loss: 0.06519188\n",
      "\n",
      "| Epoch 126 |    50/  258 batches | lr 0.00019 | ms/batch 20.54 | loss 0.07004566\n",
      "| Epoch 126 |   100/  258 batches | lr 0.00019 | ms/batch 21.99 | loss 0.06831391\n",
      "| Epoch 126 |   150/  258 batches | lr 0.00019 | ms/batch 23.78 | loss 0.06548266\n",
      "| Epoch 126 |   200/  258 batches | lr 0.00019 | ms/batch 21.61 | loss 0.06639280\n",
      "| Epoch 126 |   250/  258 batches | lr 0.00019 | ms/batch 21.27 | loss 0.06586226\n",
      "\n",
      "Val set: Average loss: 0.06512676\n",
      "\n",
      "| Epoch 127 |    50/  258 batches | lr 0.00019 | ms/batch 21.81 | loss 0.06997516\n",
      "| Epoch 127 |   100/  258 batches | lr 0.00019 | ms/batch 21.59 | loss 0.06821585\n",
      "| Epoch 127 |   150/  258 batches | lr 0.00019 | ms/batch 21.19 | loss 0.06538754\n",
      "| Epoch 127 |   200/  258 batches | lr 0.00019 | ms/batch 21.22 | loss 0.06625483\n",
      "| Epoch 127 |   250/  258 batches | lr 0.00019 | ms/batch 21.43 | loss 0.06580571\n",
      "\n",
      "Val set: Average loss: 0.06504686\n",
      "\n",
      "| Epoch 128 |    50/  258 batches | lr 0.00019 | ms/batch 19.99 | loss 0.06988065\n",
      "| Epoch 128 |   100/  258 batches | lr 0.00019 | ms/batch 21.35 | loss 0.06810743\n",
      "| Epoch 128 |   150/  258 batches | lr 0.00019 | ms/batch 21.48 | loss 0.06527216\n",
      "| Epoch 128 |   200/  258 batches | lr 0.00019 | ms/batch 21.39 | loss 0.06612643\n",
      "| Epoch 128 |   250/  258 batches | lr 0.00019 | ms/batch 21.31 | loss 0.06577083\n",
      "\n",
      "Val set: Average loss: 0.06496956\n",
      "\n",
      "| Epoch 129 |    50/  258 batches | lr 0.00019 | ms/batch 21.74 | loss 0.06978286\n",
      "| Epoch 129 |   100/  258 batches | lr 0.00019 | ms/batch 21.20 | loss 0.06799546\n",
      "| Epoch 129 |   150/  258 batches | lr 0.00019 | ms/batch 21.41 | loss 0.06517337\n",
      "| Epoch 129 |   200/  258 batches | lr 0.00019 | ms/batch 19.48 | loss 0.06596874\n",
      "| Epoch 129 |   250/  258 batches | lr 0.00019 | ms/batch 21.67 | loss 0.06576526\n",
      "\n",
      "Val set: Average loss: 0.06489863\n",
      "\n",
      "| Epoch 130 |    50/  258 batches | lr 0.00019 | ms/batch 21.54 | loss 0.06965819\n",
      "| Epoch 130 |   100/  258 batches | lr 0.00019 | ms/batch 21.43 | loss 0.06791779\n",
      "| Epoch 130 |   150/  258 batches | lr 0.00019 | ms/batch 19.52 | loss 0.06511074\n",
      "| Epoch 130 |   200/  258 batches | lr 0.00019 | ms/batch 21.27 | loss 0.06572493\n",
      "| Epoch 130 |   250/  258 batches | lr 0.00019 | ms/batch 19.58 | loss 0.06582925\n",
      "\n",
      "Val set: Average loss: 0.06485135\n",
      "\n",
      "| Epoch 131 |    50/  258 batches | lr 0.00019 | ms/batch 21.73 | loss 0.06928212\n",
      "| Epoch 131 |   100/  258 batches | lr 0.00019 | ms/batch 19.57 | loss 0.06799157\n",
      "| Epoch 131 |   150/  258 batches | lr 0.00019 | ms/batch 23.09 | loss 0.06510068\n",
      "| Epoch 131 |   200/  258 batches | lr 0.00019 | ms/batch 19.59 | loss 0.06549799\n",
      "| Epoch 131 |   250/  258 batches | lr 0.00019 | ms/batch 21.38 | loss 0.06582332\n",
      "\n",
      "Val set: Average loss: 0.06465249\n",
      "\n",
      "| Epoch 132 |    50/  258 batches | lr 0.00019 | ms/batch 22.54 | loss 0.06913800\n",
      "| Epoch 132 |   100/  258 batches | lr 0.00019 | ms/batch 20.74 | loss 0.06774901\n",
      "| Epoch 132 |   150/  258 batches | lr 0.00019 | ms/batch 22.25 | loss 0.06476097\n",
      "| Epoch 132 |   200/  258 batches | lr 0.00019 | ms/batch 21.71 | loss 0.06540208\n",
      "| Epoch 132 |   250/  258 batches | lr 0.00019 | ms/batch 21.84 | loss 0.06577862\n",
      "\n",
      "Val set: Average loss: 0.06455725\n",
      "\n",
      "| Epoch 133 |    50/  258 batches | lr 0.00019 | ms/batch 20.08 | loss 0.06900345\n",
      "| Epoch 133 |   100/  258 batches | lr 0.00019 | ms/batch 21.56 | loss 0.06763101\n",
      "| Epoch 133 |   150/  258 batches | lr 0.00019 | ms/batch 21.36 | loss 0.06458466\n",
      "| Epoch 133 |   200/  258 batches | lr 0.00019 | ms/batch 21.42 | loss 0.06530452\n",
      "| Epoch 133 |   250/  258 batches | lr 0.00019 | ms/batch 21.29 | loss 0.06570625\n",
      "\n",
      "Val set: Average loss: 0.06446399\n",
      "\n",
      "| Epoch 134 |    50/  258 batches | lr 0.00019 | ms/batch 21.62 | loss 0.06887117\n",
      "| Epoch 134 |   100/  258 batches | lr 0.00019 | ms/batch 21.37 | loss 0.06754809\n",
      "| Epoch 134 |   150/  258 batches | lr 0.00019 | ms/batch 21.48 | loss 0.06449275\n",
      "| Epoch 134 |   200/  258 batches | lr 0.00019 | ms/batch 21.28 | loss 0.06516856\n",
      "| Epoch 134 |   250/  258 batches | lr 0.00019 | ms/batch 21.43 | loss 0.06561784\n",
      "\n",
      "Val set: Average loss: 0.06436222\n",
      "\n",
      "| Epoch 135 |    50/  258 batches | lr 0.00019 | ms/batch 21.78 | loss 0.06869185\n",
      "| Epoch 135 |   100/  258 batches | lr 0.00019 | ms/batch 22.07 | loss 0.06754342\n",
      "| Epoch 135 |   150/  258 batches | lr 0.00019 | ms/batch 22.07 | loss 0.06437486\n",
      "| Epoch 135 |   200/  258 batches | lr 0.00019 | ms/batch 20.20 | loss 0.06501929\n",
      "| Epoch 135 |   250/  258 batches | lr 0.00019 | ms/batch 22.62 | loss 0.06552408\n",
      "\n",
      "Val set: Average loss: 0.06426013\n",
      "\n",
      "| Epoch 136 |    50/  258 batches | lr 0.00019 | ms/batch 24.89 | loss 0.06855922\n",
      "| Epoch 136 |   100/  258 batches | lr 0.00019 | ms/batch 23.74 | loss 0.06762097\n",
      "| Epoch 136 |   150/  258 batches | lr 0.00019 | ms/batch 23.61 | loss 0.06444588\n",
      "| Epoch 136 |   200/  258 batches | lr 0.00019 | ms/batch 23.94 | loss 0.06478539\n",
      "| Epoch 136 |   250/  258 batches | lr 0.00019 | ms/batch 23.87 | loss 0.06543401\n",
      "\n",
      "Val set: Average loss: 0.06419370\n",
      "\n",
      "| Epoch 137 |    50/  258 batches | lr 0.00019 | ms/batch 23.70 | loss 0.06847826\n",
      "| Epoch 137 |   100/  258 batches | lr 0.00019 | ms/batch 23.12 | loss 0.06758134\n",
      "| Epoch 137 |   150/  258 batches | lr 0.00019 | ms/batch 23.90 | loss 0.06448504\n",
      "| Epoch 137 |   200/  258 batches | lr 0.00019 | ms/batch 20.99 | loss 0.06465418\n",
      "| Epoch 137 |   250/  258 batches | lr 0.00019 | ms/batch 22.54 | loss 0.06533086\n",
      "\n",
      "Val set: Average loss: 0.06414944\n",
      "\n",
      "| Epoch 138 |    50/  258 batches | lr 0.00019 | ms/batch 22.28 | loss 0.06842573\n",
      "| Epoch 138 |   100/  258 batches | lr 0.00019 | ms/batch 21.93 | loss 0.06743968\n",
      "| Epoch 138 |   150/  258 batches | lr 0.00019 | ms/batch 21.55 | loss 0.06451906\n",
      "| Epoch 138 |   200/  258 batches | lr 0.00019 | ms/batch 22.05 | loss 0.06455560\n",
      "| Epoch 138 |   250/  258 batches | lr 0.00019 | ms/batch 21.90 | loss 0.06523215\n",
      "\n",
      "Val set: Average loss: 0.06410770\n",
      "\n",
      "| Epoch 139 |    50/  258 batches | lr 0.00019 | ms/batch 22.10 | loss 0.06840354\n",
      "| Epoch 139 |   100/  258 batches | lr 0.00019 | ms/batch 22.93 | loss 0.06732445\n",
      "| Epoch 139 |   150/  258 batches | lr 0.00019 | ms/batch 21.93 | loss 0.06452878\n",
      "| Epoch 139 |   200/  258 batches | lr 0.00019 | ms/batch 22.47 | loss 0.06444263\n",
      "| Epoch 139 |   250/  258 batches | lr 0.00019 | ms/batch 22.35 | loss 0.06511098\n",
      "\n",
      "Val set: Average loss: 0.06411706\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 140 |    50/  258 batches | lr 0.00019 | ms/batch 21.84 | loss 0.06841546\n",
      "| Epoch 140 |   100/  258 batches | lr 0.00019 | ms/batch 21.58 | loss 0.06730483\n",
      "| Epoch 140 |   150/  258 batches | lr 0.00019 | ms/batch 21.44 | loss 0.06441127\n",
      "| Epoch 140 |   200/  258 batches | lr 0.00019 | ms/batch 19.62 | loss 0.06432331\n",
      "| Epoch 140 |   250/  258 batches | lr 0.00019 | ms/batch 23.16 | loss 0.06500349\n",
      "\n",
      "Val set: Average loss: 0.06415655\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch 141 |    50/  258 batches | lr 0.00019 | ms/batch 21.56 | loss 0.06842621\n",
      "| Epoch 141 |   100/  258 batches | lr 0.00019 | ms/batch 21.46 | loss 0.06728771\n",
      "| Epoch 141 |   150/  258 batches | lr 0.00019 | ms/batch 21.36 | loss 0.06449965\n",
      "| Epoch 141 |   200/  258 batches | lr 0.00019 | ms/batch 21.24 | loss 0.06421737\n",
      "| Epoch 141 |   250/  258 batches | lr 0.00019 | ms/batch 21.32 | loss 0.06483201\n",
      "\n",
      "Val set: Average loss: 0.06414759\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch 142 |    50/  258 batches | lr 0.00019 | ms/batch 21.93 | loss 0.06825630\n",
      "| Epoch 142 |   100/  258 batches | lr 0.00019 | ms/batch 21.22 | loss 0.06713108\n",
      "| Epoch 142 |   150/  258 batches | lr 0.00019 | ms/batch 21.48 | loss 0.06430174\n",
      "| Epoch 142 |   200/  258 batches | lr 0.00019 | ms/batch 19.50 | loss 0.06406806\n",
      "| Epoch 142 |   250/  258 batches | lr 0.00019 | ms/batch 21.27 | loss 0.06473786\n",
      "\n",
      "Val set: Average loss: 0.06419705\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch 143 |    50/  258 batches | lr 0.00019 | ms/batch 21.67 | loss 0.06815285\n",
      "| Epoch 143 |   100/  258 batches | lr 0.00019 | ms/batch 19.50 | loss 0.06705358\n",
      "| Epoch 143 |   150/  258 batches | lr 0.00019 | ms/batch 21.40 | loss 0.06407277\n",
      "| Epoch 143 |   200/  258 batches | lr 0.00019 | ms/batch 21.78 | loss 0.06398107\n",
      "| Epoch 143 |   250/  258 batches | lr 0.00019 | ms/batch 21.38 | loss 0.06466733\n",
      "\n",
      "Val set: Average loss: 0.06422192\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch 144 |    50/  258 batches | lr 0.00019 | ms/batch 21.46 | loss 0.06803740\n",
      "| Epoch 144 |   100/  258 batches | lr 0.00019 | ms/batch 21.39 | loss 0.06698787\n",
      "| Epoch 144 |   150/  258 batches | lr 0.00019 | ms/batch 21.27 | loss 0.06385122\n",
      "| Epoch 144 |   200/  258 batches | lr 0.00019 | ms/batch 19.39 | loss 0.06411447\n",
      "| Epoch 144 |   250/  258 batches | lr 0.00019 | ms/batch 21.27 | loss 0.06459006\n",
      "\n",
      "Val set: Average loss: 0.06427162\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch 145 |    50/  258 batches | lr 0.00019 | ms/batch 21.75 | loss 0.06802256\n",
      "| Epoch 145 |   100/  258 batches | lr 0.00019 | ms/batch 19.42 | loss 0.06691910\n",
      "| Epoch 145 |   150/  258 batches | lr 0.00019 | ms/batch 21.35 | loss 0.06368695\n",
      "| Epoch 145 |   200/  258 batches | lr 0.00019 | ms/batch 21.31 | loss 0.06389135\n",
      "| Epoch 145 |   250/  258 batches | lr 0.00019 | ms/batch 21.40 | loss 0.06452455\n",
      "\n",
      "Val set: Average loss: 0.06434420\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch 146 |    50/  258 batches | lr 0.00019 | ms/batch 23.71 | loss 0.06795906\n",
      "| Epoch 146 |   100/  258 batches | lr 0.00019 | ms/batch 21.30 | loss 0.06689061\n",
      "| Epoch 146 |   150/  258 batches | lr 0.00019 | ms/batch 19.48 | loss 0.06358071\n",
      "| Epoch 146 |   200/  258 batches | lr 0.00019 | ms/batch 21.33 | loss 0.06383757\n",
      "| Epoch 146 |   250/  258 batches | lr 0.00019 | ms/batch 19.46 | loss 0.06446744\n",
      "\n",
      "Val set: Average loss: 0.06438460\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch 147 |    50/  258 batches | lr 0.00019 | ms/batch 21.56 | loss 0.06790097\n",
      "| Epoch 147 |   100/  258 batches | lr 0.00019 | ms/batch 22.05 | loss 0.06687740\n",
      "| Epoch 147 |   150/  258 batches | lr 0.00019 | ms/batch 22.11 | loss 0.06350094\n",
      "| Epoch 147 |   200/  258 batches | lr 0.00019 | ms/batch 20.22 | loss 0.06375068\n",
      "| Epoch 147 |   250/  258 batches | lr 0.00019 | ms/batch 21.67 | loss 0.06440995\n",
      "\n",
      "Val set: Average loss: 0.06440992\n",
      "\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch 148 |    50/  258 batches | lr 0.00019 | ms/batch 22.19 | loss 0.06783116\n",
      "| Epoch 148 |   100/  258 batches | lr 0.00019 | ms/batch 21.51 | loss 0.06685424\n",
      "| Epoch 148 |   150/  258 batches | lr 0.00019 | ms/batch 19.51 | loss 0.06339807\n",
      "| Epoch 148 |   200/  258 batches | lr 0.00019 | ms/batch 21.65 | loss 0.06364948\n",
      "| Epoch 148 |   250/  258 batches | lr 0.00019 | ms/batch 21.82 | loss 0.06433430\n",
      "\n",
      "Val set: Average loss: 0.06443635\n",
      "\n",
      "EarlyStopping counter: 10 out of 20\n",
      "| Epoch 149 |    50/  258 batches | lr 0.00019 | ms/batch 24.21 | loss 0.06778309\n",
      "| Epoch 149 |   100/  258 batches | lr 0.00019 | ms/batch 22.79 | loss 0.06686351\n",
      "| Epoch 149 |   150/  258 batches | lr 0.00019 | ms/batch 22.52 | loss 0.06342531\n",
      "| Epoch 149 |   200/  258 batches | lr 0.00019 | ms/batch 21.44 | loss 0.06337990\n",
      "| Epoch 149 |   250/  258 batches | lr 0.00019 | ms/batch 21.14 | loss 0.06426768\n",
      "\n",
      "Val set: Average loss: 0.06444217\n",
      "\n",
      "EarlyStopping counter: 11 out of 20\n",
      "| Epoch 150 |    50/  258 batches | lr 0.00019 | ms/batch 21.53 | loss 0.06772312\n",
      "| Epoch 150 |   100/  258 batches | lr 0.00019 | ms/batch 21.14 | loss 0.06680326\n",
      "| Epoch 150 |   150/  258 batches | lr 0.00019 | ms/batch 21.00 | loss 0.06341691\n",
      "| Epoch 150 |   200/  258 batches | lr 0.00019 | ms/batch 21.43 | loss 0.06332841\n",
      "| Epoch 150 |   250/  258 batches | lr 0.00019 | ms/batch 21.98 | loss 0.06419372\n",
      "\n",
      "Val set: Average loss: 0.06441286\n",
      "\n",
      "EarlyStopping counter: 12 out of 20\n",
      "| Epoch 151 |    50/  258 batches | lr 0.00019 | ms/batch 22.76 | loss 0.06766189\n",
      "| Epoch 151 |   100/  258 batches | lr 0.00019 | ms/batch 22.99 | loss 0.06678439\n",
      "| Epoch 151 |   150/  258 batches | lr 0.00019 | ms/batch 22.63 | loss 0.06338364\n",
      "| Epoch 151 |   200/  258 batches | lr 0.00019 | ms/batch 22.00 | loss 0.06334830\n",
      "| Epoch 151 |   250/  258 batches | lr 0.00019 | ms/batch 21.57 | loss 0.06417027\n",
      "\n",
      "Val set: Average loss: 0.06431287\n",
      "\n",
      "EarlyStopping counter: 13 out of 20\n",
      "| Epoch 152 |    50/  258 batches | lr 0.00019 | ms/batch 21.94 | loss 0.06760329\n",
      "| Epoch 152 |   100/  258 batches | lr 0.00019 | ms/batch 19.73 | loss 0.06678091\n",
      "| Epoch 152 |   150/  258 batches | lr 0.00019 | ms/batch 23.05 | loss 0.06328726\n",
      "| Epoch 152 |   200/  258 batches | lr 0.00019 | ms/batch 21.38 | loss 0.06329394\n",
      "| Epoch 152 |   250/  258 batches | lr 0.00019 | ms/batch 21.38 | loss 0.06414621\n",
      "\n",
      "Val set: Average loss: 0.06422924\n",
      "\n",
      "EarlyStopping counter: 14 out of 20\n",
      "| Epoch 153 |    50/  258 batches | lr 0.00019 | ms/batch 22.10 | loss 0.06741848\n",
      "| Epoch 153 |   100/  258 batches | lr 0.00019 | ms/batch 19.70 | loss 0.06672747\n",
      "| Epoch 153 |   150/  258 batches | lr 0.00019 | ms/batch 21.27 | loss 0.06329772\n",
      "| Epoch 153 |   200/  258 batches | lr 0.00019 | ms/batch 21.64 | loss 0.06334240\n",
      "| Epoch 153 |   250/  258 batches | lr 0.00019 | ms/batch 22.17 | loss 0.06411231\n",
      "\n",
      "Val set: Average loss: 0.06413182\n",
      "\n",
      "EarlyStopping counter: 15 out of 20\n",
      "| Epoch 154 |    50/  258 batches | lr 0.00019 | ms/batch 22.28 | loss 0.06733726\n",
      "| Epoch 154 |   100/  258 batches | lr 0.00019 | ms/batch 21.60 | loss 0.06667183\n",
      "| Epoch 154 |   150/  258 batches | lr 0.00019 | ms/batch 21.43 | loss 0.06315081\n",
      "| Epoch 154 |   200/  258 batches | lr 0.00019 | ms/batch 21.85 | loss 0.06333543\n",
      "| Epoch 154 |   250/  258 batches | lr 0.00019 | ms/batch 20.24 | loss 0.06414384\n",
      "\n",
      "Val set: Average loss: 0.06403224\n",
      "\n",
      "| Epoch 155 |    50/  258 batches | lr 0.00019 | ms/batch 23.27 | loss 0.06727847\n",
      "| Epoch 155 |   100/  258 batches | lr 0.00019 | ms/batch 23.32 | loss 0.06656983\n",
      "| Epoch 155 |   150/  258 batches | lr 0.00019 | ms/batch 19.84 | loss 0.06307463\n",
      "| Epoch 155 |   200/  258 batches | lr 0.00019 | ms/batch 21.52 | loss 0.06334647\n",
      "| Epoch 155 |   250/  258 batches | lr 0.00019 | ms/batch 21.52 | loss 0.06411151\n",
      "\n",
      "Val set: Average loss: 0.06402549\n",
      "\n",
      "| Epoch 156 |    50/  258 batches | lr 0.00019 | ms/batch 20.18 | loss 0.06722862\n",
      "| Epoch 156 |   100/  258 batches | lr 0.00019 | ms/batch 23.24 | loss 0.06659756\n",
      "| Epoch 156 |   150/  258 batches | lr 0.00019 | ms/batch 23.17 | loss 0.06321672\n",
      "| Epoch 156 |   200/  258 batches | lr 0.00019 | ms/batch 23.04 | loss 0.06337139\n",
      "| Epoch 156 |   250/  258 batches | lr 0.00019 | ms/batch 22.53 | loss 0.06405165\n",
      "\n",
      "Val set: Average loss: 0.06409350\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 157 |    50/  258 batches | lr 0.00019 | ms/batch 22.54 | loss 0.06712666\n",
      "| Epoch 157 |   100/  258 batches | lr 0.00019 | ms/batch 20.35 | loss 0.06743939\n",
      "| Epoch 157 |   150/  258 batches | lr 0.00019 | ms/batch 20.92 | loss 0.06328403\n",
      "| Epoch 157 |   200/  258 batches | lr 0.00019 | ms/batch 21.38 | loss 0.06348029\n",
      "| Epoch 157 |   250/  258 batches | lr 0.00019 | ms/batch 21.15 | loss 0.06367697\n",
      "\n",
      "Val set: Average loss: 0.06404860\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch 158 |    50/  258 batches | lr 0.00019 | ms/batch 21.77 | loss 0.06669012\n",
      "| Epoch 158 |   100/  258 batches | lr 0.00019 | ms/batch 21.19 | loss 0.06640862\n",
      "| Epoch 158 |   150/  258 batches | lr 0.00019 | ms/batch 20.98 | loss 0.06293194\n",
      "| Epoch 158 |   200/  258 batches | lr 0.00019 | ms/batch 21.13 | loss 0.06331846\n",
      "| Epoch 158 |   250/  258 batches | lr 0.00019 | ms/batch 21.00 | loss 0.06357387\n",
      "\n",
      "Val set: Average loss: 0.06405721\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch 159 |    50/  258 batches | lr 0.00019 | ms/batch 21.48 | loss 0.06657591\n",
      "| Epoch 159 |   100/  258 batches | lr 0.00019 | ms/batch 20.96 | loss 0.06600897\n",
      "| Epoch 159 |   150/  258 batches | lr 0.00019 | ms/batch 21.15 | loss 0.06269628\n",
      "| Epoch 159 |   200/  258 batches | lr 0.00019 | ms/batch 19.13 | loss 0.06326803\n",
      "| Epoch 159 |   250/  258 batches | lr 0.00019 | ms/batch 21.14 | loss 0.06343567\n",
      "\n",
      "Val set: Average loss: 0.06421045\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch 160 |    50/  258 batches | lr 0.00019 | ms/batch 21.41 | loss 0.06658886\n",
      "| Epoch 160 |   100/  258 batches | lr 0.00019 | ms/batch 19.18 | loss 0.06588146\n",
      "| Epoch 160 |   150/  258 batches | lr 0.00019 | ms/batch 20.94 | loss 0.06224065\n",
      "| Epoch 160 |   200/  258 batches | lr 0.00019 | ms/batch 21.00 | loss 0.06314414\n",
      "| Epoch 160 |   250/  258 batches | lr 0.00019 | ms/batch 21.16 | loss 0.06316638\n",
      "\n",
      "Val set: Average loss: 0.06420488\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch 161 |    50/  258 batches | lr 0.00019 | ms/batch 21.41 | loss 0.06676931\n",
      "| Epoch 161 |   100/  258 batches | lr 0.00019 | ms/batch 21.03 | loss 0.06577370\n",
      "| Epoch 161 |   150/  258 batches | lr 0.00019 | ms/batch 20.90 | loss 0.06206014\n",
      "| Epoch 161 |   200/  258 batches | lr 0.00019 | ms/batch 21.21 | loss 0.06317178\n",
      "| Epoch 161 |   250/  258 batches | lr 0.00019 | ms/batch 20.96 | loss 0.06304236\n",
      "\n",
      "Val set: Average loss: 0.06418293\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch 162 |    50/  258 batches | lr 0.00019 | ms/batch 21.20 | loss 0.06668777\n",
      "| Epoch 162 |   100/  258 batches | lr 0.00019 | ms/batch 21.06 | loss 0.06558888\n",
      "| Epoch 162 |   150/  258 batches | lr 0.00019 | ms/batch 19.10 | loss 0.06194169\n",
      "| Epoch 162 |   200/  258 batches | lr 0.00019 | ms/batch 21.04 | loss 0.06308397\n",
      "| Epoch 162 |   250/  258 batches | lr 0.00019 | ms/batch 22.74 | loss 0.06294791\n",
      "\n",
      "Val set: Average loss: 0.06430000\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch 163 |    50/  258 batches | lr 0.00019 | ms/batch 19.69 | loss 0.06666116\n",
      "| Epoch 163 |   100/  258 batches | lr 0.00019 | ms/batch 22.71 | loss 0.06544534\n",
      "| Epoch 163 |   150/  258 batches | lr 0.00019 | ms/batch 19.15 | loss 0.06188981\n",
      "| Epoch 163 |   200/  258 batches | lr 0.00019 | ms/batch 19.13 | loss 0.06303086\n",
      "| Epoch 163 |   250/  258 batches | lr 0.00019 | ms/batch 22.83 | loss 0.06286353\n",
      "\n",
      "Val set: Average loss: 0.06431631\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch 164 |    50/  258 batches | lr 0.00019 | ms/batch 22.05 | loss 0.06657969\n",
      "| Epoch 164 |   100/  258 batches | lr 0.00019 | ms/batch 21.20 | loss 0.06535085\n",
      "| Epoch 164 |   150/  258 batches | lr 0.00019 | ms/batch 21.07 | loss 0.06186211\n",
      "| Epoch 164 |   200/  258 batches | lr 0.00019 | ms/batch 21.20 | loss 0.06297775\n",
      "| Epoch 164 |   250/  258 batches | lr 0.00019 | ms/batch 19.38 | loss 0.06271293\n",
      "\n",
      "Val set: Average loss: 0.06422599\n",
      "\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch 165 |    50/  258 batches | lr 0.00019 | ms/batch 21.51 | loss 0.06662231\n",
      "| Epoch 165 |   100/  258 batches | lr 0.00019 | ms/batch 21.15 | loss 0.06526805\n",
      "| Epoch 165 |   150/  258 batches | lr 0.00019 | ms/batch 19.34 | loss 0.06177505\n",
      "| Epoch 165 |   200/  258 batches | lr 0.00019 | ms/batch 20.92 | loss 0.06289623\n",
      "| Epoch 165 |   250/  258 batches | lr 0.00019 | ms/batch 20.87 | loss 0.06264189\n",
      "\n",
      "Val set: Average loss: 0.06439788\n",
      "\n",
      "EarlyStopping counter: 10 out of 20\n",
      "| Epoch 166 |    50/  258 batches | lr 0.00019 | ms/batch 19.67 | loss 0.06673113\n",
      "| Epoch 166 |   100/  258 batches | lr 0.00019 | ms/batch 19.21 | loss 0.06526359\n",
      "| Epoch 166 |   150/  258 batches | lr 0.00019 | ms/batch 22.80 | loss 0.06182686\n",
      "| Epoch 166 |   200/  258 batches | lr 0.00019 | ms/batch 21.10 | loss 0.06274420\n",
      "| Epoch 166 |   250/  258 batches | lr 0.00019 | ms/batch 19.30 | loss 0.06281953\n",
      "\n",
      "Val set: Average loss: 0.06414946\n",
      "\n",
      "EarlyStopping counter: 11 out of 20\n",
      "| Epoch 167 |    50/  258 batches | lr 0.00019 | ms/batch 19.55 | loss 0.06648435\n",
      "| Epoch 167 |   100/  258 batches | lr 0.00019 | ms/batch 20.88 | loss 0.06595997\n",
      "| Epoch 167 |   150/  258 batches | lr 0.00019 | ms/batch 19.24 | loss 0.06225864\n",
      "| Epoch 167 |   200/  258 batches | lr 0.00019 | ms/batch 20.81 | loss 0.06251918\n",
      "| Epoch 167 |   250/  258 batches | lr 0.00019 | ms/batch 21.03 | loss 0.06243600\n",
      "\n",
      "Val set: Average loss: 0.06414012\n",
      "\n",
      "EarlyStopping counter: 12 out of 20\n",
      "| Epoch 168 |    50/  258 batches | lr 0.00019 | ms/batch 21.54 | loss 0.06638791\n",
      "| Epoch 168 |   100/  258 batches | lr 0.00019 | ms/batch 20.99 | loss 0.06518342\n",
      "| Epoch 168 |   150/  258 batches | lr 0.00019 | ms/batch 21.16 | loss 0.06195142\n",
      "| Epoch 168 |   200/  258 batches | lr 0.00019 | ms/batch 20.88 | loss 0.06243478\n",
      "| Epoch 168 |   250/  258 batches | lr 0.00019 | ms/batch 21.18 | loss 0.06235856\n",
      "\n",
      "Val set: Average loss: 0.06412903\n",
      "\n",
      "EarlyStopping counter: 13 out of 20\n",
      "| Epoch 169 |    50/  258 batches | lr 0.00019 | ms/batch 21.35 | loss 0.06596936\n",
      "| Epoch 169 |   100/  258 batches | lr 0.00019 | ms/batch 21.09 | loss 0.06520069\n",
      "| Epoch 169 |   150/  258 batches | lr 0.00019 | ms/batch 19.16 | loss 0.06208239\n",
      "| Epoch 169 |   200/  258 batches | lr 0.00019 | ms/batch 20.87 | loss 0.06236331\n",
      "| Epoch 169 |   250/  258 batches | lr 0.00019 | ms/batch 21.01 | loss 0.06229187\n",
      "\n",
      "Val set: Average loss: 0.06392206\n",
      "\n",
      "| Epoch 170 |    50/  258 batches | lr 0.00019 | ms/batch 21.31 | loss 0.06574471\n",
      "| Epoch 170 |   100/  258 batches | lr 0.00019 | ms/batch 20.99 | loss 0.06498691\n",
      "| Epoch 170 |   150/  258 batches | lr 0.00019 | ms/batch 21.12 | loss 0.06202244\n",
      "| Epoch 170 |   200/  258 batches | lr 0.00019 | ms/batch 21.01 | loss 0.06229256\n",
      "| Epoch 170 |   250/  258 batches | lr 0.00019 | ms/batch 21.02 | loss 0.06199213\n",
      "\n",
      "Val set: Average loss: 0.06390793\n",
      "\n",
      "| Epoch 171 |    50/  258 batches | lr 0.00019 | ms/batch 21.30 | loss 0.06569264\n",
      "| Epoch 171 |   100/  258 batches | lr 0.00019 | ms/batch 21.12 | loss 0.06538967\n",
      "| Epoch 171 |   150/  258 batches | lr 0.00019 | ms/batch 20.98 | loss 0.06333820\n",
      "| Epoch 171 |   200/  258 batches | lr 0.00019 | ms/batch 21.14 | loss 0.06245504\n",
      "| Epoch 171 |   250/  258 batches | lr 0.00019 | ms/batch 20.93 | loss 0.06200079\n",
      "\n",
      "Val set: Average loss: 0.06357831\n",
      "\n",
      "| Epoch 172 |    50/  258 batches | lr 0.00019 | ms/batch 21.61 | loss 0.06577208\n",
      "| Epoch 172 |   100/  258 batches | lr 0.00019 | ms/batch 19.38 | loss 0.06518338\n",
      "| Epoch 172 |   150/  258 batches | lr 0.00019 | ms/batch 21.20 | loss 0.06208290\n",
      "| Epoch 172 |   200/  258 batches | lr 0.00019 | ms/batch 20.94 | loss 0.06211820\n",
      "| Epoch 172 |   250/  258 batches | lr 0.00019 | ms/batch 21.01 | loss 0.06183611\n",
      "\n",
      "Val set: Average loss: 0.06383549\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 173 |    50/  258 batches | lr 0.00019 | ms/batch 21.59 | loss 0.06591822\n",
      "| Epoch 173 |   100/  258 batches | lr 0.00019 | ms/batch 20.93 | loss 0.06481321\n",
      "| Epoch 173 |   150/  258 batches | lr 0.00019 | ms/batch 21.06 | loss 0.06199982\n",
      "| Epoch 173 |   200/  258 batches | lr 0.00019 | ms/batch 20.97 | loss 0.06209063\n",
      "| Epoch 173 |   250/  258 batches | lr 0.00019 | ms/batch 21.07 | loss 0.06162096\n",
      "\n",
      "Val set: Average loss: 0.06389916\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch 174 |    50/  258 batches | lr 0.00019 | ms/batch 21.41 | loss 0.06592291\n",
      "| Epoch 174 |   100/  258 batches | lr 0.00019 | ms/batch 21.08 | loss 0.06488034\n",
      "| Epoch 174 |   150/  258 batches | lr 0.00019 | ms/batch 19.24 | loss 0.06197915\n",
      "| Epoch 174 |   200/  258 batches | lr 0.00019 | ms/batch 21.80 | loss 0.06198073\n",
      "| Epoch 174 |   250/  258 batches | lr 0.00019 | ms/batch 21.67 | loss 0.06156726\n",
      "\n",
      "Val set: Average loss: 0.06393354\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch 175 |    50/  258 batches | lr 0.00019 | ms/batch 24.38 | loss 0.06566901\n",
      "| Epoch 175 |   100/  258 batches | lr 0.00019 | ms/batch 22.63 | loss 0.06495105\n",
      "| Epoch 175 |   150/  258 batches | lr 0.00019 | ms/batch 22.13 | loss 0.06241974\n",
      "| Epoch 175 |   200/  258 batches | lr 0.00019 | ms/batch 19.97 | loss 0.06200695\n",
      "| Epoch 175 |   250/  258 batches | lr 0.00019 | ms/batch 21.46 | loss 0.06157140\n",
      "\n",
      "Val set: Average loss: 0.06402943\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch 176 |    50/  258 batches | lr 0.00019 | ms/batch 22.53 | loss 0.06574162\n",
      "| Epoch 176 |   100/  258 batches | lr 0.00019 | ms/batch 21.46 | loss 0.06467088\n",
      "| Epoch 176 |   150/  258 batches | lr 0.00019 | ms/batch 19.48 | loss 0.06188400\n",
      "| Epoch 176 |   200/  258 batches | lr 0.00019 | ms/batch 21.24 | loss 0.06189799\n",
      "| Epoch 176 |   250/  258 batches | lr 0.00019 | ms/batch 21.18 | loss 0.06149830\n",
      "\n",
      "Val set: Average loss: 0.06381757\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch 177 |    50/  258 batches | lr 0.00019 | ms/batch 22.14 | loss 0.06546968\n",
      "| Epoch 177 |   100/  258 batches | lr 0.00019 | ms/batch 21.25 | loss 0.06460010\n",
      "| Epoch 177 |   150/  258 batches | lr 0.00019 | ms/batch 21.42 | loss 0.06182315\n",
      "| Epoch 177 |   200/  258 batches | lr 0.00019 | ms/batch 21.16 | loss 0.06182520\n",
      "| Epoch 177 |   250/  258 batches | lr 0.00019 | ms/batch 21.38 | loss 0.06140829\n",
      "\n",
      "Val set: Average loss: 0.06373965\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch 178 |    50/  258 batches | lr 0.00019 | ms/batch 22.13 | loss 0.06514223\n",
      "| Epoch 178 |   100/  258 batches | lr 0.00019 | ms/batch 21.68 | loss 0.06450542\n",
      "| Epoch 178 |   150/  258 batches | lr 0.00019 | ms/batch 21.20 | loss 0.06152277\n",
      "| Epoch 178 |   200/  258 batches | lr 0.00019 | ms/batch 21.37 | loss 0.06175713\n",
      "| Epoch 178 |   250/  258 batches | lr 0.00019 | ms/batch 19.63 | loss 0.06139668\n",
      "\n",
      "Val set: Average loss: 0.06368063\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch 179 |    50/  258 batches | lr 0.00019 | ms/batch 21.84 | loss 0.06489822\n",
      "| Epoch 179 |   100/  258 batches | lr 0.00019 | ms/batch 21.08 | loss 0.06446209\n",
      "| Epoch 179 |   150/  258 batches | lr 0.00019 | ms/batch 21.57 | loss 0.06154786\n",
      "| Epoch 179 |   200/  258 batches | lr 0.00019 | ms/batch 21.24 | loss 0.06164812\n",
      "| Epoch 179 |   250/  258 batches | lr 0.00019 | ms/batch 21.61 | loss 0.06134196\n",
      "\n",
      "Val set: Average loss: 0.06345108\n",
      "\n",
      "| Epoch 180 |    50/  258 batches | lr 0.00019 | ms/batch 21.79 | loss 0.06467764\n",
      "| Epoch 180 |   100/  258 batches | lr 0.00019 | ms/batch 21.22 | loss 0.06450936\n",
      "| Epoch 180 |   150/  258 batches | lr 0.00019 | ms/batch 21.39 | loss 0.06184244\n",
      "| Epoch 180 |   200/  258 batches | lr 0.00019 | ms/batch 21.59 | loss 0.06157571\n",
      "| Epoch 180 |   250/  258 batches | lr 0.00019 | ms/batch 21.44 | loss 0.06127682\n",
      "\n",
      "Val set: Average loss: 0.06328876\n",
      "\n",
      "| Epoch 181 |    50/  258 batches | lr 0.00019 | ms/batch 19.92 | loss 0.06448864\n",
      "| Epoch 181 |   100/  258 batches | lr 0.00019 | ms/batch 21.91 | loss 0.06445740\n",
      "| Epoch 181 |   150/  258 batches | lr 0.00019 | ms/batch 20.67 | loss 0.06168320\n",
      "| Epoch 181 |   200/  258 batches | lr 0.00019 | ms/batch 22.54 | loss 0.06148827\n",
      "| Epoch 181 |   250/  258 batches | lr 0.00019 | ms/batch 20.74 | loss 0.06123530\n",
      "\n",
      "Val set: Average loss: 0.06314120\n",
      "\n",
      "| Epoch 182 |    50/  258 batches | lr 0.00019 | ms/batch 22.81 | loss 0.06421261\n",
      "| Epoch 182 |   100/  258 batches | lr 0.00019 | ms/batch 22.93 | loss 0.06419379\n",
      "| Epoch 182 |   150/  258 batches | lr 0.00019 | ms/batch 21.23 | loss 0.06157605\n",
      "| Epoch 182 |   200/  258 batches | lr 0.00019 | ms/batch 19.38 | loss 0.06136401\n",
      "| Epoch 182 |   250/  258 batches | lr 0.00019 | ms/batch 21.16 | loss 0.06114965\n",
      "\n",
      "Val set: Average loss: 0.06307459\n",
      "\n",
      "| Epoch 183 |    50/  258 batches | lr 0.00019 | ms/batch 21.81 | loss 0.06408769\n",
      "| Epoch 183 |   100/  258 batches | lr 0.00019 | ms/batch 21.40 | loss 0.06410755\n",
      "| Epoch 183 |   150/  258 batches | lr 0.00019 | ms/batch 19.33 | loss 0.06114703\n",
      "| Epoch 183 |   200/  258 batches | lr 0.00019 | ms/batch 21.24 | loss 0.06147657\n",
      "| Epoch 183 |   250/  258 batches | lr 0.00019 | ms/batch 21.29 | loss 0.06104229\n",
      "\n",
      "Val set: Average loss: 0.06305512\n",
      "\n",
      "| Epoch 184 |    50/  258 batches | lr 0.00019 | ms/batch 19.83 | loss 0.06405365\n",
      "| Epoch 184 |   100/  258 batches | lr 0.00019 | ms/batch 21.25 | loss 0.06380196\n",
      "| Epoch 184 |   150/  258 batches | lr 0.00019 | ms/batch 21.51 | loss 0.06105760\n",
      "| Epoch 184 |   200/  258 batches | lr 0.00019 | ms/batch 23.76 | loss 0.06142327\n",
      "| Epoch 184 |   250/  258 batches | lr 0.00019 | ms/batch 20.95 | loss 0.06111318\n",
      "\n",
      "Val set: Average loss: 0.06318369\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 185 |    50/  258 batches | lr 0.00019 | ms/batch 22.51 | loss 0.06509271\n",
      "| Epoch 185 |   100/  258 batches | lr 0.00019 | ms/batch 22.77 | loss 0.06367451\n",
      "| Epoch 185 |   150/  258 batches | lr 0.00019 | ms/batch 21.66 | loss 0.06086061\n",
      "| Epoch 185 |   200/  258 batches | lr 0.00019 | ms/batch 23.83 | loss 0.06112942\n",
      "| Epoch 185 |   250/  258 batches | lr 0.00019 | ms/batch 21.55 | loss 0.06090223\n",
      "\n",
      "Val set: Average loss: 0.06336706\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch 186 |    50/  258 batches | lr 0.00019 | ms/batch 20.97 | loss 0.06554824\n",
      "| Epoch 186 |   100/  258 batches | lr 0.00019 | ms/batch 21.71 | loss 0.06379465\n",
      "| Epoch 186 |   150/  258 batches | lr 0.00019 | ms/batch 22.17 | loss 0.06114024\n",
      "| Epoch 186 |   200/  258 batches | lr 0.00019 | ms/batch 24.28 | loss 0.06100879\n",
      "| Epoch 186 |   250/  258 batches | lr 0.00019 | ms/batch 23.88 | loss 0.06082705\n",
      "\n",
      "Val set: Average loss: 0.06328983\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch 187 |    50/  258 batches | lr 0.00019 | ms/batch 22.46 | loss 0.06558752\n",
      "| Epoch 187 |   100/  258 batches | lr 0.00019 | ms/batch 22.46 | loss 0.06396742\n",
      "| Epoch 187 |   150/  258 batches | lr 0.00019 | ms/batch 19.76 | loss 0.06079703\n",
      "| Epoch 187 |   200/  258 batches | lr 0.00019 | ms/batch 22.58 | loss 0.06111382\n",
      "| Epoch 187 |   250/  258 batches | lr 0.00019 | ms/batch 27.35 | loss 0.06082346\n",
      "\n",
      "Val set: Average loss: 0.06318959\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch 188 |    50/  258 batches | lr 0.00019 | ms/batch 23.26 | loss 0.06564698\n",
      "| Epoch 188 |   100/  258 batches | lr 0.00019 | ms/batch 22.00 | loss 0.06431483\n",
      "| Epoch 188 |   150/  258 batches | lr 0.00019 | ms/batch 22.01 | loss 0.06050852\n",
      "| Epoch 188 |   200/  258 batches | lr 0.00019 | ms/batch 21.89 | loss 0.06106619\n",
      "| Epoch 188 |   250/  258 batches | lr 0.00019 | ms/batch 21.87 | loss 0.06066283\n",
      "\n",
      "Val set: Average loss: 0.06298201\n",
      "\n",
      "| Epoch 189 |    50/  258 batches | lr 0.00019 | ms/batch 20.47 | loss 0.06425039\n",
      "| Epoch 189 |   100/  258 batches | lr 0.00019 | ms/batch 23.15 | loss 0.06498877\n",
      "| Epoch 189 |   150/  258 batches | lr 0.00019 | ms/batch 20.86 | loss 0.06213844\n",
      "| Epoch 189 |   200/  258 batches | lr 0.00019 | ms/batch 22.88 | loss 0.06107624\n",
      "| Epoch 189 |   250/  258 batches | lr 0.00019 | ms/batch 23.46 | loss 0.06078865\n",
      "\n",
      "Val set: Average loss: 0.06278265\n",
      "\n",
      "| Epoch 190 |    50/  258 batches | lr 0.00019 | ms/batch 22.43 | loss 0.06361229\n",
      "| Epoch 190 |   100/  258 batches | lr 0.00019 | ms/batch 22.76 | loss 0.06383543\n",
      "| Epoch 190 |   150/  258 batches | lr 0.00019 | ms/batch 22.32 | loss 0.06115497\n",
      "| Epoch 190 |   200/  258 batches | lr 0.00019 | ms/batch 24.34 | loss 0.06094362\n",
      "| Epoch 190 |   250/  258 batches | lr 0.00019 | ms/batch 22.85 | loss 0.06056147\n",
      "\n",
      "Val set: Average loss: 0.06271981\n",
      "\n",
      "| Epoch 191 |    50/  258 batches | lr 0.00019 | ms/batch 23.24 | loss 0.06353363\n",
      "| Epoch 191 |   100/  258 batches | lr 0.00019 | ms/batch 23.28 | loss 0.06359531\n",
      "| Epoch 191 |   150/  258 batches | lr 0.00019 | ms/batch 21.35 | loss 0.06112024\n",
      "| Epoch 191 |   200/  258 batches | lr 0.00019 | ms/batch 19.26 | loss 0.06091048\n",
      "| Epoch 191 |   250/  258 batches | lr 0.00019 | ms/batch 21.81 | loss 0.06036003\n",
      "\n",
      "Val set: Average loss: 0.06268257\n",
      "\n",
      "| Epoch 192 |    50/  258 batches | lr 0.00019 | ms/batch 23.53 | loss 0.06347546\n",
      "| Epoch 192 |   100/  258 batches | lr 0.00019 | ms/batch 22.16 | loss 0.06344904\n",
      "| Epoch 192 |   150/  258 batches | lr 0.00019 | ms/batch 21.95 | loss 0.06105234\n",
      "| Epoch 192 |   200/  258 batches | lr 0.00019 | ms/batch 22.07 | loss 0.06087012\n",
      "| Epoch 192 |   250/  258 batches | lr 0.00019 | ms/batch 20.70 | loss 0.06014337\n",
      "\n",
      "Val set: Average loss: 0.06278149\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 193 |    50/  258 batches | lr 0.00019 | ms/batch 22.26 | loss 0.06333299\n",
      "| Epoch 193 |   100/  258 batches | lr 0.00019 | ms/batch 21.41 | loss 0.06324862\n",
      "| Epoch 193 |   150/  258 batches | lr 0.00019 | ms/batch 21.50 | loss 0.06110484\n",
      "| Epoch 193 |   200/  258 batches | lr 0.00019 | ms/batch 21.68 | loss 0.06075707\n",
      "| Epoch 193 |   250/  258 batches | lr 0.00019 | ms/batch 21.68 | loss 0.06013419\n",
      "\n",
      "Val set: Average loss: 0.06273058\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch 194 |    50/  258 batches | lr 0.00019 | ms/batch 22.15 | loss 0.06348852\n",
      "| Epoch 194 |   100/  258 batches | lr 0.00019 | ms/batch 21.42 | loss 0.06317495\n",
      "| Epoch 194 |   150/  258 batches | lr 0.00019 | ms/batch 19.39 | loss 0.06115095\n",
      "| Epoch 194 |   200/  258 batches | lr 0.00019 | ms/batch 21.90 | loss 0.06060409\n",
      "| Epoch 194 |   250/  258 batches | lr 0.00019 | ms/batch 22.17 | loss 0.05999361\n",
      "\n",
      "Val set: Average loss: 0.06281972\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch 195 |    50/  258 batches | lr 0.00019 | ms/batch 22.47 | loss 0.06343351\n",
      "| Epoch 195 |   100/  258 batches | lr 0.00019 | ms/batch 21.72 | loss 0.06310534\n",
      "| Epoch 195 |   150/  258 batches | lr 0.00019 | ms/batch 21.80 | loss 0.06099218\n",
      "| Epoch 195 |   200/  258 batches | lr 0.00019 | ms/batch 21.86 | loss 0.06050355\n",
      "| Epoch 195 |   250/  258 batches | lr 0.00019 | ms/batch 19.83 | loss 0.05983610\n",
      "\n",
      "Val set: Average loss: 0.06297406\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch 196 |    50/  258 batches | lr 0.00019 | ms/batch 23.50 | loss 0.06313194\n",
      "| Epoch 196 |   100/  258 batches | lr 0.00019 | ms/batch 22.62 | loss 0.06302319\n",
      "| Epoch 196 |   150/  258 batches | lr 0.00019 | ms/batch 22.88 | loss 0.06077200\n",
      "| Epoch 196 |   200/  258 batches | lr 0.00019 | ms/batch 22.34 | loss 0.06048010\n",
      "| Epoch 196 |   250/  258 batches | lr 0.00019 | ms/batch 23.84 | loss 0.05984635\n",
      "\n",
      "Val set: Average loss: 0.06324229\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch 197 |    50/  258 batches | lr 0.00019 | ms/batch 22.91 | loss 0.06341882\n",
      "| Epoch 197 |   100/  258 batches | lr 0.00019 | ms/batch 20.16 | loss 0.06285084\n",
      "| Epoch 197 |   150/  258 batches | lr 0.00019 | ms/batch 22.09 | loss 0.06066744\n",
      "| Epoch 197 |   200/  258 batches | lr 0.00019 | ms/batch 21.89 | loss 0.06046094\n",
      "| Epoch 197 |   250/  258 batches | lr 0.00019 | ms/batch 21.73 | loss 0.05982634\n",
      "\n",
      "Val set: Average loss: 0.06356154\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch 198 |    50/  258 batches | lr 0.00019 | ms/batch 23.13 | loss 0.06376502\n",
      "| Epoch 198 |   100/  258 batches | lr 0.00019 | ms/batch 22.39 | loss 0.06286084\n",
      "| Epoch 198 |   150/  258 batches | lr 0.00019 | ms/batch 20.23 | loss 0.06195393\n",
      "| Epoch 198 |   200/  258 batches | lr 0.00019 | ms/batch 22.32 | loss 0.06051491\n",
      "| Epoch 198 |   250/  258 batches | lr 0.00019 | ms/batch 22.34 | loss 0.05986433\n",
      "\n",
      "Val set: Average loss: 0.06350133\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch 199 |    50/  258 batches | lr 0.00019 | ms/batch 22.70 | loss 0.06387018\n",
      "| Epoch 199 |   100/  258 batches | lr 0.00019 | ms/batch 20.29 | loss 0.06298520\n",
      "| Epoch 199 |   150/  258 batches | lr 0.00019 | ms/batch 22.05 | loss 0.06085337\n",
      "| Epoch 199 |   200/  258 batches | lr 0.00019 | ms/batch 23.71 | loss 0.06040978\n",
      "| Epoch 199 |   250/  258 batches | lr 0.00019 | ms/batch 22.80 | loss 0.05979790\n",
      "\n",
      "Val set: Average loss: 0.06349048\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch 200 |    50/  258 batches | lr 0.00019 | ms/batch 23.46 | loss 0.06296560\n",
      "| Epoch 200 |   100/  258 batches | lr 0.00019 | ms/batch 21.31 | loss 0.06288575\n",
      "| Epoch 200 |   150/  258 batches | lr 0.00019 | ms/batch 23.89 | loss 0.06044803\n",
      "| Epoch 200 |   200/  258 batches | lr 0.00019 | ms/batch 23.76 | loss 0.06031036\n",
      "| Epoch 200 |   250/  258 batches | lr 0.00019 | ms/batch 23.17 | loss 0.05970481\n",
      "\n",
      "Val set: Average loss: 0.06367175\n",
      "\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch 201 |    50/  258 batches | lr 0.00019 | ms/batch 23.75 | loss 0.06315493\n",
      "| Epoch 201 |   100/  258 batches | lr 0.00019 | ms/batch 21.08 | loss 0.06330810\n",
      "| Epoch 201 |   150/  258 batches | lr 0.00019 | ms/batch 24.91 | loss 0.06053673\n",
      "| Epoch 201 |   200/  258 batches | lr 0.00019 | ms/batch 22.63 | loss 0.06034167\n",
      "| Epoch 201 |   250/  258 batches | lr 0.00019 | ms/batch 20.94 | loss 0.05977438\n",
      "\n",
      "Val set: Average loss: 0.06377921\n",
      "\n",
      "EarlyStopping counter: 10 out of 20\n",
      "| Epoch 202 |    50/  258 batches | lr 0.00019 | ms/batch 23.59 | loss 0.06287669\n",
      "| Epoch 202 |   100/  258 batches | lr 0.00019 | ms/batch 23.43 | loss 0.06299940\n",
      "| Epoch 202 |   150/  258 batches | lr 0.00019 | ms/batch 23.33 | loss 0.06037663\n",
      "| Epoch 202 |   200/  258 batches | lr 0.00019 | ms/batch 23.95 | loss 0.06038530\n",
      "| Epoch 202 |   250/  258 batches | lr 0.00019 | ms/batch 22.45 | loss 0.05981815\n",
      "\n",
      "Val set: Average loss: 0.06375548\n",
      "\n",
      "EarlyStopping counter: 11 out of 20\n",
      "| Epoch 203 |    50/  258 batches | lr 0.00019 | ms/batch 22.29 | loss 0.06276357\n",
      "| Epoch 203 |   100/  258 batches | lr 0.00019 | ms/batch 22.61 | loss 0.06280432\n",
      "| Epoch 203 |   150/  258 batches | lr 0.00019 | ms/batch 23.49 | loss 0.06027549\n",
      "| Epoch 203 |   200/  258 batches | lr 0.00019 | ms/batch 23.64 | loss 0.06045987\n",
      "| Epoch 203 |   250/  258 batches | lr 0.00019 | ms/batch 20.49 | loss 0.05969025\n",
      "\n",
      "Val set: Average loss: 0.06377198\n",
      "\n",
      "EarlyStopping counter: 12 out of 20\n",
      "| Epoch 204 |    50/  258 batches | lr 0.00019 | ms/batch 23.08 | loss 0.06278848\n",
      "| Epoch 204 |   100/  258 batches | lr 0.00019 | ms/batch 23.57 | loss 0.06267467\n",
      "| Epoch 204 |   150/  258 batches | lr 0.00019 | ms/batch 23.15 | loss 0.06023349\n",
      "| Epoch 204 |   200/  258 batches | lr 0.00019 | ms/batch 22.97 | loss 0.06048233\n",
      "| Epoch 204 |   250/  258 batches | lr 0.00019 | ms/batch 23.07 | loss 0.05972562\n",
      "\n",
      "Val set: Average loss: 0.06364932\n",
      "\n",
      "EarlyStopping counter: 13 out of 20\n",
      "| Epoch 205 |    50/  258 batches | lr 0.00019 | ms/batch 24.28 | loss 0.06283531\n",
      "| Epoch 205 |   100/  258 batches | lr 0.00019 | ms/batch 21.22 | loss 0.06255641\n",
      "| Epoch 205 |   150/  258 batches | lr 0.00019 | ms/batch 23.66 | loss 0.06021250\n",
      "| Epoch 205 |   200/  258 batches | lr 0.00019 | ms/batch 22.59 | loss 0.06020682\n",
      "| Epoch 205 |   250/  258 batches | lr 0.00019 | ms/batch 23.27 | loss 0.05945211\n",
      "\n",
      "Val set: Average loss: 0.06353762\n",
      "\n",
      "EarlyStopping counter: 14 out of 20\n",
      "| Epoch 206 |    50/  258 batches | lr 0.00019 | ms/batch 22.85 | loss 0.06284206\n",
      "| Epoch 206 |   100/  258 batches | lr 0.00019 | ms/batch 22.57 | loss 0.06241014\n",
      "| Epoch 206 |   150/  258 batches | lr 0.00019 | ms/batch 23.10 | loss 0.06018157\n",
      "| Epoch 206 |   200/  258 batches | lr 0.00019 | ms/batch 22.88 | loss 0.06014164\n",
      "| Epoch 206 |   250/  258 batches | lr 0.00019 | ms/batch 22.93 | loss 0.05930219\n",
      "\n",
      "Val set: Average loss: 0.06337462\n",
      "\n",
      "EarlyStopping counter: 15 out of 20\n",
      "| Epoch 207 |    50/  258 batches | lr 0.00019 | ms/batch 23.38 | loss 0.06274080\n",
      "| Epoch 207 |   100/  258 batches | lr 0.00019 | ms/batch 20.59 | loss 0.06218200\n",
      "| Epoch 207 |   150/  258 batches | lr 0.00019 | ms/batch 23.33 | loss 0.05987580\n",
      "| Epoch 207 |   200/  258 batches | lr 0.00019 | ms/batch 22.79 | loss 0.06019400\n",
      "| Epoch 207 |   250/  258 batches | lr 0.00019 | ms/batch 22.86 | loss 0.05921885\n",
      "\n",
      "Val set: Average loss: 0.06330493\n",
      "\n",
      "EarlyStopping counter: 16 out of 20\n",
      "| Epoch 208 |    50/  258 batches | lr 0.00019 | ms/batch 25.51 | loss 0.06254301\n",
      "| Epoch 208 |   100/  258 batches | lr 0.00019 | ms/batch 21.30 | loss 0.06266108\n",
      "| Epoch 208 |   150/  258 batches | lr 0.00019 | ms/batch 22.86 | loss 0.06073514\n",
      "| Epoch 208 |   200/  258 batches | lr 0.00019 | ms/batch 22.98 | loss 0.06004792\n",
      "| Epoch 208 |   250/  258 batches | lr 0.00019 | ms/batch 23.07 | loss 0.05930062\n",
      "\n",
      "Val set: Average loss: 0.06330103\n",
      "\n",
      "EarlyStopping counter: 17 out of 20\n",
      "| Epoch 209 |    50/  258 batches | lr 0.00019 | ms/batch 23.56 | loss 0.06265571\n",
      "| Epoch 209 |   100/  258 batches | lr 0.00019 | ms/batch 25.16 | loss 0.06188468\n",
      "| Epoch 209 |   150/  258 batches | lr 0.00019 | ms/batch 21.54 | loss 0.05991484\n",
      "| Epoch 209 |   200/  258 batches | lr 0.00019 | ms/batch 23.38 | loss 0.06018739\n",
      "| Epoch 209 |   250/  258 batches | lr 0.00019 | ms/batch 23.30 | loss 0.05906519\n",
      "\n",
      "Val set: Average loss: 0.06323490\n",
      "\n",
      "EarlyStopping counter: 18 out of 20\n",
      "| Epoch 210 |    50/  258 batches | lr 0.00019 | ms/batch 23.69 | loss 0.06259119\n",
      "| Epoch 210 |   100/  258 batches | lr 0.00019 | ms/batch 23.21 | loss 0.06280928\n",
      "| Epoch 210 |   150/  258 batches | lr 0.00019 | ms/batch 23.08 | loss 0.06002251\n",
      "| Epoch 210 |   200/  258 batches | lr 0.00019 | ms/batch 22.65 | loss 0.06020018\n",
      "| Epoch 210 |   250/  258 batches | lr 0.00019 | ms/batch 23.05 | loss 0.05900197\n",
      "\n",
      "Val set: Average loss: 0.06322055\n",
      "\n",
      "EarlyStopping counter: 19 out of 20\n",
      "| Epoch 211 |    50/  258 batches | lr 0.00019 | ms/batch 23.62 | loss 0.06212938\n",
      "| Epoch 211 |   100/  258 batches | lr 0.00019 | ms/batch 23.16 | loss 0.06258400\n",
      "| Epoch 211 |   150/  258 batches | lr 0.00019 | ms/batch 22.95 | loss 0.05973412\n",
      "| Epoch 211 |   200/  258 batches | lr 0.00019 | ms/batch 22.89 | loss 0.06014444\n",
      "| Epoch 211 |   250/  258 batches | lr 0.00019 | ms/batch 23.14 | loss 0.05901221\n",
      "\n",
      "Val set: Average loss: 0.06313565\n",
      "\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Stopping at Epoch: 211\n"
     ]
    }
   ],
   "source": [
    "load = False\n",
    "save_model_path = '../models/final_real_data_model.chkpt'\n",
    "val_err_df_path = '../results/val_final_real_data_model.csv'\n",
    "\n",
    "if not load:\n",
    "  train_losses, val_losses = train(\n",
    "      epochs,\n",
    "      batch_size,\n",
    "      model,\n",
    "      optimizer,\n",
    "      loss_fn,\n",
    "      X_train,\n",
    "      y_train,\n",
    "      X_val,\n",
    "      y_val)\n",
    "  val_err_df = pd.DataFrame({\n",
    "      'Training': train_losses,\n",
    "      'Validation': val_losses})\n",
    "  val_err_df.to_csv(val_err_df_path)\n",
    "  torch.save(model.state_dict(), save_model_path)\n",
    "else:\n",
    "  model = Net(input_size, output_size, hidden_size, num_layers, act_fn)\n",
    "  model.load_state_dict(torch.load(save_model_path, map_location=device))\n",
    "  model = model.to(device)\n",
    "  val_err_df = pd.read_csv(val_err_df_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650894219921,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "KkenWSRYvDpl",
    "outputId": "37746aa5-0b46-4a80-d14d-397868e89361",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_err_df[1:].plot(xlabel='epoch', ylabel='MSE')\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlG5Ky25vDpm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1650894219922,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "TBZ4eHAdvDpn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 30\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_out = model(X_test[0:test_size].to(device))\n",
    "\n",
    "test_out = output_sc.inverse_transform(test_out.cpu().detach().numpy())\n",
    "real_out = output_sc.inverse_transform(y_test[0:test_size].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1650894219926,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "wK1WeGIZvDpo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['strike', 'close', 'hv_21', 'moneyness', 'tau', 'r',\n",
    "       'call', 'put']\n",
    "test_options = pd.DataFrame(input_sc.inverse_transform(X_test[0:test_size].detach().cpu().numpy()), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1650894220542,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "_dvE5LGXvDpq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         strike        close     hv_21  moneyness       tau         r  \\\n0   1174.959717   877.497742  0.642097   1.338952  0.060391  0.008408   \n1    149.946365   188.395157  0.685534   0.796040  1.117815  0.008530   \n2    220.005936    99.593819  0.632814   2.213053  0.966832  0.008331   \n3     85.115128   903.018188  0.662111   0.094072  1.723371  0.009323   \n4    329.800812   223.677872  0.789513   1.477507  0.294036  0.008331   \n5    599.843872   199.891800  1.549778   0.332303  0.200551  0.008812   \n6    184.976151   270.060730  0.333047   0.684347  0.738733  0.008331   \n7    469.919952   997.914734  0.554215   0.471566  1.742853  0.008430   \n8   2224.023438   727.992310  0.776381   0.328062  1.695231  0.009880   \n9    124.850418   289.882446  0.417060   0.431271  0.370881  0.008331   \n10   350.191254   188.395157  0.685534   1.856414  0.195139  0.008530   \n11    75.704117    91.268684  0.547841   0.834214  0.027380  0.008812   \n12   900.015137  1022.493713  0.581055   1.135713  1.178425  0.007851   \n13   299.999359   727.992310  0.776381   0.411478  0.079602  0.009880   \n14   650.035767  1091.076904  0.590309   0.595276  0.277666  0.006832   \n15  1070.001099   786.986755  0.743129   0.735245  0.049297  0.009071   \n16  3203.811768  2484.941162  0.643556   1.289114  0.046591  0.008812   \n17    54.790852   174.916397  0.317996   0.313923  0.277666  0.006832   \n18  1860.131958   976.903748  0.605475   1.903336  0.409438  0.008148   \n19   535.012634   166.591263  1.562065   0.311096  1.101580  0.009628   \n20   235.168106   289.486023  0.372366   0.810885  0.098813  0.008408   \n21  2700.846924  2764.031250  0.360847   1.023667  0.063367  0.008232   \n22   610.039062   984.931519  0.589848   0.618604  0.772015  0.007851   \n23   429.923248   156.680374  0.401087   0.364821  0.368175  0.008530   \n24   204.843765   174.916397  0.317996   0.853300  0.123436  0.006832   \n25   239.873550   289.882446  0.417060   0.827851  0.082308  0.008331   \n26  2124.685303   909.014282  0.662111   0.428444  0.373587  0.008812   \n27    43.811359   156.680374  0.277448   3.579169  0.390160  0.008232   \n28   415.022552   284.728760  0.407999   1.459127  0.376495  0.009323   \n29   995.007324  1077.994629  0.601559   1.083048  0.137236  0.005249   \n\n        call       put  Prediction        Real  \n0  -0.000014  1.000014  256.210968  334.656799  \n1  -0.000014  1.000014   17.714352   21.054516  \n2  -0.000014  1.000014   90.662468  110.992508  \n3  -0.000014  1.000014    5.922893    1.998104  \n4  -0.000014  1.000014  108.321938  143.512695  \n5   1.000085 -0.000085   -1.645146    0.009609  \n6  -0.000014  1.000014    3.337612    5.809383  \n7  -0.000014  1.000014   47.837841   56.433163  \n8   1.000085 -0.000085   46.199780   58.918789  \n9  -0.000014  1.000014    0.879497    0.506740  \n10 -0.000014  1.000014  135.525818  160.497757  \n11 -0.000014  1.000014    1.604900    0.672451  \n12  1.000085 -0.000085  313.461487  309.220642  \n13 -0.000014  1.000014    9.217556    1.169567  \n14 -0.000014  1.000014   12.778698    9.952084  \n15  1.000085 -0.000085   17.929028    1.666698  \n16 -0.000014  1.000014  701.645081  796.567688  \n17 -0.000014  1.000014   -4.115040    0.009609  \n18 -0.000014  1.000014  770.678223  589.432800  \n19  1.000085 -0.000085   -4.664723    1.666698  \n20 -0.000014  1.000014    1.480510    1.169567  \n21  1.000085 -0.000085  211.705887  179.999512  \n22 -0.000014  1.000014   45.216625   39.530956  \n23  1.000085 -0.000085  142.979095  143.761246  \n24  1.000085 -0.000085   -5.244420    0.341030  \n25 -0.000014  1.000014    4.118907    1.003856  \n26  1.000085 -0.000085   55.829006    2.992351  \n27  1.000085 -0.000085  131.477646  127.314743  \n28 -0.000014  1.000014   95.993492   80.957939  \n29  1.000085 -0.000085  132.827057  147.489685  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>strike</th>\n      <th>close</th>\n      <th>hv_21</th>\n      <th>moneyness</th>\n      <th>tau</th>\n      <th>r</th>\n      <th>call</th>\n      <th>put</th>\n      <th>Prediction</th>\n      <th>Real</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1174.959717</td>\n      <td>877.497742</td>\n      <td>0.642097</td>\n      <td>1.338952</td>\n      <td>0.060391</td>\n      <td>0.008408</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>256.210968</td>\n      <td>334.656799</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>149.946365</td>\n      <td>188.395157</td>\n      <td>0.685534</td>\n      <td>0.796040</td>\n      <td>1.117815</td>\n      <td>0.008530</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>17.714352</td>\n      <td>21.054516</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>220.005936</td>\n      <td>99.593819</td>\n      <td>0.632814</td>\n      <td>2.213053</td>\n      <td>0.966832</td>\n      <td>0.008331</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>90.662468</td>\n      <td>110.992508</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>85.115128</td>\n      <td>903.018188</td>\n      <td>0.662111</td>\n      <td>0.094072</td>\n      <td>1.723371</td>\n      <td>0.009323</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>5.922893</td>\n      <td>1.998104</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>329.800812</td>\n      <td>223.677872</td>\n      <td>0.789513</td>\n      <td>1.477507</td>\n      <td>0.294036</td>\n      <td>0.008331</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>108.321938</td>\n      <td>143.512695</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>599.843872</td>\n      <td>199.891800</td>\n      <td>1.549778</td>\n      <td>0.332303</td>\n      <td>0.200551</td>\n      <td>0.008812</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>-1.645146</td>\n      <td>0.009609</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>184.976151</td>\n      <td>270.060730</td>\n      <td>0.333047</td>\n      <td>0.684347</td>\n      <td>0.738733</td>\n      <td>0.008331</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>3.337612</td>\n      <td>5.809383</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>469.919952</td>\n      <td>997.914734</td>\n      <td>0.554215</td>\n      <td>0.471566</td>\n      <td>1.742853</td>\n      <td>0.008430</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>47.837841</td>\n      <td>56.433163</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2224.023438</td>\n      <td>727.992310</td>\n      <td>0.776381</td>\n      <td>0.328062</td>\n      <td>1.695231</td>\n      <td>0.009880</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>46.199780</td>\n      <td>58.918789</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>124.850418</td>\n      <td>289.882446</td>\n      <td>0.417060</td>\n      <td>0.431271</td>\n      <td>0.370881</td>\n      <td>0.008331</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>0.879497</td>\n      <td>0.506740</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>350.191254</td>\n      <td>188.395157</td>\n      <td>0.685534</td>\n      <td>1.856414</td>\n      <td>0.195139</td>\n      <td>0.008530</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>135.525818</td>\n      <td>160.497757</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>75.704117</td>\n      <td>91.268684</td>\n      <td>0.547841</td>\n      <td>0.834214</td>\n      <td>0.027380</td>\n      <td>0.008812</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>1.604900</td>\n      <td>0.672451</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>900.015137</td>\n      <td>1022.493713</td>\n      <td>0.581055</td>\n      <td>1.135713</td>\n      <td>1.178425</td>\n      <td>0.007851</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>313.461487</td>\n      <td>309.220642</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>299.999359</td>\n      <td>727.992310</td>\n      <td>0.776381</td>\n      <td>0.411478</td>\n      <td>0.079602</td>\n      <td>0.009880</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>9.217556</td>\n      <td>1.169567</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>650.035767</td>\n      <td>1091.076904</td>\n      <td>0.590309</td>\n      <td>0.595276</td>\n      <td>0.277666</td>\n      <td>0.006832</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>12.778698</td>\n      <td>9.952084</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1070.001099</td>\n      <td>786.986755</td>\n      <td>0.743129</td>\n      <td>0.735245</td>\n      <td>0.049297</td>\n      <td>0.009071</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>17.929028</td>\n      <td>1.666698</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>3203.811768</td>\n      <td>2484.941162</td>\n      <td>0.643556</td>\n      <td>1.289114</td>\n      <td>0.046591</td>\n      <td>0.008812</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>701.645081</td>\n      <td>796.567688</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>54.790852</td>\n      <td>174.916397</td>\n      <td>0.317996</td>\n      <td>0.313923</td>\n      <td>0.277666</td>\n      <td>0.006832</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>-4.115040</td>\n      <td>0.009609</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1860.131958</td>\n      <td>976.903748</td>\n      <td>0.605475</td>\n      <td>1.903336</td>\n      <td>0.409438</td>\n      <td>0.008148</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>770.678223</td>\n      <td>589.432800</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>535.012634</td>\n      <td>166.591263</td>\n      <td>1.562065</td>\n      <td>0.311096</td>\n      <td>1.101580</td>\n      <td>0.009628</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>-4.664723</td>\n      <td>1.666698</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>235.168106</td>\n      <td>289.486023</td>\n      <td>0.372366</td>\n      <td>0.810885</td>\n      <td>0.098813</td>\n      <td>0.008408</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>1.480510</td>\n      <td>1.169567</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2700.846924</td>\n      <td>2764.031250</td>\n      <td>0.360847</td>\n      <td>1.023667</td>\n      <td>0.063367</td>\n      <td>0.008232</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>211.705887</td>\n      <td>179.999512</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>610.039062</td>\n      <td>984.931519</td>\n      <td>0.589848</td>\n      <td>0.618604</td>\n      <td>0.772015</td>\n      <td>0.007851</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>45.216625</td>\n      <td>39.530956</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>429.923248</td>\n      <td>156.680374</td>\n      <td>0.401087</td>\n      <td>0.364821</td>\n      <td>0.368175</td>\n      <td>0.008530</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>142.979095</td>\n      <td>143.761246</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>204.843765</td>\n      <td>174.916397</td>\n      <td>0.317996</td>\n      <td>0.853300</td>\n      <td>0.123436</td>\n      <td>0.006832</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>-5.244420</td>\n      <td>0.341030</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>239.873550</td>\n      <td>289.882446</td>\n      <td>0.417060</td>\n      <td>0.827851</td>\n      <td>0.082308</td>\n      <td>0.008331</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>4.118907</td>\n      <td>1.003856</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2124.685303</td>\n      <td>909.014282</td>\n      <td>0.662111</td>\n      <td>0.428444</td>\n      <td>0.373587</td>\n      <td>0.008812</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>55.829006</td>\n      <td>2.992351</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>43.811359</td>\n      <td>156.680374</td>\n      <td>0.277448</td>\n      <td>3.579169</td>\n      <td>0.390160</td>\n      <td>0.008232</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>131.477646</td>\n      <td>127.314743</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>415.022552</td>\n      <td>284.728760</td>\n      <td>0.407999</td>\n      <td>1.459127</td>\n      <td>0.376495</td>\n      <td>0.009323</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>95.993492</td>\n      <td>80.957939</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>995.007324</td>\n      <td>1077.994629</td>\n      <td>0.601559</td>\n      <td>1.083048</td>\n      <td>0.137236</td>\n      <td>0.005249</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>132.827057</td>\n      <td>147.489685</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_options['Prediction'] = test_out\n",
    "test_options['Real'] = real_out\n",
    "test_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1650894220545,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "TNw5-1jgvDpr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_options['Abs Error'] = np.abs(test_options.Prediction - test_options.Real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1650894220546,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "lAC6AdeVvDpr",
    "outputId": "78bd13cc-c5a0-4746-f402-9e84d8b78696",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         strike        close     hv_21  moneyness       tau         r  \\\n20   235.168106   289.486023  0.372366   0.810885  0.098813  0.008408   \n9    124.850418   289.882446  0.417060   0.431271  0.370881  0.008331   \n23   429.923248   156.680374  0.401087   0.364821  0.368175  0.008530   \n11    75.704117    91.268684  0.547841   0.834214  0.027380  0.008812   \n5    599.843872   199.891800  1.549778   0.332303  0.200551  0.008812   \n6    184.976151   270.060730  0.333047   0.684347  0.738733  0.008331   \n14   650.035767  1091.076904  0.590309   0.595276  0.277666  0.006832   \n25   239.873550   289.882446  0.417060   0.827851  0.082308  0.008331   \n1    149.946365   188.395157  0.685534   0.796040  1.117815  0.008530   \n3     85.115128   903.018188  0.662111   0.094072  1.723371  0.009323   \n17    54.790852   174.916397  0.317996   0.313923  0.277666  0.006832   \n27    43.811359   156.680374  0.277448   3.579169  0.390160  0.008232   \n12   900.015137  1022.493713  0.581055   1.135713  1.178425  0.007851   \n24   204.843765   174.916397  0.317996   0.853300  0.123436  0.006832   \n22   610.039062   984.931519  0.589848   0.618604  0.772015  0.007851   \n19   535.012634   166.591263  1.562065   0.311096  1.101580  0.009628   \n13   299.999359   727.992310  0.776381   0.411478  0.079602  0.009880   \n7    469.919952   997.914734  0.554215   0.471566  1.742853  0.008430   \n8   2224.023438   727.992310  0.776381   0.328062  1.695231  0.009880   \n29   995.007324  1077.994629  0.601559   1.083048  0.137236  0.005249   \n28   415.022552   284.728760  0.407999   1.459127  0.376495  0.009323   \n15  1070.001099   786.986755  0.743129   0.735245  0.049297  0.009071   \n2    220.005936    99.593819  0.632814   2.213053  0.966832  0.008331   \n10   350.191254   188.395157  0.685534   1.856414  0.195139  0.008530   \n21  2700.846924  2764.031250  0.360847   1.023667  0.063367  0.008232   \n4    329.800812   223.677872  0.789513   1.477507  0.294036  0.008331   \n26  2124.685303   909.014282  0.662111   0.428444  0.373587  0.008812   \n0   1174.959717   877.497742  0.642097   1.338952  0.060391  0.008408   \n16  3203.811768  2484.941162  0.643556   1.289114  0.046591  0.008812   \n18  1860.131958   976.903748  0.605475   1.903336  0.409438  0.008148   \n\n        call       put  Prediction        Real   Abs Error  \n20 -0.000014  1.000014    1.480510    1.169567    0.310944  \n9  -0.000014  1.000014    0.879497    0.506740    0.372757  \n23  1.000085 -0.000085  142.979095  143.761246    0.782150  \n11 -0.000014  1.000014    1.604900    0.672451    0.932449  \n5   1.000085 -0.000085   -1.645146    0.009609    1.654755  \n6  -0.000014  1.000014    3.337612    5.809383    2.471771  \n14 -0.000014  1.000014   12.778698    9.952084    2.826614  \n25 -0.000014  1.000014    4.118907    1.003856    3.115051  \n1  -0.000014  1.000014   17.714352   21.054516    3.340164  \n3  -0.000014  1.000014    5.922893    1.998104    3.924789  \n17 -0.000014  1.000014   -4.115040    0.009609    4.124649  \n27  1.000085 -0.000085  131.477646  127.314743    4.162903  \n12  1.000085 -0.000085  313.461487  309.220642    4.240845  \n24  1.000085 -0.000085   -5.244420    0.341030    5.585449  \n22 -0.000014  1.000014   45.216625   39.530956    5.685669  \n19  1.000085 -0.000085   -4.664723    1.666698    6.331421  \n13 -0.000014  1.000014    9.217556    1.169567    8.047989  \n7  -0.000014  1.000014   47.837841   56.433163    8.595322  \n8   1.000085 -0.000085   46.199780   58.918789   12.719009  \n29  1.000085 -0.000085  132.827057  147.489685   14.662628  \n28 -0.000014  1.000014   95.993492   80.957939   15.035553  \n15  1.000085 -0.000085   17.929028    1.666698   16.262329  \n2  -0.000014  1.000014   90.662468  110.992508   20.330040  \n10 -0.000014  1.000014  135.525818  160.497757   24.971939  \n21  1.000085 -0.000085  211.705887  179.999512   31.706375  \n4  -0.000014  1.000014  108.321938  143.512695   35.190758  \n26  1.000085 -0.000085   55.829006    2.992351   52.836655  \n0  -0.000014  1.000014  256.210968  334.656799   78.445831  \n16 -0.000014  1.000014  701.645081  796.567688   94.922607  \n18 -0.000014  1.000014  770.678223  589.432800  181.245422  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>strike</th>\n      <th>close</th>\n      <th>hv_21</th>\n      <th>moneyness</th>\n      <th>tau</th>\n      <th>r</th>\n      <th>call</th>\n      <th>put</th>\n      <th>Prediction</th>\n      <th>Real</th>\n      <th>Abs Error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20</th>\n      <td>235.168106</td>\n      <td>289.486023</td>\n      <td>0.372366</td>\n      <td>0.810885</td>\n      <td>0.098813</td>\n      <td>0.008408</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>1.480510</td>\n      <td>1.169567</td>\n      <td>0.310944</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>124.850418</td>\n      <td>289.882446</td>\n      <td>0.417060</td>\n      <td>0.431271</td>\n      <td>0.370881</td>\n      <td>0.008331</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>0.879497</td>\n      <td>0.506740</td>\n      <td>0.372757</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>429.923248</td>\n      <td>156.680374</td>\n      <td>0.401087</td>\n      <td>0.364821</td>\n      <td>0.368175</td>\n      <td>0.008530</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>142.979095</td>\n      <td>143.761246</td>\n      <td>0.782150</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>75.704117</td>\n      <td>91.268684</td>\n      <td>0.547841</td>\n      <td>0.834214</td>\n      <td>0.027380</td>\n      <td>0.008812</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>1.604900</td>\n      <td>0.672451</td>\n      <td>0.932449</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>599.843872</td>\n      <td>199.891800</td>\n      <td>1.549778</td>\n      <td>0.332303</td>\n      <td>0.200551</td>\n      <td>0.008812</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>-1.645146</td>\n      <td>0.009609</td>\n      <td>1.654755</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>184.976151</td>\n      <td>270.060730</td>\n      <td>0.333047</td>\n      <td>0.684347</td>\n      <td>0.738733</td>\n      <td>0.008331</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>3.337612</td>\n      <td>5.809383</td>\n      <td>2.471771</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>650.035767</td>\n      <td>1091.076904</td>\n      <td>0.590309</td>\n      <td>0.595276</td>\n      <td>0.277666</td>\n      <td>0.006832</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>12.778698</td>\n      <td>9.952084</td>\n      <td>2.826614</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>239.873550</td>\n      <td>289.882446</td>\n      <td>0.417060</td>\n      <td>0.827851</td>\n      <td>0.082308</td>\n      <td>0.008331</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>4.118907</td>\n      <td>1.003856</td>\n      <td>3.115051</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>149.946365</td>\n      <td>188.395157</td>\n      <td>0.685534</td>\n      <td>0.796040</td>\n      <td>1.117815</td>\n      <td>0.008530</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>17.714352</td>\n      <td>21.054516</td>\n      <td>3.340164</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>85.115128</td>\n      <td>903.018188</td>\n      <td>0.662111</td>\n      <td>0.094072</td>\n      <td>1.723371</td>\n      <td>0.009323</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>5.922893</td>\n      <td>1.998104</td>\n      <td>3.924789</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>54.790852</td>\n      <td>174.916397</td>\n      <td>0.317996</td>\n      <td>0.313923</td>\n      <td>0.277666</td>\n      <td>0.006832</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>-4.115040</td>\n      <td>0.009609</td>\n      <td>4.124649</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>43.811359</td>\n      <td>156.680374</td>\n      <td>0.277448</td>\n      <td>3.579169</td>\n      <td>0.390160</td>\n      <td>0.008232</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>131.477646</td>\n      <td>127.314743</td>\n      <td>4.162903</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>900.015137</td>\n      <td>1022.493713</td>\n      <td>0.581055</td>\n      <td>1.135713</td>\n      <td>1.178425</td>\n      <td>0.007851</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>313.461487</td>\n      <td>309.220642</td>\n      <td>4.240845</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>204.843765</td>\n      <td>174.916397</td>\n      <td>0.317996</td>\n      <td>0.853300</td>\n      <td>0.123436</td>\n      <td>0.006832</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>-5.244420</td>\n      <td>0.341030</td>\n      <td>5.585449</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>610.039062</td>\n      <td>984.931519</td>\n      <td>0.589848</td>\n      <td>0.618604</td>\n      <td>0.772015</td>\n      <td>0.007851</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>45.216625</td>\n      <td>39.530956</td>\n      <td>5.685669</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>535.012634</td>\n      <td>166.591263</td>\n      <td>1.562065</td>\n      <td>0.311096</td>\n      <td>1.101580</td>\n      <td>0.009628</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>-4.664723</td>\n      <td>1.666698</td>\n      <td>6.331421</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>299.999359</td>\n      <td>727.992310</td>\n      <td>0.776381</td>\n      <td>0.411478</td>\n      <td>0.079602</td>\n      <td>0.009880</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>9.217556</td>\n      <td>1.169567</td>\n      <td>8.047989</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>469.919952</td>\n      <td>997.914734</td>\n      <td>0.554215</td>\n      <td>0.471566</td>\n      <td>1.742853</td>\n      <td>0.008430</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>47.837841</td>\n      <td>56.433163</td>\n      <td>8.595322</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2224.023438</td>\n      <td>727.992310</td>\n      <td>0.776381</td>\n      <td>0.328062</td>\n      <td>1.695231</td>\n      <td>0.009880</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>46.199780</td>\n      <td>58.918789</td>\n      <td>12.719009</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>995.007324</td>\n      <td>1077.994629</td>\n      <td>0.601559</td>\n      <td>1.083048</td>\n      <td>0.137236</td>\n      <td>0.005249</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>132.827057</td>\n      <td>147.489685</td>\n      <td>14.662628</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>415.022552</td>\n      <td>284.728760</td>\n      <td>0.407999</td>\n      <td>1.459127</td>\n      <td>0.376495</td>\n      <td>0.009323</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>95.993492</td>\n      <td>80.957939</td>\n      <td>15.035553</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1070.001099</td>\n      <td>786.986755</td>\n      <td>0.743129</td>\n      <td>0.735245</td>\n      <td>0.049297</td>\n      <td>0.009071</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>17.929028</td>\n      <td>1.666698</td>\n      <td>16.262329</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>220.005936</td>\n      <td>99.593819</td>\n      <td>0.632814</td>\n      <td>2.213053</td>\n      <td>0.966832</td>\n      <td>0.008331</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>90.662468</td>\n      <td>110.992508</td>\n      <td>20.330040</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>350.191254</td>\n      <td>188.395157</td>\n      <td>0.685534</td>\n      <td>1.856414</td>\n      <td>0.195139</td>\n      <td>0.008530</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>135.525818</td>\n      <td>160.497757</td>\n      <td>24.971939</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2700.846924</td>\n      <td>2764.031250</td>\n      <td>0.360847</td>\n      <td>1.023667</td>\n      <td>0.063367</td>\n      <td>0.008232</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>211.705887</td>\n      <td>179.999512</td>\n      <td>31.706375</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>329.800812</td>\n      <td>223.677872</td>\n      <td>0.789513</td>\n      <td>1.477507</td>\n      <td>0.294036</td>\n      <td>0.008331</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>108.321938</td>\n      <td>143.512695</td>\n      <td>35.190758</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2124.685303</td>\n      <td>909.014282</td>\n      <td>0.662111</td>\n      <td>0.428444</td>\n      <td>0.373587</td>\n      <td>0.008812</td>\n      <td>1.000085</td>\n      <td>-0.000085</td>\n      <td>55.829006</td>\n      <td>2.992351</td>\n      <td>52.836655</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1174.959717</td>\n      <td>877.497742</td>\n      <td>0.642097</td>\n      <td>1.338952</td>\n      <td>0.060391</td>\n      <td>0.008408</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>256.210968</td>\n      <td>334.656799</td>\n      <td>78.445831</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>3203.811768</td>\n      <td>2484.941162</td>\n      <td>0.643556</td>\n      <td>1.289114</td>\n      <td>0.046591</td>\n      <td>0.008812</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>701.645081</td>\n      <td>796.567688</td>\n      <td>94.922607</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1860.131958</td>\n      <td>976.903748</td>\n      <td>0.605475</td>\n      <td>1.903336</td>\n      <td>0.409438</td>\n      <td>0.008148</td>\n      <td>-0.000014</td>\n      <td>1.000014</td>\n      <td>770.678223</td>\n      <td>589.432800</td>\n      <td>181.245422</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_options.sort_values('Abs Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDJ-hkrSAqVh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MSE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def get_mse(model, X, y, batch_size):\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X, y), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = loss_fn(out, batch_labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1650895213041,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "GG64Xp3vAib-",
    "outputId": "f846ee65-029d-4a76-b006-d65e4ac07a48",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE on the train set is:  0.06176515080958836\n",
      "The MSE on the val set is:  0.06313564763827757\n",
      "The MSE on the test set is:  0.06786527428211588\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The MSE on the train set is: ', get_mse(model, X_train, y_train, batch_size).mean())\n",
    "print('The MSE on the val set is: ', get_mse(model, X_val, y_val, batch_size).mean())\n",
    "print('The MSE on the test set is: ', get_mse(model, X_test, y_test, batch_size).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCLf2KQJAvoE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MAE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def get_mae(model, X, y, batch_size):\n",
    "    mae_loss = nn.L1Loss()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X, y), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = mae_loss(out, batch_labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1650895213494,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "mZohsEyLAu7S",
    "outputId": "4d6bfba3-5751-478d-d53e-73751702414e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE on the train set is:  0.09360872650908869\n",
      "The MAE on the val set is:  0.09454489860570792\n",
      "The MAE on the test set is:  0.09696723791685971\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The MAE on the train set is: ', get_mae(model, X_train, y_train, batch_size).mean())\n",
    "print('The MAE on the val set is: ', get_mae(model, X_val, y_val, batch_size).mean())\n",
    "print('The MAE on the test set is: ', get_mae(model, X_test, y_test, batch_size).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmsZ4aPaA1SJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### RSME on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the train set is:  0.24752092115417043\n",
      "The RMSE on the val set is:  0.24994768747364476\n",
      "The RMSE on the test set is:  0.25961547693546116\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The RMSE on the train set is: ', np.sqrt(get_mse(model, X_train, y_train, batch_size)).mean())\n",
    "print('The RMSE on the val set is: ', np.sqrt(get_mse(model, X_val, y_val, batch_size)).mean())\n",
    "print('The RMSE on the test set is: ', np.sqrt(get_mse(model, X_test, y_test, batch_size)).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8Jxp0L4A5zb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MAPE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def get_mape(model, X, y, batch_size):\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X, y), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = MAPELoss(out, batch_labels.to(device)).detach().cpu().item()\n",
    "            losses.append(loss)\n",
    "\n",
    "    return np.array(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1650895213818,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "fHghkyflA79o",
    "outputId": "37838323-11f0-4eb9-d7d7-50303bf4e9ce",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAPE on the train set is:  0.47807757261880607\n",
      "The MAPE on the val set is:  0.4434887676528006\n",
      "The MAPE on the test set is:  0.49576801332560455\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The MAPE on the train set is: ', get_mape(model, X_train, y_train, batch_size).mean())\n",
    "print('The MAPE on the val set is: ', get_mape(model, X_val, y_val, batch_size).mean())\n",
    "print('The MAPE on the test set is: ', get_mape(model, X_test, y_test, batch_size).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLLLqkX6BEle",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650895213820,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "sf8FmLxxBGcu",
    "outputId": "764313a9-78fc-46ca-dd08-9fb8834e4374",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the R^2 score is:  0.9372445522419102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(X_test[0:batch_size].to(device)).squeeze().cpu().detach().numpy()\n",
    "\n",
    "y_true = y_test[0:batch_size].cpu().squeeze().detach().numpy()\n",
    "\n",
    "r2 = r2_score(y_pred=out, y_true=y_true)\n",
    "\n",
    "print('the R^2 score is: ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1650895214553,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "awvWu7WxBIHB",
    "outputId": "1b3dccd3-90ef-47d5-9895-267fdf93020c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 648x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.scatter(\n",
    "    y=output_sc.inverse_transform(out.reshape(-1, 1)),\n",
    "    x=output_sc.inverse_transform(y_true.squeeze().reshape(-1, 1))\n",
    ")\n",
    "ax.set_xlabel('Actual Value')\n",
    "ax.set_ylabel('Predicted Value')\n",
    "\n",
    "ax.text(20, 80, f'$R^2$ = {np.round(r2, 6)}', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model-Testing-Heston.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}