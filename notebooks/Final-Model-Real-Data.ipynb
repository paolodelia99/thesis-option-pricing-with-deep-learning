{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ju-WUaXnY5cr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Final model training on real data\n",
    "\n",
    "## Contents\n",
    "\n",
    "- Load the data\n",
    "- Preprocessing\n",
    "- Model\n",
    "- Training\n",
    "- Testing the model\n",
    "    - Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1650891681415,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "hH3-v8y-AuXg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650891681417,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "QXEzFQ3iAyj_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891681418,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "gsH02HCWA0gC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "options_path = '../data/real_options_tot.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1650891681421,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "jT4n95T6A4S6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3413,
     "status": "ok",
     "timestamp": 1650891684820,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "7H8lEQPCA5IS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "options_df = pd.read_csv(options_path, index_col=0)\n",
    "options_df = reduce_mem_usage(options_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1650891684821,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "9TKRgvuOA8pC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "options_df = shuffle(options_df, random_state=0)\n",
    "options_df = options_df.reset_index()\n",
    "options_df['r'] = options_df['r'] / 100\n",
    "options_df = options_df.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1650891684822,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "3HXliAsq132p",
    "outputId": "09bc2eb1-d20d-4471-ed50-5fa0eba727fb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             contractSymbol        lastTradeDate  strike    lastPrice  \\\n0       TSLA220916P00304000  2022-04-13 15:39:13   304.0     1.679688   \n1       TSLA230915C00375000  2022-03-29 15:21:57   375.0   752.000000   \n2        AMD221021P00090000  2022-05-02 19:41:36    90.0    13.398438   \n3       TSLA230120C01750000  2022-05-24 16:39:03  1750.0     5.769531   \n4       AMZN240119C03850000  2022-04-26 18:58:31  3850.0   228.500000   \n...                     ...                  ...     ...          ...   \n636030  TSLA230915P01425000  2022-05-19 16:13:22  1425.0   734.500000   \n636031  TSLA220520P00580000  2022-05-04 19:53:41   580.0     0.830078   \n636032  MSFT220916C00185000  2022-05-02 17:52:36   185.0    96.625000   \n636033  AMZN230317P03300000  2022-05-24 16:20:29  3300.0  1226.000000   \n636034  NFLX230616P00090000  2022-05-13 18:38:35    90.0     6.199219   \n\n                bid          ask     change  percentChange  volume  \\\n0          1.419922     1.990234  -0.770020     -31.421875     1.0   \n1        503.250000   518.500000   0.000000       0.000000     4.0   \n2         12.898438    13.101562  -1.349609      -9.156250    30.0   \n3          5.000000     6.101562  -1.259766     -17.921875    13.0   \n4         80.000000    98.000000   0.000000       0.000000     2.0   \n...             ...          ...        ...            ...     ...   \n636030   736.500000   754.500000  59.750000       8.851562     2.0   \n636031     0.660156     0.859863  -1.080078     -56.531250   271.0   \n636032    97.375000   100.875000   0.000000       0.000000     2.0   \n636033  1167.000000  1186.000000   0.000000       0.000000    17.0   \n636034     4.800781     5.851562   0.000000       0.000000     2.0   \n\n        openInterest  ...  contractSize  currency  type  expiryDate  \\\n0              153.0  ...       REGULAR       USD   put  2022-09-16   \n1                4.0  ...       REGULAR       USD  call  2023-09-15   \n2             4328.0  ...       REGULAR       USD   put  2022-10-21   \n3              518.0  ...       REGULAR       USD  call  2023-01-20   \n4               54.0  ...       REGULAR       USD  call  2024-01-19   \n...              ...  ...           ...       ...   ...         ...   \n636030          80.0  ...       REGULAR       USD   put  2023-09-15   \n636031         590.0  ...       REGULAR       USD   put  2022-05-20   \n636032        2600.0  ...       REGULAR       USD  call  2022-09-16   \n636033         107.0  ...       REGULAR       USD   put  2023-03-17   \n636034         192.0  ...       REGULAR       USD   put  2023-06-16   \n\n       downloadDate      close       vol  moneyness       tau         r  \n0        2022-04-13  1022.5000  0.581055   0.297363  0.428467  0.007851  \n1        2022-05-04   952.5000  0.653809   2.539062  1.371094  0.008331  \n2        2022-05-02    89.8125  0.541504   1.001953  0.472412  0.009323  \n3        2022-05-24   628.0000  0.859375   0.358887  0.662109  0.010880  \n4        2022-05-05  2328.0000  0.671387   0.604492  1.713867  0.008530  \n...             ...        ...       ...        ...       ...       ...  \n636030   2022-05-19   709.5000  0.834473   2.007812  1.330078  0.010323  \n636031   2022-05-04   952.5000  0.653809   0.608887  0.043945  0.008331  \n636032   2022-05-03   281.7500  0.407227   1.523438  0.373535  0.008812  \n636033   2022-05-25  2136.0000  0.752930   1.544922  0.812988  0.010719  \n636034   2022-05-17   190.5000  1.629883   0.472412  1.084961  0.010437  \n\n[636035 rows x 22 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contractSymbol</th>\n      <th>lastTradeDate</th>\n      <th>strike</th>\n      <th>lastPrice</th>\n      <th>bid</th>\n      <th>ask</th>\n      <th>change</th>\n      <th>percentChange</th>\n      <th>volume</th>\n      <th>openInterest</th>\n      <th>...</th>\n      <th>contractSize</th>\n      <th>currency</th>\n      <th>type</th>\n      <th>expiryDate</th>\n      <th>downloadDate</th>\n      <th>close</th>\n      <th>vol</th>\n      <th>moneyness</th>\n      <th>tau</th>\n      <th>r</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TSLA220916P00304000</td>\n      <td>2022-04-13 15:39:13</td>\n      <td>304.0</td>\n      <td>1.679688</td>\n      <td>1.419922</td>\n      <td>1.990234</td>\n      <td>-0.770020</td>\n      <td>-31.421875</td>\n      <td>1.0</td>\n      <td>153.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>put</td>\n      <td>2022-09-16</td>\n      <td>2022-04-13</td>\n      <td>1022.5000</td>\n      <td>0.581055</td>\n      <td>0.297363</td>\n      <td>0.428467</td>\n      <td>0.007851</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TSLA230915C00375000</td>\n      <td>2022-03-29 15:21:57</td>\n      <td>375.0</td>\n      <td>752.000000</td>\n      <td>503.250000</td>\n      <td>518.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>call</td>\n      <td>2023-09-15</td>\n      <td>2022-05-04</td>\n      <td>952.5000</td>\n      <td>0.653809</td>\n      <td>2.539062</td>\n      <td>1.371094</td>\n      <td>0.008331</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AMD221021P00090000</td>\n      <td>2022-05-02 19:41:36</td>\n      <td>90.0</td>\n      <td>13.398438</td>\n      <td>12.898438</td>\n      <td>13.101562</td>\n      <td>-1.349609</td>\n      <td>-9.156250</td>\n      <td>30.0</td>\n      <td>4328.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>put</td>\n      <td>2022-10-21</td>\n      <td>2022-05-02</td>\n      <td>89.8125</td>\n      <td>0.541504</td>\n      <td>1.001953</td>\n      <td>0.472412</td>\n      <td>0.009323</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TSLA230120C01750000</td>\n      <td>2022-05-24 16:39:03</td>\n      <td>1750.0</td>\n      <td>5.769531</td>\n      <td>5.000000</td>\n      <td>6.101562</td>\n      <td>-1.259766</td>\n      <td>-17.921875</td>\n      <td>13.0</td>\n      <td>518.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>call</td>\n      <td>2023-01-20</td>\n      <td>2022-05-24</td>\n      <td>628.0000</td>\n      <td>0.859375</td>\n      <td>0.358887</td>\n      <td>0.662109</td>\n      <td>0.010880</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AMZN240119C03850000</td>\n      <td>2022-04-26 18:58:31</td>\n      <td>3850.0</td>\n      <td>228.500000</td>\n      <td>80.000000</td>\n      <td>98.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.0</td>\n      <td>54.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>call</td>\n      <td>2024-01-19</td>\n      <td>2022-05-05</td>\n      <td>2328.0000</td>\n      <td>0.671387</td>\n      <td>0.604492</td>\n      <td>1.713867</td>\n      <td>0.008530</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>636030</th>\n      <td>TSLA230915P01425000</td>\n      <td>2022-05-19 16:13:22</td>\n      <td>1425.0</td>\n      <td>734.500000</td>\n      <td>736.500000</td>\n      <td>754.500000</td>\n      <td>59.750000</td>\n      <td>8.851562</td>\n      <td>2.0</td>\n      <td>80.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>put</td>\n      <td>2023-09-15</td>\n      <td>2022-05-19</td>\n      <td>709.5000</td>\n      <td>0.834473</td>\n      <td>2.007812</td>\n      <td>1.330078</td>\n      <td>0.010323</td>\n    </tr>\n    <tr>\n      <th>636031</th>\n      <td>TSLA220520P00580000</td>\n      <td>2022-05-04 19:53:41</td>\n      <td>580.0</td>\n      <td>0.830078</td>\n      <td>0.660156</td>\n      <td>0.859863</td>\n      <td>-1.080078</td>\n      <td>-56.531250</td>\n      <td>271.0</td>\n      <td>590.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>put</td>\n      <td>2022-05-20</td>\n      <td>2022-05-04</td>\n      <td>952.5000</td>\n      <td>0.653809</td>\n      <td>0.608887</td>\n      <td>0.043945</td>\n      <td>0.008331</td>\n    </tr>\n    <tr>\n      <th>636032</th>\n      <td>MSFT220916C00185000</td>\n      <td>2022-05-02 17:52:36</td>\n      <td>185.0</td>\n      <td>96.625000</td>\n      <td>97.375000</td>\n      <td>100.875000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.0</td>\n      <td>2600.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>call</td>\n      <td>2022-09-16</td>\n      <td>2022-05-03</td>\n      <td>281.7500</td>\n      <td>0.407227</td>\n      <td>1.523438</td>\n      <td>0.373535</td>\n      <td>0.008812</td>\n    </tr>\n    <tr>\n      <th>636033</th>\n      <td>AMZN230317P03300000</td>\n      <td>2022-05-24 16:20:29</td>\n      <td>3300.0</td>\n      <td>1226.000000</td>\n      <td>1167.000000</td>\n      <td>1186.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>17.0</td>\n      <td>107.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>put</td>\n      <td>2023-03-17</td>\n      <td>2022-05-25</td>\n      <td>2136.0000</td>\n      <td>0.752930</td>\n      <td>1.544922</td>\n      <td>0.812988</td>\n      <td>0.010719</td>\n    </tr>\n    <tr>\n      <th>636034</th>\n      <td>NFLX230616P00090000</td>\n      <td>2022-05-13 18:38:35</td>\n      <td>90.0</td>\n      <td>6.199219</td>\n      <td>4.800781</td>\n      <td>5.851562</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.0</td>\n      <td>192.0</td>\n      <td>...</td>\n      <td>REGULAR</td>\n      <td>USD</td>\n      <td>put</td>\n      <td>2023-06-16</td>\n      <td>2022-05-17</td>\n      <td>190.5000</td>\n      <td>1.629883</td>\n      <td>0.472412</td>\n      <td>1.084961</td>\n      <td>0.010437</td>\n    </tr>\n  </tbody>\n</table>\n<p>636035 rows Ã— 22 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pn28_RUMBAFH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "The preprocessing phase consist in the following steps\n",
    "\n",
    "- remove all the columns in the dataset that are not going to be use as the neural net's input\n",
    "- hot-encode all the dummy variables, which in this case, is the `type` feature\n",
    "- Standardize the input and the output data\n",
    "\n",
    "The training dataset will made of 80% of the total data, while the validation and the test set are going to represent 10% of the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1650891685322,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "32oPe6XUBCTF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop = ['impliedVolatility',\n",
    "                  'inTheMoney',\n",
    "                  'change',\n",
    "                  'percentChange',\n",
    "                  'bid',\n",
    "                  'ask',\n",
    "                  'volume',\n",
    "                  'openInterest',\n",
    "                  'contractSymbol',\n",
    "                  'lastTradeDate',\n",
    "                  'contractSize',\n",
    "                  'currency',\n",
    "                  'expiryDate',\n",
    "                  'downloadDate']\n",
    "options_df = options_df.drop(cols_to_drop, axis=1)\n",
    "options_df = pd.get_dummies(options_df, prefix='', prefix_sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1650891686346,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "_Tlc7k7xBDPV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_sc = StandardScaler()\n",
    "output_sc = StandardScaler()\n",
    "input_data = input_sc.fit_transform(options_df.drop(['lastPrice'], axis=1))\n",
    "output_data = output_sc.fit_transform(options_df['lastPrice'].values.reshape(-1, 1))\n",
    "\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "\n",
    "last_train_idx = int(np.round(len(input_data) * train_size))\n",
    "last_val_idx = last_train_idx + int(np.round(len(input_data) * val_size))\n",
    "\n",
    "X_train = input_data[0:last_train_idx]\n",
    "X_val = input_data[last_train_idx:last_val_idx]\n",
    "X_test = input_data[last_val_idx:]\n",
    "\n",
    "y_train = output_data[0:last_train_idx]\n",
    "y_val = output_data[last_train_idx:last_val_idx]\n",
    "y_test = output_data[last_val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1650891686349,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "IwMVtvfABIhR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = Variable(torch.Tensor(X_train))\n",
    "X_val = Variable(torch.Tensor(X_val))\n",
    "X_test = Variable(torch.Tensor(X_test))\n",
    "\n",
    "y_train = Variable(torch.Tensor(y_train))\n",
    "y_val = Variable(torch.Tensor(y_val))\n",
    "y_test = Variable(torch.Tensor(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ra2l5P1nBVCz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model\n",
    "\n",
    "The deep learning model is a fully connected neural network with the following architecture:\n",
    "\n",
    "- 4 hidden layers\n",
    "- 800 neurons for each layer\n",
    "- $LeakyReLU$ used as activation function\n",
    "- a residual connection every two hidden layers (helps to stabilize the gradient)\n",
    "\n",
    "The values of these hyperparameters are being obtained after executing the Bayesian Optimization.\n",
    "\n",
    "The weights of the network are initialized using the Glorot uniform distribution described in *Understanding the difficulty of training deep feedforward neural networks* - Glorot, X. & Bengio, Y. (2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891686350,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "xTLMLFoSBWDy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "device = 'cuda:0' if CUDA else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1650891686352,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "CQ9Gl3bUBWsj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, module):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.module = module\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.module(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891686353,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "YL3SicQPBYq0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, layer_size, act_fn):\n",
    "      super(HiddenLayer, self).__init__()\n",
    "      \n",
    "      if act_fn == 'ReLU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.ReLU())\n",
    "      elif act_fn == 'LeakyReLU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.LeakyReLU())\n",
    "      elif act_fn == 'ELU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.ELU())\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1650891686792,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "lHUFGf9xBawS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, output_size, hidden_size, num_layers, act_fn):\n",
    "    super(Net, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.output_size = output_size\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if act_fn == 'ReLU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ReLU())\n",
    "    elif act_fn == 'LeakyReLU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.LeakyReLU())\n",
    "    elif act_fn == 'ELU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ELU())\n",
    "\n",
    "    self.hidden_layers_list = []\n",
    "\n",
    "    for i in range(num_layers // 2):\n",
    "      self.hidden_layers_list.append(\n",
    "          ResBlock(\n",
    "            nn.Sequential(\n",
    "                HiddenLayer(self.hidden_size, act_fn),\n",
    "                HiddenLayer(self.hidden_size, act_fn)\n",
    "            )\n",
    "        )\n",
    "      )\n",
    "\n",
    "    self.hidden_layers = nn.Sequential(*self.hidden_layers_list)\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        self.initial_layer,\n",
    "        self.hidden_layers,\n",
    "        nn.Linear(self.hidden_size, self.output_size)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650891686793,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "zcq_lQrHBdH8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(m, init_m: str):\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_uniform(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.uniform_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_normal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.normal_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_xuniform(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.xavier_uniform_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_xnormal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.xavier_normal_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  if init_m == 'uniform':\n",
    "    m.apply(init_uniform)\n",
    "  elif init_m == 'normal':\n",
    "    m.apply(init_normal)\n",
    "  elif init_m == 'xaiver uniform':\n",
    "    m.apply(init_xuniform)\n",
    "  elif init_m == 'xavier normal':\n",
    "    m.apply(init_xnormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuCpyycNCKEZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training\n",
    "\n",
    "Regarding the training phase the number of max epochs is set to 2000, but since running 2000 epochs could (as well as take a long time) **overfit** the neural network, an *Early Stopping* technique was used to stop the neural network training if the validation error no longer improves. In particular, if for 20 epochs the validation error does not improve with respect to the minimum obtained then the training is stopped.\n",
    "\n",
    "The loss function used to train the neural net is the Mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650891686794,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "YbCOnCSNCL25",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 8\n",
    "output_size = 1\n",
    "num_layers = 4\n",
    "hidden_size = 800\n",
    "batch_size = 774\n",
    "epochs = 2000\n",
    "lr = 5.973524887918111e-05\n",
    "init_method = 'xaiver uniform'\n",
    "act_fn = 'LeakyReLU'\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers, act_fn)\n",
    "init_weights(model, init_method)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 7351,
     "status": "ok",
     "timestamp": 1650891694138,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "QqZbxrppvDpZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1650891694142,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "Q-9T0GArCMgp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class OptDataset(Dataset):\n",
    "\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X[idx], self.y[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6-hCH2ivDpb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Losses Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1650891694144,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "mVaO8TruHW4M",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def MAPELoss(output, target):\n",
    "  return torch.mean(torch.abs((target - output) / target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1650891694149,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "tVLW5dHhvDpe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, X_val, y_val):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X_val, y_val), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = loss_fn(out, batch_labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    print('\\nVal set: Average loss: {:.8f}\\n'.format(\n",
    "                losses.mean()))\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68eF5_EovDpf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Early Stopping class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650891694150,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "n2So2ffSvDpg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code took form: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, \n",
    "                 patience=10, \n",
    "                 verbose=False, \n",
    "                 delta=0, \n",
    "                 path='../models/final_heston_model.chkpt',\n",
    "                 trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL_xmnKXvDph",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1650891694152,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "DrBBTGKJvDph",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val\n",
    "):\n",
    "\n",
    "  training_losses = []\n",
    "  validation_losses = []\n",
    "\n",
    "  early_stopping = EarlyStopping(patience=20)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "\n",
    "    for batch, batch_labels in DataLoader(OptDataset(X_train, y_train), batch_size=batch_size):\n",
    "      out = model(batch.to(device))\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss = loss_fn(out, batch_labels.to(device))\n",
    "      epoch_losses.append(loss.item())\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if i > 0 and i % 50 == 0:\n",
    "        avg_loss = total_loss / 50\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| Epoch {:3d} | {:5d}/{:5d} batches | lr {:2.5f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.8f}'.format(\n",
    "              epoch, i, len(X_train) // batch_size+1, lr, elapsed * 1000 / 50,\n",
    "              avg_loss))\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "      i += 1\n",
    "\n",
    "    training_losses.append(np.array(epoch_losses).mean())\n",
    "    val_loss = evaluate(model, loss_fn, X_val, y_val)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Stopping at Epoch: {epoch}\")\n",
    "        break\n",
    "\n",
    "  return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2525796,
     "status": "ok",
     "timestamp": 1650894219916,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "hBETWoCfvDpj",
    "outputId": "30acf99f-3a1e-4d87-d1ff-13068ebe4cd4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 |    50/  658 batches | lr 0.00006 | ms/batch 21.94 | loss 0.49702895\n",
      "| Epoch   0 |   100/  658 batches | lr 0.00006 | ms/batch 17.26 | loss 0.18200452\n",
      "| Epoch   0 |   150/  658 batches | lr 0.00006 | ms/batch 15.24 | loss 0.13740353\n",
      "| Epoch   0 |   200/  658 batches | lr 0.00006 | ms/batch 14.95 | loss 0.12140823\n",
      "| Epoch   0 |   250/  658 batches | lr 0.00006 | ms/batch 17.55 | loss 0.11612058\n",
      "| Epoch   0 |   300/  658 batches | lr 0.00006 | ms/batch 15.02 | loss 0.11127010\n",
      "| Epoch   0 |   350/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.11121804\n",
      "| Epoch   0 |   400/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.10380239\n",
      "| Epoch   0 |   450/  658 batches | lr 0.00006 | ms/batch 14.60 | loss 0.10421183\n",
      "| Epoch   0 |   500/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.10745940\n",
      "| Epoch   0 |   550/  658 batches | lr 0.00006 | ms/batch 16.58 | loss 0.10395367\n",
      "| Epoch   0 |   600/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.09775824\n",
      "| Epoch   0 |   650/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.10072356\n",
      "\n",
      "Val set: Average loss: 0.09959791\n",
      "\n",
      "| Epoch   1 |    50/  658 batches | lr 0.00006 | ms/batch 16.20 | loss 0.10939499\n",
      "| Epoch   1 |   100/  658 batches | lr 0.00006 | ms/batch 14.27 | loss 0.10068620\n",
      "| Epoch   1 |   150/  658 batches | lr 0.00006 | ms/batch 14.31 | loss 0.09847772\n",
      "| Epoch   1 |   200/  658 batches | lr 0.00006 | ms/batch 16.07 | loss 0.09730328\n",
      "| Epoch   1 |   250/  658 batches | lr 0.00006 | ms/batch 15.98 | loss 0.10128118\n",
      "| Epoch   1 |   300/  658 batches | lr 0.00006 | ms/batch 14.37 | loss 0.09796730\n",
      "| Epoch   1 |   350/  658 batches | lr 0.00006 | ms/batch 15.88 | loss 0.10022298\n",
      "| Epoch   1 |   400/  658 batches | lr 0.00006 | ms/batch 14.25 | loss 0.09438841\n",
      "| Epoch   1 |   450/  658 batches | lr 0.00006 | ms/batch 14.28 | loss 0.09487912\n",
      "| Epoch   1 |   500/  658 batches | lr 0.00006 | ms/batch 16.19 | loss 0.09766465\n",
      "| Epoch   1 |   550/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.09350728\n",
      "| Epoch   1 |   600/  658 batches | lr 0.00006 | ms/batch 14.34 | loss 0.09048439\n",
      "| Epoch   1 |   650/  658 batches | lr 0.00006 | ms/batch 16.05 | loss 0.09374886\n",
      "\n",
      "Val set: Average loss: 0.10031340\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch   2 |    50/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.10443973\n",
      "| Epoch   2 |   100/  658 batches | lr 0.00006 | ms/batch 14.31 | loss 0.09484247\n",
      "| Epoch   2 |   150/  658 batches | lr 0.00006 | ms/batch 15.93 | loss 0.09028227\n",
      "| Epoch   2 |   200/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.09069541\n",
      "| Epoch   2 |   250/  658 batches | lr 0.00006 | ms/batch 16.04 | loss 0.09493117\n",
      "| Epoch   2 |   300/  658 batches | lr 0.00006 | ms/batch 16.00 | loss 0.09265461\n",
      "| Epoch   2 |   350/  658 batches | lr 0.00006 | ms/batch 14.30 | loss 0.09498413\n",
      "| Epoch   2 |   400/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.08974214\n",
      "| Epoch   2 |   450/  658 batches | lr 0.00006 | ms/batch 15.97 | loss 0.09046535\n",
      "| Epoch   2 |   500/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.09290390\n",
      "| Epoch   2 |   550/  658 batches | lr 0.00006 | ms/batch 16.04 | loss 0.08843467\n",
      "| Epoch   2 |   600/  658 batches | lr 0.00006 | ms/batch 15.93 | loss 0.08677732\n",
      "| Epoch   2 |   650/  658 batches | lr 0.00006 | ms/batch 14.35 | loss 0.08982116\n",
      "\n",
      "Val set: Average loss: 0.09440942\n",
      "\n",
      "| Epoch   3 |    50/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.09854622\n",
      "| Epoch   3 |   100/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.09161691\n",
      "| Epoch   3 |   150/  658 batches | lr 0.00006 | ms/batch 16.02 | loss 0.08698816\n",
      "| Epoch   3 |   200/  658 batches | lr 0.00006 | ms/batch 16.03 | loss 0.08728724\n",
      "| Epoch   3 |   250/  658 batches | lr 0.00006 | ms/batch 16.04 | loss 0.09091396\n",
      "| Epoch   3 |   300/  658 batches | lr 0.00006 | ms/batch 14.52 | loss 0.08940634\n",
      "| Epoch   3 |   350/  658 batches | lr 0.00006 | ms/batch 14.24 | loss 0.09164677\n",
      "| Epoch   3 |   400/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.08648319\n",
      "| Epoch   3 |   450/  658 batches | lr 0.00006 | ms/batch 15.90 | loss 0.08721145\n",
      "| Epoch   3 |   500/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.08990916\n",
      "| Epoch   3 |   550/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.08536557\n",
      "| Epoch   3 |   600/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.08432776\n",
      "| Epoch   3 |   650/  658 batches | lr 0.00006 | ms/batch 14.33 | loss 0.08699440\n",
      "\n",
      "Val set: Average loss: 0.08893968\n",
      "\n",
      "| Epoch   4 |    50/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.09487998\n",
      "| Epoch   4 |   100/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.08913387\n",
      "| Epoch   4 |   150/  658 batches | lr 0.00006 | ms/batch 14.35 | loss 0.08511785\n",
      "| Epoch   4 |   200/  658 batches | lr 0.00006 | ms/batch 15.95 | loss 0.08594679\n",
      "| Epoch   4 |   250/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.08844184\n",
      "| Epoch   4 |   300/  658 batches | lr 0.00006 | ms/batch 14.32 | loss 0.08719490\n",
      "| Epoch   4 |   350/  658 batches | lr 0.00006 | ms/batch 14.25 | loss 0.08923537\n",
      "| Epoch   4 |   400/  658 batches | lr 0.00006 | ms/batch 16.02 | loss 0.08398027\n",
      "| Epoch   4 |   450/  658 batches | lr 0.00006 | ms/batch 14.35 | loss 0.08459947\n",
      "| Epoch   4 |   500/  658 batches | lr 0.00006 | ms/batch 15.95 | loss 0.08753251\n",
      "| Epoch   4 |   550/  658 batches | lr 0.00006 | ms/batch 16.00 | loss 0.08336347\n",
      "| Epoch   4 |   600/  658 batches | lr 0.00006 | ms/batch 14.30 | loss 0.08229598\n",
      "| Epoch   4 |   650/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.08402747\n",
      "\n",
      "Val set: Average loss: 0.08512304\n",
      "\n",
      "| Epoch   5 |    50/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.09214299\n",
      "| Epoch   5 |   100/  658 batches | lr 0.00006 | ms/batch 16.01 | loss 0.08701890\n",
      "| Epoch   5 |   150/  658 batches | lr 0.00006 | ms/batch 14.28 | loss 0.08241154\n",
      "| Epoch   5 |   200/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.08230886\n",
      "| Epoch   5 |   250/  658 batches | lr 0.00006 | ms/batch 15.97 | loss 0.08634883\n",
      "| Epoch   5 |   300/  658 batches | lr 0.00006 | ms/batch 14.35 | loss 0.08522335\n",
      "| Epoch   5 |   350/  658 batches | lr 0.00006 | ms/batch 14.30 | loss 0.08724196\n",
      "| Epoch   5 |   400/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.08184595\n",
      "| Epoch   5 |   450/  658 batches | lr 0.00006 | ms/batch 14.35 | loss 0.08233592\n",
      "| Epoch   5 |   500/  658 batches | lr 0.00006 | ms/batch 15.96 | loss 0.08549300\n",
      "| Epoch   5 |   550/  658 batches | lr 0.00006 | ms/batch 15.98 | loss 0.08164698\n",
      "| Epoch   5 |   600/  658 batches | lr 0.00006 | ms/batch 14.30 | loss 0.08075697\n",
      "| Epoch   5 |   650/  658 batches | lr 0.00006 | ms/batch 14.41 | loss 0.08198982\n",
      "\n",
      "Val set: Average loss: 0.08399223\n",
      "\n",
      "| Epoch   6 |    50/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.09056771\n",
      "| Epoch   6 |   100/  658 batches | lr 0.00006 | ms/batch 15.92 | loss 0.08521011\n",
      "| Epoch   6 |   150/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.07942718\n",
      "| Epoch   6 |   200/  658 batches | lr 0.00006 | ms/batch 16.00 | loss 0.07983552\n",
      "| Epoch   6 |   250/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.08442456\n",
      "| Epoch   6 |   300/  658 batches | lr 0.00006 | ms/batch 15.04 | loss 0.08339657\n",
      "| Epoch   6 |   350/  658 batches | lr 0.00006 | ms/batch 14.38 | loss 0.08530878\n",
      "| Epoch   6 |   400/  658 batches | lr 0.00006 | ms/batch 16.03 | loss 0.07976225\n",
      "| Epoch   6 |   450/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.08013108\n",
      "| Epoch   6 |   500/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.08359193\n",
      "| Epoch   6 |   550/  658 batches | lr 0.00006 | ms/batch 16.01 | loss 0.08006916\n",
      "| Epoch   6 |   600/  658 batches | lr 0.00006 | ms/batch 14.27 | loss 0.07959518\n",
      "| Epoch   6 |   650/  658 batches | lr 0.00006 | ms/batch 14.28 | loss 0.08032423\n",
      "\n",
      "Val set: Average loss: 0.08329011\n",
      "\n",
      "| Epoch   7 |    50/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.08922556\n",
      "| Epoch   7 |   100/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.08383464\n",
      "| Epoch   7 |   150/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.07747311\n",
      "| Epoch   7 |   200/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.07913971\n",
      "| Epoch   7 |   250/  658 batches | lr 0.00006 | ms/batch 17.82 | loss 0.08271456\n",
      "| Epoch   7 |   300/  658 batches | lr 0.00006 | ms/batch 15.79 | loss 0.08178675\n",
      "| Epoch   7 |   350/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.08368183\n",
      "| Epoch   7 |   400/  658 batches | lr 0.00006 | ms/batch 17.38 | loss 0.07777051\n",
      "| Epoch   7 |   450/  658 batches | lr 0.00006 | ms/batch 15.00 | loss 0.07808018\n",
      "| Epoch   7 |   500/  658 batches | lr 0.00006 | ms/batch 17.20 | loss 0.08191525\n",
      "| Epoch   7 |   550/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.07855505\n",
      "| Epoch   7 |   600/  658 batches | lr 0.00006 | ms/batch 15.22 | loss 0.07867264\n",
      "| Epoch   7 |   650/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.07875702\n",
      "\n",
      "Val set: Average loss: 0.08143526\n",
      "\n",
      "| Epoch   8 |    50/  658 batches | lr 0.00006 | ms/batch 15.13 | loss 0.08735201\n",
      "| Epoch   8 |   100/  658 batches | lr 0.00006 | ms/batch 16.29 | loss 0.08253070\n",
      "| Epoch   8 |   150/  658 batches | lr 0.00006 | ms/batch 15.01 | loss 0.07581077\n",
      "| Epoch   8 |   200/  658 batches | lr 0.00006 | ms/batch 16.53 | loss 0.07767433\n",
      "| Epoch   8 |   250/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.08141275\n",
      "| Epoch   8 |   300/  658 batches | lr 0.00006 | ms/batch 14.57 | loss 0.08042812\n",
      "| Epoch   8 |   350/  658 batches | lr 0.00006 | ms/batch 17.04 | loss 0.08245046\n",
      "| Epoch   8 |   400/  658 batches | lr 0.00006 | ms/batch 21.69 | loss 0.07594427\n",
      "| Epoch   8 |   450/  658 batches | lr 0.00006 | ms/batch 16.40 | loss 0.07638136\n",
      "| Epoch   8 |   500/  658 batches | lr 0.00006 | ms/batch 17.73 | loss 0.08056459\n",
      "| Epoch   8 |   550/  658 batches | lr 0.00006 | ms/batch 17.39 | loss 0.07687629\n",
      "| Epoch   8 |   600/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.07756373\n",
      "| Epoch   8 |   650/  658 batches | lr 0.00006 | ms/batch 15.53 | loss 0.07725264\n",
      "\n",
      "Val set: Average loss: 0.07899390\n",
      "\n",
      "| Epoch   9 |    50/  658 batches | lr 0.00006 | ms/batch 15.27 | loss 0.08563449\n",
      "| Epoch   9 |   100/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.08135361\n",
      "| Epoch   9 |   150/  658 batches | lr 0.00006 | ms/batch 15.24 | loss 0.07441679\n",
      "| Epoch   9 |   200/  658 batches | lr 0.00006 | ms/batch 16.95 | loss 0.07715289\n",
      "| Epoch   9 |   250/  658 batches | lr 0.00006 | ms/batch 17.78 | loss 0.08018533\n",
      "| Epoch   9 |   300/  658 batches | lr 0.00006 | ms/batch 15.22 | loss 0.07909995\n",
      "| Epoch   9 |   350/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.08106412\n",
      "| Epoch   9 |   400/  658 batches | lr 0.00006 | ms/batch 17.16 | loss 0.07430783\n",
      "| Epoch   9 |   450/  658 batches | lr 0.00006 | ms/batch 14.89 | loss 0.07476410\n",
      "| Epoch   9 |   500/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.07923989\n",
      "| Epoch   9 |   550/  658 batches | lr 0.00006 | ms/batch 16.04 | loss 0.07533087\n",
      "| Epoch   9 |   600/  658 batches | lr 0.00006 | ms/batch 14.37 | loss 0.07637010\n",
      "| Epoch   9 |   650/  658 batches | lr 0.00006 | ms/batch 14.34 | loss 0.07569458\n",
      "\n",
      "Val set: Average loss: 0.07668606\n",
      "\n",
      "| Epoch  10 |    50/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.08375979\n",
      "| Epoch  10 |   100/  658 batches | lr 0.00006 | ms/batch 15.90 | loss 0.07993004\n",
      "| Epoch  10 |   150/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.07279026\n",
      "| Epoch  10 |   200/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.07471802\n",
      "| Epoch  10 |   250/  658 batches | lr 0.00006 | ms/batch 16.07 | loss 0.07862309\n",
      "| Epoch  10 |   300/  658 batches | lr 0.00006 | ms/batch 14.33 | loss 0.07783886\n",
      "| Epoch  10 |   350/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.07967767\n",
      "| Epoch  10 |   400/  658 batches | lr 0.00006 | ms/batch 15.94 | loss 0.07257234\n",
      "| Epoch  10 |   450/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.07305658\n",
      "| Epoch  10 |   500/  658 batches | lr 0.00006 | ms/batch 16.04 | loss 0.07782484\n",
      "| Epoch  10 |   550/  658 batches | lr 0.00006 | ms/batch 16.53 | loss 0.07361822\n",
      "| Epoch  10 |   600/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.07518523\n",
      "| Epoch  10 |   650/  658 batches | lr 0.00006 | ms/batch 14.35 | loss 0.07425852\n",
      "\n",
      "Val set: Average loss: 0.07609874\n",
      "\n",
      "| Epoch  11 |    50/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.08287177\n",
      "| Epoch  11 |   100/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.07861783\n",
      "| Epoch  11 |   150/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.07157232\n",
      "| Epoch  11 |   200/  658 batches | lr 0.00006 | ms/batch 16.07 | loss 0.07366211\n",
      "| Epoch  11 |   250/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.07737412\n",
      "| Epoch  11 |   300/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.07651479\n",
      "| Epoch  11 |   350/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.07814900\n",
      "| Epoch  11 |   400/  658 batches | lr 0.00006 | ms/batch 15.95 | loss 0.07101557\n",
      "| Epoch  11 |   450/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.07138037\n",
      "| Epoch  11 |   500/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.07634750\n",
      "| Epoch  11 |   550/  658 batches | lr 0.00006 | ms/batch 15.93 | loss 0.07209961\n",
      "| Epoch  11 |   600/  658 batches | lr 0.00006 | ms/batch 14.39 | loss 0.07410793\n",
      "| Epoch  11 |   650/  658 batches | lr 0.00006 | ms/batch 14.32 | loss 0.07275304\n",
      "\n",
      "Val set: Average loss: 0.07409537\n",
      "\n",
      "| Epoch  12 |    50/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.08101893\n",
      "| Epoch  12 |   100/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.07702429\n",
      "| Epoch  12 |   150/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.07041572\n",
      "| Epoch  12 |   200/  658 batches | lr 0.00006 | ms/batch 16.06 | loss 0.07210264\n",
      "| Epoch  12 |   250/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.07606078\n",
      "| Epoch  12 |   300/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.07520498\n",
      "| Epoch  12 |   350/  658 batches | lr 0.00006 | ms/batch 14.39 | loss 0.07682702\n",
      "| Epoch  12 |   400/  658 batches | lr 0.00006 | ms/batch 15.96 | loss 0.06977873\n",
      "| Epoch  12 |   450/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.06995258\n",
      "| Epoch  12 |   500/  658 batches | lr 0.00006 | ms/batch 16.03 | loss 0.07524154\n",
      "| Epoch  12 |   550/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.07075826\n",
      "| Epoch  12 |   600/  658 batches | lr 0.00006 | ms/batch 14.32 | loss 0.07318287\n",
      "| Epoch  12 |   650/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.07162371\n",
      "\n",
      "Val set: Average loss: 0.07310485\n",
      "\n",
      "| Epoch  13 |    50/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.07998662\n",
      "| Epoch  13 |   100/  658 batches | lr 0.00006 | ms/batch 16.02 | loss 0.07570115\n",
      "| Epoch  13 |   150/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.06949450\n",
      "| Epoch  13 |   200/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.07115696\n",
      "| Epoch  13 |   250/  658 batches | lr 0.00006 | ms/batch 16.05 | loss 0.07524869\n",
      "| Epoch  13 |   300/  658 batches | lr 0.00006 | ms/batch 14.41 | loss 0.07404409\n",
      "| Epoch  13 |   350/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.07577585\n",
      "| Epoch  13 |   400/  658 batches | lr 0.00006 | ms/batch 15.99 | loss 0.06862816\n",
      "| Epoch  13 |   450/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.06877744\n",
      "| Epoch  13 |   500/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.07353650\n",
      "| Epoch  13 |   550/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.06949592\n",
      "| Epoch  13 |   600/  658 batches | lr 0.00006 | ms/batch 14.35 | loss 0.07203027\n",
      "| Epoch  13 |   650/  658 batches | lr 0.00006 | ms/batch 14.39 | loss 0.07098853\n",
      "\n",
      "Val set: Average loss: 0.07218971\n",
      "\n",
      "| Epoch  14 |    50/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.07908787\n",
      "| Epoch  14 |   100/  658 batches | lr 0.00006 | ms/batch 16.02 | loss 0.07472909\n",
      "| Epoch  14 |   150/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.06877320\n",
      "| Epoch  14 |   200/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.07026499\n",
      "| Epoch  14 |   250/  658 batches | lr 0.00006 | ms/batch 16.73 | loss 0.07446504\n",
      "| Epoch  14 |   300/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.07326709\n",
      "| Epoch  14 |   350/  658 batches | lr 0.00006 | ms/batch 15.64 | loss 0.07539597\n",
      "| Epoch  14 |   400/  658 batches | lr 0.00006 | ms/batch 17.99 | loss 0.06787661\n",
      "| Epoch  14 |   450/  658 batches | lr 0.00006 | ms/batch 16.90 | loss 0.06807056\n",
      "| Epoch  14 |   500/  658 batches | lr 0.00006 | ms/batch 17.86 | loss 0.07246336\n",
      "| Epoch  14 |   550/  658 batches | lr 0.00006 | ms/batch 16.81 | loss 0.06857433\n",
      "| Epoch  14 |   600/  658 batches | lr 0.00006 | ms/batch 14.96 | loss 0.07128796\n",
      "| Epoch  14 |   650/  658 batches | lr 0.00006 | ms/batch 15.28 | loss 0.07029738\n",
      "\n",
      "Val set: Average loss: 0.07153063\n",
      "\n",
      "| Epoch  15 |    50/  658 batches | lr 0.00006 | ms/batch 15.64 | loss 0.07842517\n",
      "| Epoch  15 |   100/  658 batches | lr 0.00006 | ms/batch 16.71 | loss 0.07381787\n",
      "| Epoch  15 |   150/  658 batches | lr 0.00006 | ms/batch 14.96 | loss 0.06809329\n",
      "| Epoch  15 |   200/  658 batches | lr 0.00006 | ms/batch 16.97 | loss 0.06927387\n",
      "| Epoch  15 |   250/  658 batches | lr 0.00006 | ms/batch 16.59 | loss 0.07382333\n",
      "| Epoch  15 |   300/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.07258482\n",
      "| Epoch  15 |   350/  658 batches | lr 0.00006 | ms/batch 14.58 | loss 0.07442286\n",
      "| Epoch  15 |   400/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.06723116\n",
      "| Epoch  15 |   450/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.06735006\n",
      "| Epoch  15 |   500/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.07144734\n",
      "| Epoch  15 |   550/  658 batches | lr 0.00006 | ms/batch 16.30 | loss 0.06795939\n",
      "| Epoch  15 |   600/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.07081166\n",
      "| Epoch  15 |   650/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.06993684\n",
      "\n",
      "Val set: Average loss: 0.07116498\n",
      "\n",
      "| Epoch  16 |    50/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.07801093\n",
      "| Epoch  16 |   100/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.07332924\n",
      "| Epoch  16 |   150/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06774060\n",
      "| Epoch  16 |   200/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.06909787\n",
      "| Epoch  16 |   250/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.07326079\n",
      "| Epoch  16 |   300/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.07210715\n",
      "| Epoch  16 |   350/  658 batches | lr 0.00006 | ms/batch 15.80 | loss 0.07416989\n",
      "| Epoch  16 |   400/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.06681983\n",
      "| Epoch  16 |   450/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.06676977\n",
      "| Epoch  16 |   500/  658 batches | lr 0.00006 | ms/batch 17.04 | loss 0.07066293\n",
      "| Epoch  16 |   550/  658 batches | lr 0.00006 | ms/batch 16.77 | loss 0.06749528\n",
      "| Epoch  16 |   600/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.07038524\n",
      "| Epoch  16 |   650/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.06952060\n",
      "\n",
      "Val set: Average loss: 0.07085083\n",
      "\n",
      "| Epoch  17 |    50/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.07737939\n",
      "| Epoch  17 |   100/  658 batches | lr 0.00006 | ms/batch 16.69 | loss 0.07281913\n",
      "| Epoch  17 |   150/  658 batches | lr 0.00006 | ms/batch 15.17 | loss 0.06720954\n",
      "| Epoch  17 |   200/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06842463\n",
      "| Epoch  17 |   250/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.07283478\n",
      "| Epoch  17 |   300/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.07168651\n",
      "| Epoch  17 |   350/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.07361077\n",
      "| Epoch  17 |   400/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.06639048\n",
      "| Epoch  17 |   450/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.06632372\n",
      "| Epoch  17 |   500/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.07001940\n",
      "| Epoch  17 |   550/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06697004\n",
      "| Epoch  17 |   600/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.06992749\n",
      "| Epoch  17 |   650/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06913749\n",
      "\n",
      "Val set: Average loss: 0.07008131\n",
      "\n",
      "| Epoch  18 |    50/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.07700511\n",
      "| Epoch  18 |   100/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.07212584\n",
      "| Epoch  18 |   150/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.06665368\n",
      "| Epoch  18 |   200/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06780999\n",
      "| Epoch  18 |   250/  658 batches | lr 0.00006 | ms/batch 16.29 | loss 0.07249271\n",
      "| Epoch  18 |   300/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.07136113\n",
      "| Epoch  18 |   350/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.07313211\n",
      "| Epoch  18 |   400/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06605409\n",
      "| Epoch  18 |   450/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.06592947\n",
      "| Epoch  18 |   500/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06963563\n",
      "| Epoch  18 |   550/  658 batches | lr 0.00006 | ms/batch 16.29 | loss 0.06674688\n",
      "| Epoch  18 |   600/  658 batches | lr 0.00006 | ms/batch 14.58 | loss 0.06942933\n",
      "| Epoch  18 |   650/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.06878695\n",
      "\n",
      "Val set: Average loss: 0.06995070\n",
      "\n",
      "| Epoch  19 |    50/  658 batches | lr 0.00006 | ms/batch 14.97 | loss 0.07672880\n",
      "| Epoch  19 |   100/  658 batches | lr 0.00006 | ms/batch 16.20 | loss 0.07176636\n",
      "| Epoch  19 |   150/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.06624204\n",
      "| Epoch  19 |   200/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.06739325\n",
      "| Epoch  19 |   250/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.07205898\n",
      "| Epoch  19 |   300/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.07105293\n",
      "| Epoch  19 |   350/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.07273586\n",
      "| Epoch  19 |   400/  658 batches | lr 0.00006 | ms/batch 16.55 | loss 0.06576396\n",
      "| Epoch  19 |   450/  658 batches | lr 0.00006 | ms/batch 14.92 | loss 0.06555169\n",
      "| Epoch  19 |   500/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.06903270\n",
      "| Epoch  19 |   550/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.06643712\n",
      "| Epoch  19 |   600/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06921982\n",
      "| Epoch  19 |   650/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06866415\n",
      "\n",
      "Val set: Average loss: 0.06960054\n",
      "\n",
      "| Epoch  20 |    50/  658 batches | lr 0.00006 | ms/batch 14.91 | loss 0.07632376\n",
      "| Epoch  20 |   100/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.07139160\n",
      "| Epoch  20 |   150/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06608043\n",
      "| Epoch  20 |   200/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.06716761\n",
      "| Epoch  20 |   250/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.07159327\n",
      "| Epoch  20 |   300/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.07084736\n",
      "| Epoch  20 |   350/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.07251272\n",
      "| Epoch  20 |   400/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.06548808\n",
      "| Epoch  20 |   450/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.06514796\n",
      "| Epoch  20 |   500/  658 batches | lr 0.00006 | ms/batch 16.70 | loss 0.06838682\n",
      "| Epoch  20 |   550/  658 batches | lr 0.00006 | ms/batch 17.39 | loss 0.06616332\n",
      "| Epoch  20 |   600/  658 batches | lr 0.00006 | ms/batch 16.87 | loss 0.06869031\n",
      "| Epoch  20 |   650/  658 batches | lr 0.00006 | ms/batch 16.29 | loss 0.06802067\n",
      "\n",
      "Val set: Average loss: 0.06942386\n",
      "\n",
      "| Epoch  21 |    50/  658 batches | lr 0.00006 | ms/batch 15.08 | loss 0.07605328\n",
      "| Epoch  21 |   100/  658 batches | lr 0.00006 | ms/batch 16.79 | loss 0.07075212\n",
      "| Epoch  21 |   150/  658 batches | lr 0.00006 | ms/batch 15.27 | loss 0.06567585\n",
      "| Epoch  21 |   200/  658 batches | lr 0.00006 | ms/batch 16.59 | loss 0.06666611\n",
      "| Epoch  21 |   250/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.07136681\n",
      "| Epoch  21 |   300/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.07069341\n",
      "| Epoch  21 |   350/  658 batches | lr 0.00006 | ms/batch 14.60 | loss 0.07195992\n",
      "| Epoch  21 |   400/  658 batches | lr 0.00006 | ms/batch 16.30 | loss 0.06532047\n",
      "| Epoch  21 |   450/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.06477040\n",
      "| Epoch  21 |   500/  658 batches | lr 0.00006 | ms/batch 18.16 | loss 0.06801100\n",
      "| Epoch  21 |   550/  658 batches | lr 0.00006 | ms/batch 17.18 | loss 0.06598318\n",
      "| Epoch  21 |   600/  658 batches | lr 0.00006 | ms/batch 14.97 | loss 0.06859587\n",
      "| Epoch  21 |   650/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.06774923\n",
      "\n",
      "Val set: Average loss: 0.06897062\n",
      "\n",
      "| Epoch  22 |    50/  658 batches | lr 0.00006 | ms/batch 15.26 | loss 0.07577262\n",
      "| Epoch  22 |   100/  658 batches | lr 0.00006 | ms/batch 16.70 | loss 0.07040145\n",
      "| Epoch  22 |   150/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.06528124\n",
      "| Epoch  22 |   200/  658 batches | lr 0.00006 | ms/batch 17.60 | loss 0.06654542\n",
      "| Epoch  22 |   250/  658 batches | lr 0.00006 | ms/batch 16.57 | loss 0.07109162\n",
      "| Epoch  22 |   300/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.07034248\n",
      "| Epoch  22 |   350/  658 batches | lr 0.00006 | ms/batch 14.88 | loss 0.07168513\n",
      "| Epoch  22 |   400/  658 batches | lr 0.00006 | ms/batch 16.66 | loss 0.06512483\n",
      "| Epoch  22 |   450/  658 batches | lr 0.00006 | ms/batch 15.30 | loss 0.06448460\n",
      "| Epoch  22 |   500/  658 batches | lr 0.00006 | ms/batch 16.55 | loss 0.06750110\n",
      "| Epoch  22 |   550/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.06574626\n",
      "| Epoch  22 |   600/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06844533\n",
      "| Epoch  22 |   650/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.06749737\n",
      "\n",
      "Val set: Average loss: 0.06872874\n",
      "\n",
      "| Epoch  23 |    50/  658 batches | lr 0.00006 | ms/batch 15.28 | loss 0.07563411\n",
      "| Epoch  23 |   100/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.07009540\n",
      "| Epoch  23 |   150/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.06509428\n",
      "| Epoch  23 |   200/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.06625135\n",
      "| Epoch  23 |   250/  658 batches | lr 0.00006 | ms/batch 16.40 | loss 0.07056252\n",
      "| Epoch  23 |   300/  658 batches | lr 0.00006 | ms/batch 14.58 | loss 0.06991721\n",
      "| Epoch  23 |   350/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.07136726\n",
      "| Epoch  23 |   400/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.06501995\n",
      "| Epoch  23 |   450/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06420854\n",
      "| Epoch  23 |   500/  658 batches | lr 0.00006 | ms/batch 16.61 | loss 0.06727723\n",
      "| Epoch  23 |   550/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.06554653\n",
      "| Epoch  23 |   600/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.06794900\n",
      "| Epoch  23 |   650/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.06757083\n",
      "\n",
      "Val set: Average loss: 0.06850808\n",
      "\n",
      "| Epoch  24 |    50/  658 batches | lr 0.00006 | ms/batch 15.03 | loss 0.07523690\n",
      "| Epoch  24 |   100/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.06977156\n",
      "| Epoch  24 |   150/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06485654\n",
      "| Epoch  24 |   200/  658 batches | lr 0.00006 | ms/batch 16.55 | loss 0.06585455\n",
      "| Epoch  24 |   250/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.07036286\n",
      "| Epoch  24 |   300/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.06976919\n",
      "| Epoch  24 |   350/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.07089672\n",
      "| Epoch  24 |   400/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06480358\n",
      "| Epoch  24 |   450/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.06396526\n",
      "| Epoch  24 |   500/  658 batches | lr 0.00006 | ms/batch 16.40 | loss 0.06714177\n",
      "| Epoch  24 |   550/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06549454\n",
      "| Epoch  24 |   600/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06767615\n",
      "| Epoch  24 |   650/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.06706201\n",
      "\n",
      "Val set: Average loss: 0.06805894\n",
      "\n",
      "| Epoch  25 |    50/  658 batches | lr 0.00006 | ms/batch 14.92 | loss 0.07507435\n",
      "| Epoch  25 |   100/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06943565\n",
      "| Epoch  25 |   150/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.06476220\n",
      "| Epoch  25 |   200/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.06554322\n",
      "| Epoch  25 |   250/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.07006381\n",
      "| Epoch  25 |   300/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06958360\n",
      "| Epoch  25 |   350/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.07063913\n",
      "| Epoch  25 |   400/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.06462084\n",
      "| Epoch  25 |   450/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.06368006\n",
      "| Epoch  25 |   500/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06681506\n",
      "| Epoch  25 |   550/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06509198\n",
      "| Epoch  25 |   600/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06780703\n",
      "| Epoch  25 |   650/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.06706996\n",
      "\n",
      "Val set: Average loss: 0.06779812\n",
      "\n",
      "| Epoch  26 |    50/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.07500661\n",
      "| Epoch  26 |   100/  658 batches | lr 0.00006 | ms/batch 16.76 | loss 0.06931503\n",
      "| Epoch  26 |   150/  658 batches | lr 0.00006 | ms/batch 16.99 | loss 0.06434477\n",
      "| Epoch  26 |   200/  658 batches | lr 0.00006 | ms/batch 16.82 | loss 0.06523192\n",
      "| Epoch  26 |   250/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06981057\n",
      "| Epoch  26 |   300/  658 batches | lr 0.00006 | ms/batch 15.03 | loss 0.06925960\n",
      "| Epoch  26 |   350/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.07029503\n",
      "| Epoch  26 |   400/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06460786\n",
      "| Epoch  26 |   450/  658 batches | lr 0.00006 | ms/batch 15.09 | loss 0.06353112\n",
      "| Epoch  26 |   500/  658 batches | lr 0.00006 | ms/batch 17.11 | loss 0.06635454\n",
      "| Epoch  26 |   550/  658 batches | lr 0.00006 | ms/batch 16.91 | loss 0.06490060\n",
      "| Epoch  26 |   600/  658 batches | lr 0.00006 | ms/batch 15.26 | loss 0.06725869\n",
      "| Epoch  26 |   650/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.06686197\n",
      "\n",
      "Val set: Average loss: 0.06758328\n",
      "\n",
      "| Epoch  27 |    50/  658 batches | lr 0.00006 | ms/batch 15.25 | loss 0.07480547\n",
      "| Epoch  27 |   100/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.06904364\n",
      "| Epoch  27 |   150/  658 batches | lr 0.00006 | ms/batch 14.92 | loss 0.06411273\n",
      "| Epoch  27 |   200/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06517740\n",
      "| Epoch  27 |   250/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.06956013\n",
      "| Epoch  27 |   300/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.06917891\n",
      "| Epoch  27 |   350/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.07007368\n",
      "| Epoch  27 |   400/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06446080\n",
      "| Epoch  27 |   450/  658 batches | lr 0.00006 | ms/batch 14.88 | loss 0.06318716\n",
      "| Epoch  27 |   500/  658 batches | lr 0.00006 | ms/batch 16.64 | loss 0.06606497\n",
      "| Epoch  27 |   550/  658 batches | lr 0.00006 | ms/batch 16.52 | loss 0.06474784\n",
      "| Epoch  27 |   600/  658 batches | lr 0.00006 | ms/batch 15.02 | loss 0.06710226\n",
      "| Epoch  27 |   650/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.06658632\n",
      "\n",
      "Val set: Average loss: 0.06746346\n",
      "\n",
      "| Epoch  28 |    50/  658 batches | lr 0.00006 | ms/batch 15.73 | loss 0.07444157\n",
      "| Epoch  28 |   100/  658 batches | lr 0.00006 | ms/batch 17.66 | loss 0.06878487\n",
      "| Epoch  28 |   150/  658 batches | lr 0.00006 | ms/batch 15.82 | loss 0.06377897\n",
      "| Epoch  28 |   200/  658 batches | lr 0.00006 | ms/batch 17.09 | loss 0.06488734\n",
      "| Epoch  28 |   250/  658 batches | lr 0.00006 | ms/batch 17.60 | loss 0.06935390\n",
      "| Epoch  28 |   300/  658 batches | lr 0.00006 | ms/batch 15.32 | loss 0.06901935\n",
      "| Epoch  28 |   350/  658 batches | lr 0.00006 | ms/batch 15.10 | loss 0.06961614\n",
      "| Epoch  28 |   400/  658 batches | lr 0.00006 | ms/batch 19.15 | loss 0.06421435\n",
      "| Epoch  28 |   450/  658 batches | lr 0.00006 | ms/batch 15.54 | loss 0.06305410\n",
      "| Epoch  28 |   500/  658 batches | lr 0.00006 | ms/batch 17.73 | loss 0.06574398\n",
      "| Epoch  28 |   550/  658 batches | lr 0.00006 | ms/batch 17.40 | loss 0.06462728\n",
      "| Epoch  28 |   600/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.06697173\n",
      "| Epoch  28 |   650/  658 batches | lr 0.00006 | ms/batch 17.07 | loss 0.06639850\n",
      "\n",
      "Val set: Average loss: 0.06722475\n",
      "\n",
      "| Epoch  29 |    50/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.07423224\n",
      "| Epoch  29 |   100/  658 batches | lr 0.00006 | ms/batch 17.50 | loss 0.06865274\n",
      "| Epoch  29 |   150/  658 batches | lr 0.00006 | ms/batch 16.05 | loss 0.06362420\n",
      "| Epoch  29 |   200/  658 batches | lr 0.00006 | ms/batch 17.93 | loss 0.06458263\n",
      "| Epoch  29 |   250/  658 batches | lr 0.00006 | ms/batch 19.28 | loss 0.06916033\n",
      "| Epoch  29 |   300/  658 batches | lr 0.00006 | ms/batch 15.88 | loss 0.06868823\n",
      "| Epoch  29 |   350/  658 batches | lr 0.00006 | ms/batch 15.15 | loss 0.06954820\n",
      "| Epoch  29 |   400/  658 batches | lr 0.00006 | ms/batch 16.88 | loss 0.06424009\n",
      "| Epoch  29 |   450/  658 batches | lr 0.00006 | ms/batch 14.25 | loss 0.06282136\n",
      "| Epoch  29 |   500/  658 batches | lr 0.00006 | ms/batch 16.19 | loss 0.06563947\n",
      "| Epoch  29 |   550/  658 batches | lr 0.00006 | ms/batch 15.91 | loss 0.06435961\n",
      "| Epoch  29 |   600/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.06671050\n",
      "| Epoch  29 |   650/  658 batches | lr 0.00006 | ms/batch 15.20 | loss 0.06643962\n",
      "\n",
      "Val set: Average loss: 0.06683541\n",
      "\n",
      "| Epoch  30 |    50/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.07378553\n",
      "| Epoch  30 |   100/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.06832708\n",
      "| Epoch  30 |   150/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.06342406\n",
      "| Epoch  30 |   200/  658 batches | lr 0.00006 | ms/batch 16.64 | loss 0.06440565\n",
      "| Epoch  30 |   250/  658 batches | lr 0.00006 | ms/batch 15.85 | loss 0.06899448\n",
      "| Epoch  30 |   300/  658 batches | lr 0.00006 | ms/batch 14.02 | loss 0.06850511\n",
      "| Epoch  30 |   350/  658 batches | lr 0.00006 | ms/batch 13.96 | loss 0.06935222\n",
      "| Epoch  30 |   400/  658 batches | lr 0.00006 | ms/batch 15.53 | loss 0.06402102\n",
      "| Epoch  30 |   450/  658 batches | lr 0.00006 | ms/batch 14.08 | loss 0.06260386\n",
      "| Epoch  30 |   500/  658 batches | lr 0.00006 | ms/batch 15.96 | loss 0.06546601\n",
      "| Epoch  30 |   550/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.06416746\n",
      "| Epoch  30 |   600/  658 batches | lr 0.00006 | ms/batch 14.18 | loss 0.06661840\n",
      "| Epoch  30 |   650/  658 batches | lr 0.00006 | ms/batch 14.16 | loss 0.06607682\n",
      "\n",
      "Val set: Average loss: 0.06654821\n",
      "\n",
      "| Epoch  31 |    50/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.07370074\n",
      "| Epoch  31 |   100/  658 batches | lr 0.00006 | ms/batch 17.91 | loss 0.06803622\n",
      "| Epoch  31 |   150/  658 batches | lr 0.00006 | ms/batch 15.14 | loss 0.06319953\n",
      "| Epoch  31 |   200/  658 batches | lr 0.00006 | ms/batch 17.46 | loss 0.06424928\n",
      "| Epoch  31 |   250/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.06884762\n",
      "| Epoch  31 |   300/  658 batches | lr 0.00006 | ms/batch 13.95 | loss 0.06822186\n",
      "| Epoch  31 |   350/  658 batches | lr 0.00006 | ms/batch 13.88 | loss 0.06903551\n",
      "| Epoch  31 |   400/  658 batches | lr 0.00006 | ms/batch 15.81 | loss 0.06384185\n",
      "| Epoch  31 |   450/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.06239100\n",
      "| Epoch  31 |   500/  658 batches | lr 0.00006 | ms/batch 17.91 | loss 0.06521390\n",
      "| Epoch  31 |   550/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.06402237\n",
      "| Epoch  31 |   600/  658 batches | lr 0.00006 | ms/batch 14.38 | loss 0.06657507\n",
      "| Epoch  31 |   650/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.06585785\n",
      "\n",
      "Val set: Average loss: 0.06630954\n",
      "\n",
      "| Epoch  32 |    50/  658 batches | lr 0.00006 | ms/batch 14.88 | loss 0.07352997\n",
      "| Epoch  32 |   100/  658 batches | lr 0.00006 | ms/batch 15.73 | loss 0.06777150\n",
      "| Epoch  32 |   150/  658 batches | lr 0.00006 | ms/batch 13.92 | loss 0.06306509\n",
      "| Epoch  32 |   200/  658 batches | lr 0.00006 | ms/batch 15.55 | loss 0.06384740\n",
      "| Epoch  32 |   250/  658 batches | lr 0.00006 | ms/batch 15.56 | loss 0.06854219\n",
      "| Epoch  32 |   300/  658 batches | lr 0.00006 | ms/batch 13.92 | loss 0.06817208\n",
      "| Epoch  32 |   350/  658 batches | lr 0.00006 | ms/batch 14.28 | loss 0.06872446\n",
      "| Epoch  32 |   400/  658 batches | lr 0.00006 | ms/batch 15.68 | loss 0.06377800\n",
      "| Epoch  32 |   450/  658 batches | lr 0.00006 | ms/batch 14.15 | loss 0.06210734\n",
      "| Epoch  32 |   500/  658 batches | lr 0.00006 | ms/batch 16.06 | loss 0.06503306\n",
      "| Epoch  32 |   550/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06368078\n",
      "| Epoch  32 |   600/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.06625423\n",
      "| Epoch  32 |   650/  658 batches | lr 0.00006 | ms/batch 14.88 | loss 0.06587179\n",
      "\n",
      "Val set: Average loss: 0.06628524\n",
      "\n",
      "| Epoch  33 |    50/  658 batches | lr 0.00006 | ms/batch 17.02 | loss 0.07331977\n",
      "| Epoch  33 |   100/  658 batches | lr 0.00006 | ms/batch 18.84 | loss 0.06771751\n",
      "| Epoch  33 |   150/  658 batches | lr 0.00006 | ms/batch 16.04 | loss 0.06282647\n",
      "| Epoch  33 |   200/  658 batches | lr 0.00006 | ms/batch 18.26 | loss 0.06384538\n",
      "| Epoch  33 |   250/  658 batches | lr 0.00006 | ms/batch 17.34 | loss 0.06837275\n",
      "| Epoch  33 |   300/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06766796\n",
      "| Epoch  33 |   350/  658 batches | lr 0.00006 | ms/batch 15.13 | loss 0.06857598\n",
      "| Epoch  33 |   400/  658 batches | lr 0.00006 | ms/batch 17.28 | loss 0.06343476\n",
      "| Epoch  33 |   450/  658 batches | lr 0.00006 | ms/batch 15.33 | loss 0.06195885\n",
      "| Epoch  33 |   500/  658 batches | lr 0.00006 | ms/batch 17.05 | loss 0.06481849\n",
      "| Epoch  33 |   550/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06376353\n",
      "| Epoch  33 |   600/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.06613456\n",
      "| Epoch  33 |   650/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.06576796\n",
      "\n",
      "Val set: Average loss: 0.06607420\n",
      "\n",
      "| Epoch  34 |    50/  658 batches | lr 0.00006 | ms/batch 15.05 | loss 0.07323981\n",
      "| Epoch  34 |   100/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06759779\n",
      "| Epoch  34 |   150/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06269442\n",
      "| Epoch  34 |   200/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06358827\n",
      "| Epoch  34 |   250/  658 batches | lr 0.00006 | ms/batch 16.57 | loss 0.06809785\n",
      "| Epoch  34 |   300/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.06764149\n",
      "| Epoch  34 |   350/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.06839655\n",
      "| Epoch  34 |   400/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.06338207\n",
      "| Epoch  34 |   450/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06184166\n",
      "| Epoch  34 |   500/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.06468096\n",
      "| Epoch  34 |   550/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06357307\n",
      "| Epoch  34 |   600/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06614349\n",
      "| Epoch  34 |   650/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.06558908\n",
      "\n",
      "Val set: Average loss: 0.06592599\n",
      "\n",
      "| Epoch  35 |    50/  658 batches | lr 0.00006 | ms/batch 15.01 | loss 0.07323486\n",
      "| Epoch  35 |   100/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06736867\n",
      "| Epoch  35 |   150/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.06247367\n",
      "| Epoch  35 |   200/  658 batches | lr 0.00006 | ms/batch 16.30 | loss 0.06352018\n",
      "| Epoch  35 |   250/  658 batches | lr 0.00006 | ms/batch 16.54 | loss 0.06799645\n",
      "| Epoch  35 |   300/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.06751380\n",
      "| Epoch  35 |   350/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.06796032\n",
      "| Epoch  35 |   400/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.06337010\n",
      "| Epoch  35 |   450/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06160654\n",
      "| Epoch  35 |   500/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.06452831\n",
      "| Epoch  35 |   550/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06371051\n",
      "| Epoch  35 |   600/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06596102\n",
      "| Epoch  35 |   650/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.06551294\n",
      "\n",
      "Val set: Average loss: 0.06579965\n",
      "\n",
      "| Epoch  36 |    50/  658 batches | lr 0.00006 | ms/batch 15.03 | loss 0.07272218\n",
      "| Epoch  36 |   100/  658 batches | lr 0.00006 | ms/batch 16.64 | loss 0.06720635\n",
      "| Epoch  36 |   150/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.06247183\n",
      "| Epoch  36 |   200/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.06335392\n",
      "| Epoch  36 |   250/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06782352\n",
      "| Epoch  36 |   300/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.06716937\n",
      "| Epoch  36 |   350/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06800356\n",
      "| Epoch  36 |   400/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.06326370\n",
      "| Epoch  36 |   450/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.06139272\n",
      "| Epoch  36 |   500/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.06445658\n",
      "| Epoch  36 |   550/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.06353378\n",
      "| Epoch  36 |   600/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06574317\n",
      "| Epoch  36 |   650/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.06526410\n",
      "\n",
      "Val set: Average loss: 0.06534705\n",
      "\n",
      "| Epoch  37 |    50/  658 batches | lr 0.00006 | ms/batch 14.96 | loss 0.07247961\n",
      "| Epoch  37 |   100/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06715802\n",
      "| Epoch  37 |   150/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06218308\n",
      "| Epoch  37 |   200/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06321847\n",
      "| Epoch  37 |   250/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.06761379\n",
      "| Epoch  37 |   300/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.06710053\n",
      "| Epoch  37 |   350/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.06764172\n",
      "| Epoch  37 |   400/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.06297335\n",
      "| Epoch  37 |   450/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.06125965\n",
      "| Epoch  37 |   500/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.06422361\n",
      "| Epoch  37 |   550/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.06327592\n",
      "| Epoch  37 |   600/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.06569536\n",
      "| Epoch  37 |   650/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.06521096\n",
      "\n",
      "Val set: Average loss: 0.06505994\n",
      "\n",
      "| Epoch  38 |    50/  658 batches | lr 0.00006 | ms/batch 15.05 | loss 0.07226166\n",
      "| Epoch  38 |   100/  658 batches | lr 0.00006 | ms/batch 16.64 | loss 0.06684572\n",
      "| Epoch  38 |   150/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06217047\n",
      "| Epoch  38 |   200/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.06281715\n",
      "| Epoch  38 |   250/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.06735001\n",
      "| Epoch  38 |   300/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06671914\n",
      "| Epoch  38 |   350/  658 batches | lr 0.00006 | ms/batch 14.58 | loss 0.06745756\n",
      "| Epoch  38 |   400/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.06285017\n",
      "| Epoch  38 |   450/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06117529\n",
      "| Epoch  38 |   500/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.06402116\n",
      "| Epoch  38 |   550/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.06316247\n",
      "| Epoch  38 |   600/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.06545698\n",
      "| Epoch  38 |   650/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06478748\n",
      "\n",
      "Val set: Average loss: 0.06526345\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  39 |    50/  658 batches | lr 0.00006 | ms/batch 14.91 | loss 0.07190284\n",
      "| Epoch  39 |   100/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.06665543\n",
      "| Epoch  39 |   150/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06219995\n",
      "| Epoch  39 |   200/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.06287090\n",
      "| Epoch  39 |   250/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.06716916\n",
      "| Epoch  39 |   300/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.06651746\n",
      "| Epoch  39 |   350/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.06708556\n",
      "| Epoch  39 |   400/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.06276542\n",
      "| Epoch  39 |   450/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.06097313\n",
      "| Epoch  39 |   500/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.06395812\n",
      "| Epoch  39 |   550/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.06311355\n",
      "| Epoch  39 |   600/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06523064\n",
      "| Epoch  39 |   650/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.06480686\n",
      "\n",
      "Val set: Average loss: 0.06497019\n",
      "\n",
      "| Epoch  40 |    50/  658 batches | lr 0.00006 | ms/batch 14.93 | loss 0.07167085\n",
      "| Epoch  40 |   100/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.06639941\n",
      "| Epoch  40 |   150/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.06199594\n",
      "| Epoch  40 |   200/  658 batches | lr 0.00006 | ms/batch 16.53 | loss 0.06279228\n",
      "| Epoch  40 |   250/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06710030\n",
      "| Epoch  40 |   300/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06657843\n",
      "| Epoch  40 |   350/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06709204\n",
      "| Epoch  40 |   400/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06262766\n",
      "| Epoch  40 |   450/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06086310\n",
      "| Epoch  40 |   500/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06396931\n",
      "| Epoch  40 |   550/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.06287324\n",
      "| Epoch  40 |   600/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.06523428\n",
      "| Epoch  40 |   650/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.06468873\n",
      "\n",
      "Val set: Average loss: 0.06469460\n",
      "\n",
      "| Epoch  41 |    50/  658 batches | lr 0.00006 | ms/batch 15.02 | loss 0.07139197\n",
      "| Epoch  41 |   100/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.06629037\n",
      "| Epoch  41 |   150/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06175283\n",
      "| Epoch  41 |   200/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.06250619\n",
      "| Epoch  41 |   250/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.06692641\n",
      "| Epoch  41 |   300/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.06632809\n",
      "| Epoch  41 |   350/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.06690254\n",
      "| Epoch  41 |   400/  658 batches | lr 0.00006 | ms/batch 16.21 | loss 0.06252784\n",
      "| Epoch  41 |   450/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.06067061\n",
      "| Epoch  41 |   500/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.06385140\n",
      "| Epoch  41 |   550/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06276039\n",
      "| Epoch  41 |   600/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.06504087\n",
      "| Epoch  41 |   650/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.06438707\n",
      "\n",
      "Val set: Average loss: 0.06448656\n",
      "\n",
      "| Epoch  42 |    50/  658 batches | lr 0.00006 | ms/batch 15.00 | loss 0.07127426\n",
      "| Epoch  42 |   100/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06615748\n",
      "| Epoch  42 |   150/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.06175978\n",
      "| Epoch  42 |   200/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06229141\n",
      "| Epoch  42 |   250/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.06668387\n",
      "| Epoch  42 |   300/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.06625105\n",
      "| Epoch  42 |   350/  658 batches | lr 0.00006 | ms/batch 14.58 | loss 0.06687065\n",
      "| Epoch  42 |   400/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.06246472\n",
      "| Epoch  42 |   450/  658 batches | lr 0.00006 | ms/batch 14.61 | loss 0.06051085\n",
      "| Epoch  42 |   500/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.06374395\n",
      "| Epoch  42 |   550/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06270394\n",
      "| Epoch  42 |   600/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.06508486\n",
      "| Epoch  42 |   650/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.06436951\n",
      "\n",
      "Val set: Average loss: 0.06437164\n",
      "\n",
      "| Epoch  43 |    50/  658 batches | lr 0.00006 | ms/batch 15.08 | loss 0.07114192\n",
      "| Epoch  43 |   100/  658 batches | lr 0.00006 | ms/batch 16.76 | loss 0.06604814\n",
      "| Epoch  43 |   150/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06155359\n",
      "| Epoch  43 |   200/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.06206254\n",
      "| Epoch  43 |   250/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.06643720\n",
      "| Epoch  43 |   300/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.06619350\n",
      "| Epoch  43 |   350/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.06678017\n",
      "| Epoch  43 |   400/  658 batches | lr 0.00006 | ms/batch 16.30 | loss 0.06247171\n",
      "| Epoch  43 |   450/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.06024893\n",
      "| Epoch  43 |   500/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06351817\n",
      "| Epoch  43 |   550/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.06250286\n",
      "| Epoch  43 |   600/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06493091\n",
      "| Epoch  43 |   650/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.06407723\n",
      "\n",
      "Val set: Average loss: 0.06406632\n",
      "\n",
      "| Epoch  44 |    50/  658 batches | lr 0.00006 | ms/batch 15.08 | loss 0.07096081\n",
      "| Epoch  44 |   100/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06599537\n",
      "| Epoch  44 |   150/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06144728\n",
      "| Epoch  44 |   200/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.06199589\n",
      "| Epoch  44 |   250/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06645968\n",
      "| Epoch  44 |   300/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.06561074\n",
      "| Epoch  44 |   350/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06604237\n",
      "| Epoch  44 |   400/  658 batches | lr 0.00006 | ms/batch 16.40 | loss 0.06231461\n",
      "| Epoch  44 |   450/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06023187\n",
      "| Epoch  44 |   500/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.06341806\n",
      "| Epoch  44 |   550/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.06236026\n",
      "| Epoch  44 |   600/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06473639\n",
      "| Epoch  44 |   650/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.06390770\n",
      "\n",
      "Val set: Average loss: 0.06402301\n",
      "\n",
      "| Epoch  45 |    50/  658 batches | lr 0.00006 | ms/batch 14.95 | loss 0.07086920\n",
      "| Epoch  45 |   100/  658 batches | lr 0.00006 | ms/batch 16.21 | loss 0.06595842\n",
      "| Epoch  45 |   150/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06144785\n",
      "| Epoch  45 |   200/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06185926\n",
      "| Epoch  45 |   250/  658 batches | lr 0.00006 | ms/batch 16.29 | loss 0.06621583\n",
      "| Epoch  45 |   300/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06552547\n",
      "| Epoch  45 |   350/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.06636743\n",
      "| Epoch  45 |   400/  658 batches | lr 0.00006 | ms/batch 16.21 | loss 0.06210224\n",
      "| Epoch  45 |   450/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.06008666\n",
      "| Epoch  45 |   500/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06325330\n",
      "| Epoch  45 |   550/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06221061\n",
      "| Epoch  45 |   600/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.06465856\n",
      "| Epoch  45 |   650/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06392258\n",
      "\n",
      "Val set: Average loss: 0.06389302\n",
      "\n",
      "| Epoch  46 |    50/  658 batches | lr 0.00006 | ms/batch 15.02 | loss 0.07068319\n",
      "| Epoch  46 |   100/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06576146\n",
      "| Epoch  46 |   150/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.06128770\n",
      "| Epoch  46 |   200/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06183523\n",
      "| Epoch  46 |   250/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.06610541\n",
      "| Epoch  46 |   300/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.06543500\n",
      "| Epoch  46 |   350/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06577297\n",
      "| Epoch  46 |   400/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.06213381\n",
      "| Epoch  46 |   450/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.05999255\n",
      "| Epoch  46 |   500/  658 batches | lr 0.00006 | ms/batch 16.54 | loss 0.06317488\n",
      "| Epoch  46 |   550/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06209057\n",
      "| Epoch  46 |   600/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.06464507\n",
      "| Epoch  46 |   650/  658 batches | lr 0.00006 | ms/batch 14.88 | loss 0.06379876\n",
      "\n",
      "Val set: Average loss: 0.06370850\n",
      "\n",
      "| Epoch  47 |    50/  658 batches | lr 0.00006 | ms/batch 14.95 | loss 0.07043590\n",
      "| Epoch  47 |   100/  658 batches | lr 0.00006 | ms/batch 16.40 | loss 0.06553593\n",
      "| Epoch  47 |   150/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.06124963\n",
      "| Epoch  47 |   200/  658 batches | lr 0.00006 | ms/batch 16.56 | loss 0.06165197\n",
      "| Epoch  47 |   250/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.06590294\n",
      "| Epoch  47 |   300/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.06511968\n",
      "| Epoch  47 |   350/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06588058\n",
      "| Epoch  47 |   400/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.06209993\n",
      "| Epoch  47 |   450/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.05990684\n",
      "| Epoch  47 |   500/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.06312614\n",
      "| Epoch  47 |   550/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06218986\n",
      "| Epoch  47 |   600/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.06445233\n",
      "| Epoch  47 |   650/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.06359840\n",
      "\n",
      "Val set: Average loss: 0.06358458\n",
      "\n",
      "| Epoch  48 |    50/  658 batches | lr 0.00006 | ms/batch 14.99 | loss 0.07017827\n",
      "| Epoch  48 |   100/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06529900\n",
      "| Epoch  48 |   150/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.06111106\n",
      "| Epoch  48 |   200/  658 batches | lr 0.00006 | ms/batch 17.14 | loss 0.06139699\n",
      "| Epoch  48 |   250/  658 batches | lr 0.00006 | ms/batch 16.57 | loss 0.06577979\n",
      "| Epoch  48 |   300/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.06560021\n",
      "| Epoch  48 |   350/  658 batches | lr 0.00006 | ms/batch 15.92 | loss 0.06578853\n",
      "| Epoch  48 |   400/  658 batches | lr 0.00006 | ms/batch 16.90 | loss 0.06187323\n",
      "| Epoch  48 |   450/  658 batches | lr 0.00006 | ms/batch 15.40 | loss 0.05978271\n",
      "| Epoch  48 |   500/  658 batches | lr 0.00006 | ms/batch 16.81 | loss 0.06309763\n",
      "| Epoch  48 |   550/  658 batches | lr 0.00006 | ms/batch 16.56 | loss 0.06191227\n",
      "| Epoch  48 |   600/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06443092\n",
      "| Epoch  48 |   650/  658 batches | lr 0.00006 | ms/batch 14.58 | loss 0.06343678\n",
      "\n",
      "Val set: Average loss: 0.06348630\n",
      "\n",
      "| Epoch  49 |    50/  658 batches | lr 0.00006 | ms/batch 15.06 | loss 0.07019169\n",
      "| Epoch  49 |   100/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.06532325\n",
      "| Epoch  49 |   150/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.06108032\n",
      "| Epoch  49 |   200/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.06134690\n",
      "| Epoch  49 |   250/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.06573611\n",
      "| Epoch  49 |   300/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.06481583\n",
      "| Epoch  49 |   350/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.06508411\n",
      "| Epoch  49 |   400/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06191837\n",
      "| Epoch  49 |   450/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.05961908\n",
      "| Epoch  49 |   500/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.06279644\n",
      "| Epoch  49 |   550/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.06194919\n",
      "| Epoch  49 |   600/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.06438229\n",
      "| Epoch  49 |   650/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.06336306\n",
      "\n",
      "Val set: Average loss: 0.06349036\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  50 |    50/  658 batches | lr 0.00006 | ms/batch 14.99 | loss 0.06984843\n",
      "| Epoch  50 |   100/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.06532763\n",
      "| Epoch  50 |   150/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.06103546\n",
      "| Epoch  50 |   200/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06123391\n",
      "| Epoch  50 |   250/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06559444\n",
      "| Epoch  50 |   300/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.06497174\n",
      "| Epoch  50 |   350/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06547523\n",
      "| Epoch  50 |   400/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.06168743\n",
      "| Epoch  50 |   450/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.05948873\n",
      "| Epoch  50 |   500/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.06283872\n",
      "| Epoch  50 |   550/  658 batches | lr 0.00006 | ms/batch 16.53 | loss 0.06191738\n",
      "| Epoch  50 |   600/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.06406579\n",
      "| Epoch  50 |   650/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.06316354\n",
      "\n",
      "Val set: Average loss: 0.06320584\n",
      "\n",
      "| Epoch  51 |    50/  658 batches | lr 0.00006 | ms/batch 15.04 | loss 0.06972419\n",
      "| Epoch  51 |   100/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06496844\n",
      "| Epoch  51 |   150/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06080144\n",
      "| Epoch  51 |   200/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06104351\n",
      "| Epoch  51 |   250/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.06530924\n",
      "| Epoch  51 |   300/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06458774\n",
      "| Epoch  51 |   350/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.06480186\n",
      "| Epoch  51 |   400/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.06171105\n",
      "| Epoch  51 |   450/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.05932637\n",
      "| Epoch  51 |   500/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.06274179\n",
      "| Epoch  51 |   550/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.06172081\n",
      "| Epoch  51 |   600/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06413968\n",
      "| Epoch  51 |   650/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.06300155\n",
      "\n",
      "Val set: Average loss: 0.06320371\n",
      "\n",
      "| Epoch  52 |    50/  658 batches | lr 0.00006 | ms/batch 15.05 | loss 0.06952103\n",
      "| Epoch  52 |   100/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.06491762\n",
      "| Epoch  52 |   150/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.06073738\n",
      "| Epoch  52 |   200/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06087610\n",
      "| Epoch  52 |   250/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.06515826\n",
      "| Epoch  52 |   300/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.06440908\n",
      "| Epoch  52 |   350/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06505847\n",
      "| Epoch  52 |   400/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.06153636\n",
      "| Epoch  52 |   450/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.05917948\n",
      "| Epoch  52 |   500/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.06265556\n",
      "| Epoch  52 |   550/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.06154167\n",
      "| Epoch  52 |   600/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.06398573\n",
      "| Epoch  52 |   650/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.06278767\n",
      "\n",
      "Val set: Average loss: 0.06317780\n",
      "\n",
      "| Epoch  53 |    50/  658 batches | lr 0.00006 | ms/batch 15.01 | loss 0.06949737\n",
      "| Epoch  53 |   100/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.06501897\n",
      "| Epoch  53 |   150/  658 batches | lr 0.00006 | ms/batch 15.02 | loss 0.06071480\n",
      "| Epoch  53 |   200/  658 batches | lr 0.00006 | ms/batch 16.95 | loss 0.06081229\n",
      "| Epoch  53 |   250/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.06512596\n",
      "| Epoch  53 |   300/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.06422322\n",
      "| Epoch  53 |   350/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06462231\n",
      "| Epoch  53 |   400/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.06153938\n",
      "| Epoch  53 |   450/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.05906970\n",
      "| Epoch  53 |   500/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.06266162\n",
      "| Epoch  53 |   550/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.06141156\n",
      "| Epoch  53 |   600/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.06397588\n",
      "| Epoch  53 |   650/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.06275890\n",
      "\n",
      "Val set: Average loss: 0.06302191\n",
      "\n",
      "| Epoch  54 |    50/  658 batches | lr 0.00006 | ms/batch 15.00 | loss 0.06940299\n",
      "| Epoch  54 |   100/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.06492007\n",
      "| Epoch  54 |   150/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.06060355\n",
      "| Epoch  54 |   200/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06064093\n",
      "| Epoch  54 |   250/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06487274\n",
      "| Epoch  54 |   300/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.06408253\n",
      "| Epoch  54 |   350/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.06453458\n",
      "| Epoch  54 |   400/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.06144623\n",
      "| Epoch  54 |   450/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.05889649\n",
      "| Epoch  54 |   500/  658 batches | lr 0.00006 | ms/batch 16.52 | loss 0.06265769\n",
      "| Epoch  54 |   550/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.06129810\n",
      "| Epoch  54 |   600/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.06376599\n",
      "| Epoch  54 |   650/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.06276400\n",
      "\n",
      "Val set: Average loss: 0.06296608\n",
      "\n",
      "| Epoch  55 |    50/  658 batches | lr 0.00006 | ms/batch 15.04 | loss 0.06924531\n",
      "| Epoch  55 |   100/  658 batches | lr 0.00006 | ms/batch 16.52 | loss 0.06482474\n",
      "| Epoch  55 |   150/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.06060850\n",
      "| Epoch  55 |   200/  658 batches | lr 0.00006 | ms/batch 16.55 | loss 0.06047210\n",
      "| Epoch  55 |   250/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06475663\n",
      "| Epoch  55 |   300/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.06392713\n",
      "| Epoch  55 |   350/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.06431926\n",
      "| Epoch  55 |   400/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.06148643\n",
      "| Epoch  55 |   450/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.05891874\n",
      "| Epoch  55 |   500/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.06238716\n",
      "| Epoch  55 |   550/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.06143378\n",
      "| Epoch  55 |   600/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.06379747\n",
      "| Epoch  55 |   650/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.06249229\n",
      "\n",
      "Val set: Average loss: 0.06272159\n",
      "\n",
      "| Epoch  56 |    50/  658 batches | lr 0.00006 | ms/batch 14.99 | loss 0.06906805\n",
      "| Epoch  56 |   100/  658 batches | lr 0.00006 | ms/batch 16.62 | loss 0.06468579\n",
      "| Epoch  56 |   150/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.06044885\n",
      "| Epoch  56 |   200/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.06045255\n",
      "| Epoch  56 |   250/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.06463630\n",
      "| Epoch  56 |   300/  658 batches | lr 0.00006 | ms/batch 14.93 | loss 0.06372065\n",
      "| Epoch  56 |   350/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.06434810\n",
      "| Epoch  56 |   400/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.06143783\n",
      "| Epoch  56 |   450/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.05883825\n",
      "| Epoch  56 |   500/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.06210488\n",
      "| Epoch  56 |   550/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06110372\n",
      "| Epoch  56 |   600/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.06361064\n",
      "| Epoch  56 |   650/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06241862\n",
      "\n",
      "Val set: Average loss: 0.06268948\n",
      "\n",
      "| Epoch  57 |    50/  658 batches | lr 0.00006 | ms/batch 14.97 | loss 0.06888724\n",
      "| Epoch  57 |   100/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.06448014\n",
      "| Epoch  57 |   150/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.06039738\n",
      "| Epoch  57 |   200/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06051631\n",
      "| Epoch  57 |   250/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.06445282\n",
      "| Epoch  57 |   300/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06358241\n",
      "| Epoch  57 |   350/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06417474\n",
      "| Epoch  57 |   400/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06117244\n",
      "| Epoch  57 |   450/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.05866928\n",
      "| Epoch  57 |   500/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06241305\n",
      "| Epoch  57 |   550/  658 batches | lr 0.00006 | ms/batch 16.52 | loss 0.06104351\n",
      "| Epoch  57 |   600/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06343876\n",
      "| Epoch  57 |   650/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06219479\n",
      "\n",
      "Val set: Average loss: 0.06261352\n",
      "\n",
      "| Epoch  58 |    50/  658 batches | lr 0.00006 | ms/batch 14.95 | loss 0.06873378\n",
      "| Epoch  58 |   100/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.06438967\n",
      "| Epoch  58 |   150/  658 batches | lr 0.00006 | ms/batch 14.99 | loss 0.06021565\n",
      "| Epoch  58 |   200/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.06022123\n",
      "| Epoch  58 |   250/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06435538\n",
      "| Epoch  58 |   300/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06354263\n",
      "| Epoch  58 |   350/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06403954\n",
      "| Epoch  58 |   400/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06104194\n",
      "| Epoch  58 |   450/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.05861409\n",
      "| Epoch  58 |   500/  658 batches | lr 0.00006 | ms/batch 16.63 | loss 0.06175679\n",
      "| Epoch  58 |   550/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06099885\n",
      "| Epoch  58 |   600/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06352153\n",
      "| Epoch  58 |   650/  658 batches | lr 0.00006 | ms/batch 14.93 | loss 0.06224093\n",
      "\n",
      "Val set: Average loss: 0.06252176\n",
      "\n",
      "| Epoch  59 |    50/  658 batches | lr 0.00006 | ms/batch 15.02 | loss 0.06862135\n",
      "| Epoch  59 |   100/  658 batches | lr 0.00006 | ms/batch 16.66 | loss 0.06430206\n",
      "| Epoch  59 |   150/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06018701\n",
      "| Epoch  59 |   200/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06019178\n",
      "| Epoch  59 |   250/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.06424012\n",
      "| Epoch  59 |   300/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.06355533\n",
      "| Epoch  59 |   350/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.06415922\n",
      "| Epoch  59 |   400/  658 batches | lr 0.00006 | ms/batch 16.67 | loss 0.06088714\n",
      "| Epoch  59 |   450/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.05849632\n",
      "| Epoch  59 |   500/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06232286\n",
      "| Epoch  59 |   550/  658 batches | lr 0.00006 | ms/batch 16.62 | loss 0.06105975\n",
      "| Epoch  59 |   600/  658 batches | lr 0.00006 | ms/batch 14.88 | loss 0.06344457\n",
      "| Epoch  59 |   650/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06210683\n",
      "\n",
      "Val set: Average loss: 0.06246340\n",
      "\n",
      "| Epoch  60 |    50/  658 batches | lr 0.00006 | ms/batch 15.19 | loss 0.06858472\n",
      "| Epoch  60 |   100/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06422518\n",
      "| Epoch  60 |   150/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.06011295\n",
      "| Epoch  60 |   200/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06003537\n",
      "| Epoch  60 |   250/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06411062\n",
      "| Epoch  60 |   300/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.06329453\n",
      "| Epoch  60 |   350/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06375053\n",
      "| Epoch  60 |   400/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06096151\n",
      "| Epoch  60 |   450/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.05840087\n",
      "| Epoch  60 |   500/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.06220625\n",
      "| Epoch  60 |   550/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06086363\n",
      "| Epoch  60 |   600/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.06329507\n",
      "| Epoch  60 |   650/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.06183477\n",
      "\n",
      "Val set: Average loss: 0.06225322\n",
      "\n",
      "| Epoch  61 |    50/  658 batches | lr 0.00006 | ms/batch 15.15 | loss 0.06844299\n",
      "| Epoch  61 |   100/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06412973\n",
      "| Epoch  61 |   150/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.06002241\n",
      "| Epoch  61 |   200/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.05984877\n",
      "| Epoch  61 |   250/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.06399339\n",
      "| Epoch  61 |   300/  658 batches | lr 0.00006 | ms/batch 15.07 | loss 0.06324078\n",
      "| Epoch  61 |   350/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.06369093\n",
      "| Epoch  61 |   400/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.06097505\n",
      "| Epoch  61 |   450/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.05822594\n",
      "| Epoch  61 |   500/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06201661\n",
      "| Epoch  61 |   550/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06073618\n",
      "| Epoch  61 |   600/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.06314870\n",
      "| Epoch  61 |   650/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.06197422\n",
      "\n",
      "Val set: Average loss: 0.06220047\n",
      "\n",
      "| Epoch  62 |    50/  658 batches | lr 0.00006 | ms/batch 15.09 | loss 0.06833257\n",
      "| Epoch  62 |   100/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.06407133\n",
      "| Epoch  62 |   150/  658 batches | lr 0.00006 | ms/batch 14.94 | loss 0.05991633\n",
      "| Epoch  62 |   200/  658 batches | lr 0.00006 | ms/batch 16.58 | loss 0.05974102\n",
      "| Epoch  62 |   250/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06388764\n",
      "| Epoch  62 |   300/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06314915\n",
      "| Epoch  62 |   350/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06371231\n",
      "| Epoch  62 |   400/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.06079326\n",
      "| Epoch  62 |   450/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.05809371\n",
      "| Epoch  62 |   500/  658 batches | lr 0.00006 | ms/batch 16.52 | loss 0.06185006\n",
      "| Epoch  62 |   550/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.06057439\n",
      "| Epoch  62 |   600/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06306214\n",
      "| Epoch  62 |   650/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06178484\n",
      "\n",
      "Val set: Average loss: 0.06219606\n",
      "\n",
      "| Epoch  63 |    50/  658 batches | lr 0.00006 | ms/batch 15.02 | loss 0.06818074\n",
      "| Epoch  63 |   100/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06402458\n",
      "| Epoch  63 |   150/  658 batches | lr 0.00006 | ms/batch 14.91 | loss 0.05980166\n",
      "| Epoch  63 |   200/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.05972624\n",
      "| Epoch  63 |   250/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06389600\n",
      "| Epoch  63 |   300/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06289646\n",
      "| Epoch  63 |   350/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.06348329\n",
      "| Epoch  63 |   400/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06066983\n",
      "| Epoch  63 |   450/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.05800278\n",
      "| Epoch  63 |   500/  658 batches | lr 0.00006 | ms/batch 16.54 | loss 0.06159478\n",
      "| Epoch  63 |   550/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.06071335\n",
      "| Epoch  63 |   600/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06294414\n",
      "| Epoch  63 |   650/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06169116\n",
      "\n",
      "Val set: Average loss: 0.06211857\n",
      "\n",
      "| Epoch  64 |    50/  658 batches | lr 0.00006 | ms/batch 15.04 | loss 0.06816474\n",
      "| Epoch  64 |   100/  658 batches | lr 0.00006 | ms/batch 16.87 | loss 0.06396489\n",
      "| Epoch  64 |   150/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.05986149\n",
      "| Epoch  64 |   200/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.05955834\n",
      "| Epoch  64 |   250/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06364079\n",
      "| Epoch  64 |   300/  658 batches | lr 0.00006 | ms/batch 14.89 | loss 0.06278148\n",
      "| Epoch  64 |   350/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06338968\n",
      "| Epoch  64 |   400/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.06086764\n",
      "| Epoch  64 |   450/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.05794062\n",
      "| Epoch  64 |   500/  658 batches | lr 0.00006 | ms/batch 16.51 | loss 0.06125106\n",
      "| Epoch  64 |   550/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06054730\n",
      "| Epoch  64 |   600/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.06278515\n",
      "| Epoch  64 |   650/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06161423\n",
      "\n",
      "Val set: Average loss: 0.06197164\n",
      "\n",
      "| Epoch  65 |    50/  658 batches | lr 0.00006 | ms/batch 15.10 | loss 0.06798836\n",
      "| Epoch  65 |   100/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06398034\n",
      "| Epoch  65 |   150/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.05961446\n",
      "| Epoch  65 |   200/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.05942835\n",
      "| Epoch  65 |   250/  658 batches | lr 0.00006 | ms/batch 16.57 | loss 0.06348654\n",
      "| Epoch  65 |   300/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.06287201\n",
      "| Epoch  65 |   350/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.06344497\n",
      "| Epoch  65 |   400/  658 batches | lr 0.00006 | ms/batch 16.40 | loss 0.06060788\n",
      "| Epoch  65 |   450/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.05773681\n",
      "| Epoch  65 |   500/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06142055\n",
      "| Epoch  65 |   550/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.06056858\n",
      "| Epoch  65 |   600/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06277144\n",
      "| Epoch  65 |   650/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.06141134\n",
      "\n",
      "Val set: Average loss: 0.06183635\n",
      "\n",
      "| Epoch  66 |    50/  658 batches | lr 0.00006 | ms/batch 15.04 | loss 0.06773971\n",
      "| Epoch  66 |   100/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.06367725\n",
      "| Epoch  66 |   150/  658 batches | lr 0.00006 | ms/batch 14.88 | loss 0.05967451\n",
      "| Epoch  66 |   200/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.05946124\n",
      "| Epoch  66 |   250/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.06344440\n",
      "| Epoch  66 |   300/  658 batches | lr 0.00006 | ms/batch 15.03 | loss 0.06273005\n",
      "| Epoch  66 |   350/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.06335480\n",
      "| Epoch  66 |   400/  658 batches | lr 0.00006 | ms/batch 16.30 | loss 0.06055138\n",
      "| Epoch  66 |   450/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.05760596\n",
      "| Epoch  66 |   500/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06111659\n",
      "| Epoch  66 |   550/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.06038837\n",
      "| Epoch  66 |   600/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.06259843\n",
      "| Epoch  66 |   650/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06146637\n",
      "\n",
      "Val set: Average loss: 0.06191436\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  67 |    50/  658 batches | lr 0.00006 | ms/batch 15.14 | loss 0.06778627\n",
      "| Epoch  67 |   100/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06381087\n",
      "| Epoch  67 |   150/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.05946912\n",
      "| Epoch  67 |   200/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.05928937\n",
      "| Epoch  67 |   250/  658 batches | lr 0.00006 | ms/batch 16.53 | loss 0.06343311\n",
      "| Epoch  67 |   300/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.06268528\n",
      "| Epoch  67 |   350/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.06334964\n",
      "| Epoch  67 |   400/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06049133\n",
      "| Epoch  67 |   450/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.05761515\n",
      "| Epoch  67 |   500/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06110981\n",
      "| Epoch  67 |   550/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.06033315\n",
      "| Epoch  67 |   600/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.06273916\n",
      "| Epoch  67 |   650/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.06127255\n",
      "\n",
      "Val set: Average loss: 0.06175539\n",
      "\n",
      "| Epoch  68 |    50/  658 batches | lr 0.00006 | ms/batch 15.13 | loss 0.06783431\n",
      "| Epoch  68 |   100/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06374097\n",
      "| Epoch  68 |   150/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.05930573\n",
      "| Epoch  68 |   200/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.05908734\n",
      "| Epoch  68 |   250/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06326219\n",
      "| Epoch  68 |   300/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.06250976\n",
      "| Epoch  68 |   350/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.06322880\n",
      "| Epoch  68 |   400/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.06041956\n",
      "| Epoch  68 |   450/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.05748511\n",
      "| Epoch  68 |   500/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06133651\n",
      "| Epoch  68 |   550/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.06009698\n",
      "| Epoch  68 |   600/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.06266966\n",
      "| Epoch  68 |   650/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.06110380\n",
      "\n",
      "Val set: Average loss: 0.06172975\n",
      "\n",
      "| Epoch  69 |    50/  658 batches | lr 0.00006 | ms/batch 15.07 | loss 0.06752605\n",
      "| Epoch  69 |   100/  658 batches | lr 0.00006 | ms/batch 16.52 | loss 0.06364987\n",
      "| Epoch  69 |   150/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.05926788\n",
      "| Epoch  69 |   200/  658 batches | lr 0.00006 | ms/batch 16.58 | loss 0.05903206\n",
      "| Epoch  69 |   250/  658 batches | lr 0.00006 | ms/batch 16.54 | loss 0.06316666\n",
      "| Epoch  69 |   300/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06243011\n",
      "| Epoch  69 |   350/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06304221\n",
      "| Epoch  69 |   400/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.06049520\n",
      "| Epoch  69 |   450/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.05735842\n",
      "| Epoch  69 |   500/  658 batches | lr 0.00006 | ms/batch 16.57 | loss 0.06097382\n",
      "| Epoch  69 |   550/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.06012124\n",
      "| Epoch  69 |   600/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.06245633\n",
      "| Epoch  69 |   650/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06101617\n",
      "\n",
      "Val set: Average loss: 0.06165796\n",
      "\n",
      "| Epoch  70 |    50/  658 batches | lr 0.00006 | ms/batch 15.15 | loss 0.06748102\n",
      "| Epoch  70 |   100/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.06354564\n",
      "| Epoch  70 |   150/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.05929436\n",
      "| Epoch  70 |   200/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.05892605\n",
      "| Epoch  70 |   250/  658 batches | lr 0.00006 | ms/batch 16.29 | loss 0.06306714\n",
      "| Epoch  70 |   300/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06241755\n",
      "| Epoch  70 |   350/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.06313153\n",
      "| Epoch  70 |   400/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06025575\n",
      "| Epoch  70 |   450/  658 batches | lr 0.00006 | ms/batch 15.05 | loss 0.05732597\n",
      "| Epoch  70 |   500/  658 batches | lr 0.00006 | ms/batch 16.63 | loss 0.06112790\n",
      "| Epoch  70 |   550/  658 batches | lr 0.00006 | ms/batch 16.61 | loss 0.06014222\n",
      "| Epoch  70 |   600/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.06244373\n",
      "| Epoch  70 |   650/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06096099\n",
      "\n",
      "Val set: Average loss: 0.06158793\n",
      "\n",
      "| Epoch  71 |    50/  658 batches | lr 0.00006 | ms/batch 15.05 | loss 0.06718209\n",
      "| Epoch  71 |   100/  658 batches | lr 0.00006 | ms/batch 16.53 | loss 0.06340832\n",
      "| Epoch  71 |   150/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.05910242\n",
      "| Epoch  71 |   200/  658 batches | lr 0.00006 | ms/batch 16.59 | loss 0.05890011\n",
      "| Epoch  71 |   250/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.06296694\n",
      "| Epoch  71 |   300/  658 batches | lr 0.00006 | ms/batch 14.92 | loss 0.06252919\n",
      "| Epoch  71 |   350/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.06361284\n",
      "| Epoch  71 |   400/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06020098\n",
      "| Epoch  71 |   450/  658 batches | lr 0.00006 | ms/batch 14.92 | loss 0.05721759\n",
      "| Epoch  71 |   500/  658 batches | lr 0.00006 | ms/batch 16.54 | loss 0.06066645\n",
      "| Epoch  71 |   550/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.06002172\n",
      "| Epoch  71 |   600/  658 batches | lr 0.00006 | ms/batch 14.88 | loss 0.06240142\n",
      "| Epoch  71 |   650/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.06072840\n",
      "\n",
      "Val set: Average loss: 0.06140099\n",
      "\n",
      "| Epoch  72 |    50/  658 batches | lr 0.00006 | ms/batch 15.18 | loss 0.06739655\n",
      "| Epoch  72 |   100/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.06353196\n",
      "| Epoch  72 |   150/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.05903753\n",
      "| Epoch  72 |   200/  658 batches | lr 0.00006 | ms/batch 16.60 | loss 0.05884069\n",
      "| Epoch  72 |   250/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.06297838\n",
      "| Epoch  72 |   300/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06260229\n",
      "| Epoch  72 |   350/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.06327097\n",
      "| Epoch  72 |   400/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06017311\n",
      "| Epoch  72 |   450/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.05706261\n",
      "| Epoch  72 |   500/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.06106734\n",
      "| Epoch  72 |   550/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.05988347\n",
      "| Epoch  72 |   600/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06230247\n",
      "| Epoch  72 |   650/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06074003\n",
      "\n",
      "Val set: Average loss: 0.06149881\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  73 |    50/  658 batches | lr 0.00006 | ms/batch 15.01 | loss 0.06734625\n",
      "| Epoch  73 |   100/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.06334236\n",
      "| Epoch  73 |   150/  658 batches | lr 0.00006 | ms/batch 14.95 | loss 0.05910310\n",
      "| Epoch  73 |   200/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.05862019\n",
      "| Epoch  73 |   250/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.06271261\n",
      "| Epoch  73 |   300/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.06215330\n",
      "| Epoch  73 |   350/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.06279728\n",
      "| Epoch  73 |   400/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06004934\n",
      "| Epoch  73 |   450/  658 batches | lr 0.00006 | ms/batch 14.89 | loss 0.05704873\n",
      "| Epoch  73 |   500/  658 batches | lr 0.00006 | ms/batch 16.57 | loss 0.06061276\n",
      "| Epoch  73 |   550/  658 batches | lr 0.00006 | ms/batch 16.57 | loss 0.05976394\n",
      "| Epoch  73 |   600/  658 batches | lr 0.00006 | ms/batch 14.94 | loss 0.06221528\n",
      "| Epoch  73 |   650/  658 batches | lr 0.00006 | ms/batch 14.99 | loss 0.06061533\n",
      "\n",
      "Val set: Average loss: 0.06121949\n",
      "\n",
      "| Epoch  74 |    50/  658 batches | lr 0.00006 | ms/batch 15.15 | loss 0.06683093\n",
      "| Epoch  74 |   100/  658 batches | lr 0.00006 | ms/batch 16.59 | loss 0.06318540\n",
      "| Epoch  74 |   150/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.05884656\n",
      "| Epoch  74 |   200/  658 batches | lr 0.00006 | ms/batch 16.51 | loss 0.05852880\n",
      "| Epoch  74 |   250/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.06261527\n",
      "| Epoch  74 |   300/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06203465\n",
      "| Epoch  74 |   350/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.06275268\n",
      "| Epoch  74 |   400/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.06007265\n",
      "| Epoch  74 |   450/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.05700534\n",
      "| Epoch  74 |   500/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.06063411\n",
      "| Epoch  74 |   550/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.05977669\n",
      "| Epoch  74 |   600/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.06213888\n",
      "| Epoch  74 |   650/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06053831\n",
      "\n",
      "Val set: Average loss: 0.06133590\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  75 |    50/  658 batches | lr 0.00006 | ms/batch 15.08 | loss 0.06708102\n",
      "| Epoch  75 |   100/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.06330695\n",
      "| Epoch  75 |   150/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.05887648\n",
      "| Epoch  75 |   200/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.05843747\n",
      "| Epoch  75 |   250/  658 batches | lr 0.00006 | ms/batch 16.79 | loss 0.06251148\n",
      "| Epoch  75 |   300/  658 batches | lr 0.00006 | ms/batch 15.09 | loss 0.06198660\n",
      "| Epoch  75 |   350/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.06263856\n",
      "| Epoch  75 |   400/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.06021052\n",
      "| Epoch  75 |   450/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.05686068\n",
      "| Epoch  75 |   500/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.06064622\n",
      "| Epoch  75 |   550/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05973938\n",
      "| Epoch  75 |   600/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.06202964\n",
      "| Epoch  75 |   650/  658 batches | lr 0.00006 | ms/batch 14.88 | loss 0.06050503\n",
      "\n",
      "Val set: Average loss: 0.06123001\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  76 |    50/  658 batches | lr 0.00006 | ms/batch 15.06 | loss 0.06704647\n",
      "| Epoch  76 |   100/  658 batches | lr 0.00006 | ms/batch 16.40 | loss 0.06316529\n",
      "| Epoch  76 |   150/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.05887609\n",
      "| Epoch  76 |   200/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.05840219\n",
      "| Epoch  76 |   250/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06249419\n",
      "| Epoch  76 |   300/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.06183144\n",
      "| Epoch  76 |   350/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.06265352\n",
      "| Epoch  76 |   400/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.05969337\n",
      "| Epoch  76 |   450/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.05676604\n",
      "| Epoch  76 |   500/  658 batches | lr 0.00006 | ms/batch 16.73 | loss 0.06046795\n",
      "| Epoch  76 |   550/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.05972511\n",
      "| Epoch  76 |   600/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.06205130\n",
      "| Epoch  76 |   650/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.06038395\n",
      "\n",
      "Val set: Average loss: 0.06113106\n",
      "\n",
      "| Epoch  77 |    50/  658 batches | lr 0.00006 | ms/batch 15.13 | loss 0.06670446\n",
      "| Epoch  77 |   100/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.06301713\n",
      "| Epoch  77 |   150/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.05873258\n",
      "| Epoch  77 |   200/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.05824297\n",
      "| Epoch  77 |   250/  658 batches | lr 0.00006 | ms/batch 16.59 | loss 0.06233347\n",
      "| Epoch  77 |   300/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.06169191\n",
      "| Epoch  77 |   350/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06247322\n",
      "| Epoch  77 |   400/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.05980293\n",
      "| Epoch  77 |   450/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.05664890\n",
      "| Epoch  77 |   500/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.06038030\n",
      "| Epoch  77 |   550/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05960144\n",
      "| Epoch  77 |   600/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06187762\n",
      "| Epoch  77 |   650/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.06023379\n",
      "\n",
      "Val set: Average loss: 0.06102308\n",
      "\n",
      "| Epoch  78 |    50/  658 batches | lr 0.00006 | ms/batch 15.10 | loss 0.06667504\n",
      "| Epoch  78 |   100/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.06296064\n",
      "| Epoch  78 |   150/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.05858716\n",
      "| Epoch  78 |   200/  658 batches | lr 0.00006 | ms/batch 16.51 | loss 0.05811393\n",
      "| Epoch  78 |   250/  658 batches | lr 0.00006 | ms/batch 16.51 | loss 0.06230003\n",
      "| Epoch  78 |   300/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06165217\n",
      "| Epoch  78 |   350/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.06240949\n",
      "| Epoch  78 |   400/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.05959274\n",
      "| Epoch  78 |   450/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.05652099\n",
      "| Epoch  78 |   500/  658 batches | lr 0.00006 | ms/batch 16.30 | loss 0.06012124\n",
      "| Epoch  78 |   550/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.05935799\n",
      "| Epoch  78 |   600/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.06178296\n",
      "| Epoch  78 |   650/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.06006886\n",
      "\n",
      "Val set: Average loss: 0.06104826\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  79 |    50/  658 batches | lr 0.00006 | ms/batch 14.97 | loss 0.06663352\n",
      "| Epoch  79 |   100/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.06294987\n",
      "| Epoch  79 |   150/  658 batches | lr 0.00006 | ms/batch 15.00 | loss 0.05844828\n",
      "| Epoch  79 |   200/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.05815414\n",
      "| Epoch  79 |   250/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.06221040\n",
      "| Epoch  79 |   300/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.06157930\n",
      "| Epoch  79 |   350/  658 batches | lr 0.00006 | ms/batch 15.01 | loss 0.06223707\n",
      "| Epoch  79 |   400/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.05964248\n",
      "| Epoch  79 |   450/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.05640383\n",
      "| Epoch  79 |   500/  658 batches | lr 0.00006 | ms/batch 16.64 | loss 0.06012855\n",
      "| Epoch  79 |   550/  658 batches | lr 0.00006 | ms/batch 16.52 | loss 0.05935403\n",
      "| Epoch  79 |   600/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.06181640\n",
      "| Epoch  79 |   650/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.06005116\n",
      "\n",
      "Val set: Average loss: 0.06098475\n",
      "\n",
      "| Epoch  80 |    50/  658 batches | lr 0.00006 | ms/batch 15.08 | loss 0.06651909\n",
      "| Epoch  80 |   100/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.06285073\n",
      "| Epoch  80 |   150/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.05839726\n",
      "| Epoch  80 |   200/  658 batches | lr 0.00006 | ms/batch 16.59 | loss 0.05793177\n",
      "| Epoch  80 |   250/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.06210871\n",
      "| Epoch  80 |   300/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.06136338\n",
      "| Epoch  80 |   350/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.06223930\n",
      "| Epoch  80 |   400/  658 batches | lr 0.00006 | ms/batch 16.81 | loss 0.05948032\n",
      "| Epoch  80 |   450/  658 batches | lr 0.00006 | ms/batch 15.17 | loss 0.05634308\n",
      "| Epoch  80 |   500/  658 batches | lr 0.00006 | ms/batch 16.40 | loss 0.06011594\n",
      "| Epoch  80 |   550/  658 batches | lr 0.00006 | ms/batch 16.62 | loss 0.05929375\n",
      "| Epoch  80 |   600/  658 batches | lr 0.00006 | ms/batch 14.61 | loss 0.06162682\n",
      "| Epoch  80 |   650/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.05996739\n",
      "\n",
      "Val set: Average loss: 0.06094196\n",
      "\n",
      "| Epoch  81 |    50/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.06623465\n",
      "| Epoch  81 |   100/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.06268548\n",
      "| Epoch  81 |   150/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.05832345\n",
      "| Epoch  81 |   200/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.05787661\n",
      "| Epoch  81 |   250/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06203211\n",
      "| Epoch  81 |   300/  658 batches | lr 0.00006 | ms/batch 14.93 | loss 0.06128809\n",
      "| Epoch  81 |   350/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06208493\n",
      "| Epoch  81 |   400/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.05961145\n",
      "| Epoch  81 |   450/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.05629642\n",
      "| Epoch  81 |   500/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.05993043\n",
      "| Epoch  81 |   550/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.05914764\n",
      "| Epoch  81 |   600/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.06170791\n",
      "| Epoch  81 |   650/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.05981215\n",
      "\n",
      "Val set: Average loss: 0.06082007\n",
      "\n",
      "| Epoch  82 |    50/  658 batches | lr 0.00006 | ms/batch 14.96 | loss 0.06617857\n",
      "| Epoch  82 |   100/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.06258645\n",
      "| Epoch  82 |   150/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.05816344\n",
      "| Epoch  82 |   200/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.05773568\n",
      "| Epoch  82 |   250/  658 batches | lr 0.00006 | ms/batch 16.52 | loss 0.06195239\n",
      "| Epoch  82 |   300/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06131999\n",
      "| Epoch  82 |   350/  658 batches | lr 0.00006 | ms/batch 14.61 | loss 0.06210162\n",
      "| Epoch  82 |   400/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.05943675\n",
      "| Epoch  82 |   450/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.05626302\n",
      "| Epoch  82 |   500/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.05978283\n",
      "| Epoch  82 |   550/  658 batches | lr 0.00006 | ms/batch 16.52 | loss 0.05910423\n",
      "| Epoch  82 |   600/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06167382\n",
      "| Epoch  82 |   650/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.05983886\n",
      "\n",
      "Val set: Average loss: 0.06077757\n",
      "\n",
      "| Epoch  83 |    50/  658 batches | lr 0.00006 | ms/batch 14.91 | loss 0.06623228\n",
      "| Epoch  83 |   100/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.06264192\n",
      "| Epoch  83 |   150/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.05817148\n",
      "| Epoch  83 |   200/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.05772891\n",
      "| Epoch  83 |   250/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06175801\n",
      "| Epoch  83 |   300/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06124500\n",
      "| Epoch  83 |   350/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.06201019\n",
      "| Epoch  83 |   400/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.05940687\n",
      "| Epoch  83 |   450/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.05612515\n",
      "| Epoch  83 |   500/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06004112\n",
      "| Epoch  83 |   550/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.05915905\n",
      "| Epoch  83 |   600/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.06140322\n",
      "| Epoch  83 |   650/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.05961819\n",
      "\n",
      "Val set: Average loss: 0.06073178\n",
      "\n",
      "| Epoch  84 |    50/  658 batches | lr 0.00006 | ms/batch 15.07 | loss 0.06605324\n",
      "| Epoch  84 |   100/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.06247741\n",
      "| Epoch  84 |   150/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.05798111\n",
      "| Epoch  84 |   200/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.05765176\n",
      "| Epoch  84 |   250/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.06170347\n",
      "| Epoch  84 |   300/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.06127841\n",
      "| Epoch  84 |   350/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06184857\n",
      "| Epoch  84 |   400/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.05932876\n",
      "| Epoch  84 |   450/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.05597340\n",
      "| Epoch  84 |   500/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.05999735\n",
      "| Epoch  84 |   550/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.05900278\n",
      "| Epoch  84 |   600/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.06132434\n",
      "| Epoch  84 |   650/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.05949560\n",
      "\n",
      "Val set: Average loss: 0.06073086\n",
      "\n",
      "| Epoch  85 |    50/  658 batches | lr 0.00006 | ms/batch 15.15 | loss 0.06607560\n",
      "| Epoch  85 |   100/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.06249323\n",
      "| Epoch  85 |   150/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.05790843\n",
      "| Epoch  85 |   200/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.05751798\n",
      "| Epoch  85 |   250/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06160927\n",
      "| Epoch  85 |   300/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.06100757\n",
      "| Epoch  85 |   350/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06188377\n",
      "| Epoch  85 |   400/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.05937483\n",
      "| Epoch  85 |   450/  658 batches | lr 0.00006 | ms/batch 14.91 | loss 0.05604034\n",
      "| Epoch  85 |   500/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05965486\n",
      "| Epoch  85 |   550/  658 batches | lr 0.00006 | ms/batch 16.85 | loss 0.05885509\n",
      "| Epoch  85 |   600/  658 batches | lr 0.00006 | ms/batch 14.95 | loss 0.06137120\n",
      "| Epoch  85 |   650/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.05936887\n",
      "\n",
      "Val set: Average loss: 0.06065778\n",
      "\n",
      "| Epoch  86 |    50/  658 batches | lr 0.00006 | ms/batch 15.94 | loss 0.06599119\n",
      "| Epoch  86 |   100/  658 batches | lr 0.00006 | ms/batch 16.60 | loss 0.06238983\n",
      "| Epoch  86 |   150/  658 batches | lr 0.00006 | ms/batch 15.79 | loss 0.05792550\n",
      "| Epoch  86 |   200/  658 batches | lr 0.00006 | ms/batch 16.61 | loss 0.05741762\n",
      "| Epoch  86 |   250/  658 batches | lr 0.00006 | ms/batch 16.52 | loss 0.06150542\n",
      "| Epoch  86 |   300/  658 batches | lr 0.00006 | ms/batch 15.42 | loss 0.06107120\n",
      "| Epoch  86 |   350/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.06189716\n",
      "| Epoch  86 |   400/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.05921388\n",
      "| Epoch  86 |   450/  658 batches | lr 0.00006 | ms/batch 15.00 | loss 0.05592480\n",
      "| Epoch  86 |   500/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.05956826\n",
      "| Epoch  86 |   550/  658 batches | lr 0.00006 | ms/batch 16.61 | loss 0.05883164\n",
      "| Epoch  86 |   600/  658 batches | lr 0.00006 | ms/batch 14.61 | loss 0.06125156\n",
      "| Epoch  86 |   650/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.05933006\n",
      "\n",
      "Val set: Average loss: 0.06061133\n",
      "\n",
      "| Epoch  87 |    50/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.06556819\n",
      "| Epoch  87 |   100/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06229573\n",
      "| Epoch  87 |   150/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.05782925\n",
      "| Epoch  87 |   200/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05738022\n",
      "| Epoch  87 |   250/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.06141974\n",
      "| Epoch  87 |   300/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.06091194\n",
      "| Epoch  87 |   350/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.06167915\n",
      "| Epoch  87 |   400/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.05944914\n",
      "| Epoch  87 |   450/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.05570627\n",
      "| Epoch  87 |   500/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.05965685\n",
      "| Epoch  87 |   550/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.05878728\n",
      "| Epoch  87 |   600/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06125994\n",
      "| Epoch  87 |   650/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.05934128\n",
      "\n",
      "Val set: Average loss: 0.06051454\n",
      "\n",
      "| Epoch  88 |    50/  658 batches | lr 0.00006 | ms/batch 15.00 | loss 0.06559870\n",
      "| Epoch  88 |   100/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.06228435\n",
      "| Epoch  88 |   150/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.05772620\n",
      "| Epoch  88 |   200/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.05720183\n",
      "| Epoch  88 |   250/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06137603\n",
      "| Epoch  88 |   300/  658 batches | lr 0.00006 | ms/batch 14.93 | loss 0.06089003\n",
      "| Epoch  88 |   350/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.06171164\n",
      "| Epoch  88 |   400/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.05916271\n",
      "| Epoch  88 |   450/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.05570592\n",
      "| Epoch  88 |   500/  658 batches | lr 0.00006 | ms/batch 16.40 | loss 0.05953253\n",
      "| Epoch  88 |   550/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.05874706\n",
      "| Epoch  88 |   600/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.06117621\n",
      "| Epoch  88 |   650/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.05925921\n",
      "\n",
      "Val set: Average loss: 0.06057480\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  89 |    50/  658 batches | lr 0.00006 | ms/batch 15.02 | loss 0.06544427\n",
      "| Epoch  89 |   100/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.06205930\n",
      "| Epoch  89 |   150/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.05763098\n",
      "| Epoch  89 |   200/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.05726442\n",
      "| Epoch  89 |   250/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.06121395\n",
      "| Epoch  89 |   300/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.06068858\n",
      "| Epoch  89 |   350/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.06170230\n",
      "| Epoch  89 |   400/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.05899727\n",
      "| Epoch  89 |   450/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.05561617\n",
      "| Epoch  89 |   500/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.05956009\n",
      "| Epoch  89 |   550/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.05865240\n",
      "| Epoch  89 |   600/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.06100335\n",
      "| Epoch  89 |   650/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.05916968\n",
      "\n",
      "Val set: Average loss: 0.06058285\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  90 |    50/  658 batches | lr 0.00006 | ms/batch 15.00 | loss 0.06538019\n",
      "| Epoch  90 |   100/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.06209472\n",
      "| Epoch  90 |   150/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.05759282\n",
      "| Epoch  90 |   200/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.05725870\n",
      "| Epoch  90 |   250/  658 batches | lr 0.00006 | ms/batch 16.54 | loss 0.06110442\n",
      "| Epoch  90 |   300/  658 batches | lr 0.00006 | ms/batch 15.21 | loss 0.06076953\n",
      "| Epoch  90 |   350/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06180099\n",
      "| Epoch  90 |   400/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.05925176\n",
      "| Epoch  90 |   450/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.05562495\n",
      "| Epoch  90 |   500/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.06013723\n",
      "| Epoch  90 |   550/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.05876121\n",
      "| Epoch  90 |   600/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.06092478\n",
      "| Epoch  90 |   650/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.05918115\n",
      "\n",
      "Val set: Average loss: 0.06047316\n",
      "\n",
      "| Epoch  91 |    50/  658 batches | lr 0.00006 | ms/batch 15.03 | loss 0.06527066\n",
      "| Epoch  91 |   100/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.06208401\n",
      "| Epoch  91 |   150/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.05752987\n",
      "| Epoch  91 |   200/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.05701367\n",
      "| Epoch  91 |   250/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.06103226\n",
      "| Epoch  91 |   300/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06062664\n",
      "| Epoch  91 |   350/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06165234\n",
      "| Epoch  91 |   400/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05918243\n",
      "| Epoch  91 |   450/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.05539441\n",
      "| Epoch  91 |   500/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.05948908\n",
      "| Epoch  91 |   550/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.05841458\n",
      "| Epoch  91 |   600/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.06088194\n",
      "| Epoch  91 |   650/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.05905794\n",
      "\n",
      "Val set: Average loss: 0.06047113\n",
      "\n",
      "| Epoch  92 |    50/  658 batches | lr 0.00006 | ms/batch 15.04 | loss 0.06517902\n",
      "| Epoch  92 |   100/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.06190341\n",
      "| Epoch  92 |   150/  658 batches | lr 0.00006 | ms/batch 14.89 | loss 0.05751948\n",
      "| Epoch  92 |   200/  658 batches | lr 0.00006 | ms/batch 16.60 | loss 0.05694602\n",
      "| Epoch  92 |   250/  658 batches | lr 0.00006 | ms/batch 16.58 | loss 0.06100784\n",
      "| Epoch  92 |   300/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06049400\n",
      "| Epoch  92 |   350/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06155033\n",
      "| Epoch  92 |   400/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.05892078\n",
      "| Epoch  92 |   450/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05529659\n",
      "| Epoch  92 |   500/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.05955274\n",
      "| Epoch  92 |   550/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.05845709\n",
      "| Epoch  92 |   600/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06073196\n",
      "| Epoch  92 |   650/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.05881541\n",
      "\n",
      "Val set: Average loss: 0.06031424\n",
      "\n",
      "| Epoch  93 |    50/  658 batches | lr 0.00006 | ms/batch 14.96 | loss 0.06512528\n",
      "| Epoch  93 |   100/  658 batches | lr 0.00006 | ms/batch 16.54 | loss 0.06184913\n",
      "| Epoch  93 |   150/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.05739288\n",
      "| Epoch  93 |   200/  658 batches | lr 0.00006 | ms/batch 16.56 | loss 0.05684839\n",
      "| Epoch  93 |   250/  658 batches | lr 0.00006 | ms/batch 16.55 | loss 0.06092896\n",
      "| Epoch  93 |   300/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.06054585\n",
      "| Epoch  93 |   350/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.06158897\n",
      "| Epoch  93 |   400/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05887570\n",
      "| Epoch  93 |   450/  658 batches | lr 0.00006 | ms/batch 15.11 | loss 0.05514489\n",
      "| Epoch  93 |   500/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.05911536\n",
      "| Epoch  93 |   550/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.05826652\n",
      "| Epoch  93 |   600/  658 batches | lr 0.00006 | ms/batch 14.93 | loss 0.06075960\n",
      "| Epoch  93 |   650/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.05872903\n",
      "\n",
      "Val set: Average loss: 0.06028048\n",
      "\n",
      "| Epoch  94 |    50/  658 batches | lr 0.00006 | ms/batch 15.12 | loss 0.06491428\n",
      "| Epoch  94 |   100/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.06171910\n",
      "| Epoch  94 |   150/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.05734958\n",
      "| Epoch  94 |   200/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.05681865\n",
      "| Epoch  94 |   250/  658 batches | lr 0.00006 | ms/batch 16.64 | loss 0.06084054\n",
      "| Epoch  94 |   300/  658 batches | lr 0.00006 | ms/batch 14.95 | loss 0.06046226\n",
      "| Epoch  94 |   350/  658 batches | lr 0.00006 | ms/batch 14.93 | loss 0.06130811\n",
      "| Epoch  94 |   400/  658 batches | lr 0.00006 | ms/batch 16.56 | loss 0.05897575\n",
      "| Epoch  94 |   450/  658 batches | lr 0.00006 | ms/batch 15.04 | loss 0.05506356\n",
      "| Epoch  94 |   500/  658 batches | lr 0.00006 | ms/batch 16.66 | loss 0.05939465\n",
      "| Epoch  94 |   550/  658 batches | lr 0.00006 | ms/batch 16.53 | loss 0.05819524\n",
      "| Epoch  94 |   600/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.06048182\n",
      "| Epoch  94 |   650/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.05878516\n",
      "\n",
      "Val set: Average loss: 0.06027070\n",
      "\n",
      "| Epoch  95 |    50/  658 batches | lr 0.00006 | ms/batch 15.25 | loss 0.06493807\n",
      "| Epoch  95 |   100/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.06173796\n",
      "| Epoch  95 |   150/  658 batches | lr 0.00006 | ms/batch 14.91 | loss 0.05720846\n",
      "| Epoch  95 |   200/  658 batches | lr 0.00006 | ms/batch 16.55 | loss 0.05674391\n",
      "| Epoch  95 |   250/  658 batches | lr 0.00006 | ms/batch 16.57 | loss 0.06089085\n",
      "| Epoch  95 |   300/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.06052470\n",
      "| Epoch  95 |   350/  658 batches | lr 0.00006 | ms/batch 15.00 | loss 0.06130390\n",
      "| Epoch  95 |   400/  658 batches | lr 0.00006 | ms/batch 16.54 | loss 0.05909014\n",
      "| Epoch  95 |   450/  658 batches | lr 0.00006 | ms/batch 14.94 | loss 0.05497910\n",
      "| Epoch  95 |   500/  658 batches | lr 0.00006 | ms/batch 16.55 | loss 0.05942678\n",
      "| Epoch  95 |   550/  658 batches | lr 0.00006 | ms/batch 16.75 | loss 0.05818910\n",
      "| Epoch  95 |   600/  658 batches | lr 0.00006 | ms/batch 14.88 | loss 0.06065059\n",
      "| Epoch  95 |   650/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.05867773\n",
      "\n",
      "Val set: Average loss: 0.06021595\n",
      "\n",
      "| Epoch  96 |    50/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.06486024\n",
      "| Epoch  96 |   100/  658 batches | lr 0.00006 | ms/batch 17.98 | loss 0.06160008\n",
      "| Epoch  96 |   150/  658 batches | lr 0.00006 | ms/batch 15.46 | loss 0.05717290\n",
      "| Epoch  96 |   200/  658 batches | lr 0.00006 | ms/batch 17.01 | loss 0.05667879\n",
      "| Epoch  96 |   250/  658 batches | lr 0.00006 | ms/batch 17.23 | loss 0.06073060\n",
      "| Epoch  96 |   300/  658 batches | lr 0.00006 | ms/batch 19.29 | loss 0.06039467\n",
      "| Epoch  96 |   350/  658 batches | lr 0.00006 | ms/batch 14.91 | loss 0.06111679\n",
      "| Epoch  96 |   400/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.05891970\n",
      "| Epoch  96 |   450/  658 batches | lr 0.00006 | ms/batch 14.93 | loss 0.05487943\n",
      "| Epoch  96 |   500/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.05917492\n",
      "| Epoch  96 |   550/  658 batches | lr 0.00006 | ms/batch 16.59 | loss 0.05808994\n",
      "| Epoch  96 |   600/  658 batches | lr 0.00006 | ms/batch 15.02 | loss 0.06063676\n",
      "| Epoch  96 |   650/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.05850153\n",
      "\n",
      "Val set: Average loss: 0.06029399\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  97 |    50/  658 batches | lr 0.00006 | ms/batch 15.21 | loss 0.06469453\n",
      "| Epoch  97 |   100/  658 batches | lr 0.00006 | ms/batch 16.58 | loss 0.06149657\n",
      "| Epoch  97 |   150/  658 batches | lr 0.00006 | ms/batch 14.99 | loss 0.05713821\n",
      "| Epoch  97 |   200/  658 batches | lr 0.00006 | ms/batch 16.66 | loss 0.05665876\n",
      "| Epoch  97 |   250/  658 batches | lr 0.00006 | ms/batch 16.65 | loss 0.06063318\n",
      "| Epoch  97 |   300/  658 batches | lr 0.00006 | ms/batch 14.89 | loss 0.06062550\n",
      "| Epoch  97 |   350/  658 batches | lr 0.00006 | ms/batch 14.94 | loss 0.06109212\n",
      "| Epoch  97 |   400/  658 batches | lr 0.00006 | ms/batch 16.60 | loss 0.05881446\n",
      "| Epoch  97 |   450/  658 batches | lr 0.00006 | ms/batch 15.01 | loss 0.05483142\n",
      "| Epoch  97 |   500/  658 batches | lr 0.00006 | ms/batch 16.62 | loss 0.05927577\n",
      "| Epoch  97 |   550/  658 batches | lr 0.00006 | ms/batch 16.75 | loss 0.05800911\n",
      "| Epoch  97 |   600/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.06048039\n",
      "| Epoch  97 |   650/  658 batches | lr 0.00006 | ms/batch 14.91 | loss 0.05845323\n",
      "\n",
      "Val set: Average loss: 0.06016265\n",
      "\n",
      "| Epoch  98 |    50/  658 batches | lr 0.00006 | ms/batch 15.13 | loss 0.06465358\n",
      "| Epoch  98 |   100/  658 batches | lr 0.00006 | ms/batch 16.59 | loss 0.06144483\n",
      "| Epoch  98 |   150/  658 batches | lr 0.00006 | ms/batch 14.99 | loss 0.05701317\n",
      "| Epoch  98 |   200/  658 batches | lr 0.00006 | ms/batch 16.59 | loss 0.05648610\n",
      "| Epoch  98 |   250/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.06058266\n",
      "| Epoch  98 |   300/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.06069959\n",
      "| Epoch  98 |   350/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.06112018\n",
      "| Epoch  98 |   400/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.05875717\n",
      "| Epoch  98 |   450/  658 batches | lr 0.00006 | ms/batch 14.89 | loss 0.05472511\n",
      "| Epoch  98 |   500/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.05908770\n",
      "| Epoch  98 |   550/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.05790088\n",
      "| Epoch  98 |   600/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.06044067\n",
      "| Epoch  98 |   650/  658 batches | lr 0.00006 | ms/batch 14.91 | loss 0.05849820\n",
      "\n",
      "Val set: Average loss: 0.06025642\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  99 |    50/  658 batches | lr 0.00006 | ms/batch 15.23 | loss 0.06454340\n",
      "| Epoch  99 |   100/  658 batches | lr 0.00006 | ms/batch 16.51 | loss 0.06148018\n",
      "| Epoch  99 |   150/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.05702722\n",
      "| Epoch  99 |   200/  658 batches | lr 0.00006 | ms/batch 16.60 | loss 0.05646908\n",
      "| Epoch  99 |   250/  658 batches | lr 0.00006 | ms/batch 16.64 | loss 0.06048632\n",
      "| Epoch  99 |   300/  658 batches | lr 0.00006 | ms/batch 14.94 | loss 0.06070621\n",
      "| Epoch  99 |   350/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.06104519\n",
      "| Epoch  99 |   400/  658 batches | lr 0.00006 | ms/batch 16.58 | loss 0.05874353\n",
      "| Epoch  99 |   450/  658 batches | lr 0.00006 | ms/batch 14.94 | loss 0.05464102\n",
      "| Epoch  99 |   500/  658 batches | lr 0.00006 | ms/batch 16.66 | loss 0.05916527\n",
      "| Epoch  99 |   550/  658 batches | lr 0.00006 | ms/batch 16.72 | loss 0.05793323\n",
      "| Epoch  99 |   600/  658 batches | lr 0.00006 | ms/batch 15.22 | loss 0.06043561\n",
      "| Epoch  99 |   650/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.05830082\n",
      "\n",
      "Val set: Average loss: 0.06031194\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch 100 |    50/  658 batches | lr 0.00006 | ms/batch 15.11 | loss 0.06451452\n",
      "| Epoch 100 |   100/  658 batches | lr 0.00006 | ms/batch 16.58 | loss 0.06138263\n",
      "| Epoch 100 |   150/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.05694373\n",
      "| Epoch 100 |   200/  658 batches | lr 0.00006 | ms/batch 16.57 | loss 0.05641227\n",
      "| Epoch 100 |   250/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.06041846\n",
      "| Epoch 100 |   300/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.06061710\n",
      "| Epoch 100 |   350/  658 batches | lr 0.00006 | ms/batch 15.11 | loss 0.06106790\n",
      "| Epoch 100 |   400/  658 batches | lr 0.00006 | ms/batch 16.88 | loss 0.05877201\n",
      "| Epoch 100 |   450/  658 batches | lr 0.00006 | ms/batch 14.89 | loss 0.05462306\n",
      "| Epoch 100 |   500/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.05911970\n",
      "| Epoch 100 |   550/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.05788213\n",
      "| Epoch 100 |   600/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.06038645\n",
      "| Epoch 100 |   650/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.05826117\n",
      "\n",
      "Val set: Average loss: 0.06019111\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch 101 |    50/  658 batches | lr 0.00006 | ms/batch 15.18 | loss 0.06440754\n",
      "| Epoch 101 |   100/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.06136790\n",
      "| Epoch 101 |   150/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.05678666\n",
      "| Epoch 101 |   200/  658 batches | lr 0.00006 | ms/batch 16.51 | loss 0.05634455\n",
      "| Epoch 101 |   250/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.06035490\n",
      "| Epoch 101 |   300/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.06036518\n",
      "| Epoch 101 |   350/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.06086399\n",
      "| Epoch 101 |   400/  658 batches | lr 0.00006 | ms/batch 18.37 | loss 0.05894358\n",
      "| Epoch 101 |   450/  658 batches | lr 0.00006 | ms/batch 15.31 | loss 0.05463787\n",
      "| Epoch 101 |   500/  658 batches | lr 0.00006 | ms/batch 16.96 | loss 0.05884868\n",
      "| Epoch 101 |   550/  658 batches | lr 0.00006 | ms/batch 20.24 | loss 0.05765773\n",
      "| Epoch 101 |   600/  658 batches | lr 0.00006 | ms/batch 15.28 | loss 0.06036383\n",
      "| Epoch 101 |   650/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.05824438\n",
      "\n",
      "Val set: Average loss: 0.06014932\n",
      "\n",
      "| Epoch 102 |    50/  658 batches | lr 0.00006 | ms/batch 15.08 | loss 0.06429107\n",
      "| Epoch 102 |   100/  658 batches | lr 0.00006 | ms/batch 16.60 | loss 0.06124252\n",
      "| Epoch 102 |   150/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.05678109\n",
      "| Epoch 102 |   200/  658 batches | lr 0.00006 | ms/batch 17.29 | loss 0.05634456\n",
      "| Epoch 102 |   250/  658 batches | lr 0.00006 | ms/batch 16.85 | loss 0.06023337\n",
      "| Epoch 102 |   300/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06049875\n",
      "| Epoch 102 |   350/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.06086178\n",
      "| Epoch 102 |   400/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.05867338\n",
      "| Epoch 102 |   450/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.05448123\n",
      "| Epoch 102 |   500/  658 batches | lr 0.00006 | ms/batch 16.65 | loss 0.05855987\n",
      "| Epoch 102 |   550/  658 batches | lr 0.00006 | ms/batch 16.58 | loss 0.05750216\n",
      "| Epoch 102 |   600/  658 batches | lr 0.00006 | ms/batch 15.02 | loss 0.06024374\n",
      "| Epoch 102 |   650/  658 batches | lr 0.00006 | ms/batch 16.62 | loss 0.05811270\n",
      "\n",
      "Val set: Average loss: 0.06008808\n",
      "\n",
      "| Epoch 103 |    50/  658 batches | lr 0.00006 | ms/batch 15.85 | loss 0.06419706\n",
      "| Epoch 103 |   100/  658 batches | lr 0.00006 | ms/batch 17.29 | loss 0.06111843\n",
      "| Epoch 103 |   150/  658 batches | lr 0.00006 | ms/batch 15.05 | loss 0.05673791\n",
      "| Epoch 103 |   200/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05634696\n",
      "| Epoch 103 |   250/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.06014077\n",
      "| Epoch 103 |   300/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06040143\n",
      "| Epoch 103 |   350/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06081355\n",
      "| Epoch 103 |   400/  658 batches | lr 0.00006 | ms/batch 16.62 | loss 0.05845245\n",
      "| Epoch 103 |   450/  658 batches | lr 0.00006 | ms/batch 15.22 | loss 0.05438642\n",
      "| Epoch 103 |   500/  658 batches | lr 0.00006 | ms/batch 16.56 | loss 0.05878084\n",
      "| Epoch 103 |   550/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.05755232\n",
      "| Epoch 103 |   600/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.06020211\n",
      "| Epoch 103 |   650/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.05802511\n",
      "\n",
      "Val set: Average loss: 0.05997038\n",
      "\n",
      "| Epoch 104 |    50/  658 batches | lr 0.00006 | ms/batch 15.79 | loss 0.06407707\n",
      "| Epoch 104 |   100/  658 batches | lr 0.00006 | ms/batch 16.96 | loss 0.06101431\n",
      "| Epoch 104 |   150/  658 batches | lr 0.00006 | ms/batch 15.71 | loss 0.05671519\n",
      "| Epoch 104 |   200/  658 batches | lr 0.00006 | ms/batch 17.20 | loss 0.05634746\n",
      "| Epoch 104 |   250/  658 batches | lr 0.00006 | ms/batch 17.59 | loss 0.06010718\n",
      "| Epoch 104 |   300/  658 batches | lr 0.00006 | ms/batch 15.12 | loss 0.06036523\n",
      "| Epoch 104 |   350/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06064337\n",
      "| Epoch 104 |   400/  658 batches | lr 0.00006 | ms/batch 18.80 | loss 0.05854647\n",
      "| Epoch 104 |   450/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.05439790\n",
      "| Epoch 104 |   500/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.05927263\n",
      "| Epoch 104 |   550/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.05754804\n",
      "| Epoch 104 |   600/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.06015240\n",
      "| Epoch 104 |   650/  658 batches | lr 0.00006 | ms/batch 15.33 | loss 0.05800386\n",
      "\n",
      "Val set: Average loss: 0.05988615\n",
      "\n",
      "| Epoch 105 |    50/  658 batches | lr 0.00006 | ms/batch 14.97 | loss 0.06401653\n",
      "| Epoch 105 |   100/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.06103975\n",
      "| Epoch 105 |   150/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.05671229\n",
      "| Epoch 105 |   200/  658 batches | lr 0.00006 | ms/batch 16.54 | loss 0.05624982\n",
      "| Epoch 105 |   250/  658 batches | lr 0.00006 | ms/batch 17.45 | loss 0.05997838\n",
      "| Epoch 105 |   300/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.06042127\n",
      "| Epoch 105 |   350/  658 batches | lr 0.00006 | ms/batch 15.22 | loss 0.06072697\n",
      "| Epoch 105 |   400/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.05849726\n",
      "| Epoch 105 |   450/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.05431447\n",
      "| Epoch 105 |   500/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.05932035\n",
      "| Epoch 105 |   550/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.05747725\n",
      "| Epoch 105 |   600/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.06001018\n",
      "| Epoch 105 |   650/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.05800379\n",
      "\n",
      "Val set: Average loss: 0.05978846\n",
      "\n",
      "| Epoch 106 |    50/  658 batches | lr 0.00006 | ms/batch 14.95 | loss 0.06409626\n",
      "| Epoch 106 |   100/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.06096801\n",
      "| Epoch 106 |   150/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.05676539\n",
      "| Epoch 106 |   200/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05631607\n",
      "| Epoch 106 |   250/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.05980815\n",
      "| Epoch 106 |   300/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.06041020\n",
      "| Epoch 106 |   350/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.06065513\n",
      "| Epoch 106 |   400/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.05832316\n",
      "| Epoch 106 |   450/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.05427999\n",
      "| Epoch 106 |   500/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.05919422\n",
      "| Epoch 106 |   550/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.05744167\n",
      "| Epoch 106 |   600/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.05995849\n",
      "| Epoch 106 |   650/  658 batches | lr 0.00006 | ms/batch 15.02 | loss 0.05790789\n",
      "\n",
      "Val set: Average loss: 0.05962491\n",
      "\n",
      "| Epoch 107 |    50/  658 batches | lr 0.00006 | ms/batch 16.78 | loss 0.06396407\n",
      "| Epoch 107 |   100/  658 batches | lr 0.00006 | ms/batch 18.15 | loss 0.06111823\n",
      "| Epoch 107 |   150/  658 batches | lr 0.00006 | ms/batch 15.76 | loss 0.05663470\n",
      "| Epoch 107 |   200/  658 batches | lr 0.00006 | ms/batch 17.93 | loss 0.05611269\n",
      "| Epoch 107 |   250/  658 batches | lr 0.00006 | ms/batch 19.56 | loss 0.05983895\n",
      "| Epoch 107 |   300/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.06031045\n",
      "| Epoch 107 |   350/  658 batches | lr 0.00006 | ms/batch 15.80 | loss 0.06064973\n",
      "| Epoch 107 |   400/  658 batches | lr 0.00006 | ms/batch 16.80 | loss 0.05819052\n",
      "| Epoch 107 |   450/  658 batches | lr 0.00006 | ms/batch 15.61 | loss 0.05421729\n",
      "| Epoch 107 |   500/  658 batches | lr 0.00006 | ms/batch 18.63 | loss 0.05883781\n",
      "| Epoch 107 |   550/  658 batches | lr 0.00006 | ms/batch 21.06 | loss 0.05733955\n",
      "| Epoch 107 |   600/  658 batches | lr 0.00006 | ms/batch 19.48 | loss 0.06001498\n",
      "| Epoch 107 |   650/  658 batches | lr 0.00006 | ms/batch 15.96 | loss 0.05771557\n",
      "\n",
      "Val set: Average loss: 0.05974681\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 108 |    50/  658 batches | lr 0.00006 | ms/batch 15.70 | loss 0.06373499\n",
      "| Epoch 108 |   100/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.06086274\n",
      "| Epoch 108 |   150/  658 batches | lr 0.00006 | ms/batch 14.56 | loss 0.05657100\n",
      "| Epoch 108 |   200/  658 batches | lr 0.00006 | ms/batch 16.07 | loss 0.05612116\n",
      "| Epoch 108 |   250/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05971833\n",
      "| Epoch 108 |   300/  658 batches | lr 0.00006 | ms/batch 14.56 | loss 0.06012559\n",
      "| Epoch 108 |   350/  658 batches | lr 0.00006 | ms/batch 14.52 | loss 0.06059220\n",
      "| Epoch 108 |   400/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.05838459\n",
      "| Epoch 108 |   450/  658 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05410978\n",
      "| Epoch 108 |   500/  658 batches | lr 0.00006 | ms/batch 16.76 | loss 0.05905961\n",
      "| Epoch 108 |   550/  658 batches | lr 0.00006 | ms/batch 16.05 | loss 0.05732162\n",
      "| Epoch 108 |   600/  658 batches | lr 0.00006 | ms/batch 14.92 | loss 0.05997566\n",
      "| Epoch 108 |   650/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05781748\n",
      "\n",
      "Val set: Average loss: 0.05955811\n",
      "\n",
      "| Epoch 109 |    50/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06378376\n",
      "| Epoch 109 |   100/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.06088572\n",
      "| Epoch 109 |   150/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.05651557\n",
      "| Epoch 109 |   200/  658 batches | lr 0.00006 | ms/batch 16.71 | loss 0.05606720\n",
      "| Epoch 109 |   250/  658 batches | lr 0.00006 | ms/batch 16.30 | loss 0.05960713\n",
      "| Epoch 109 |   300/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.06022646\n",
      "| Epoch 109 |   350/  658 batches | lr 0.00006 | ms/batch 15.15 | loss 0.06059476\n",
      "| Epoch 109 |   400/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.05827295\n",
      "| Epoch 109 |   450/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05403675\n",
      "| Epoch 109 |   500/  658 batches | lr 0.00006 | ms/batch 17.26 | loss 0.05903231\n",
      "| Epoch 109 |   550/  658 batches | lr 0.00006 | ms/batch 17.20 | loss 0.05721861\n",
      "| Epoch 109 |   600/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.05988762\n",
      "| Epoch 109 |   650/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05774781\n",
      "\n",
      "Val set: Average loss: 0.05955598\n",
      "\n",
      "| Epoch 110 |    50/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.06372396\n",
      "| Epoch 110 |   100/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.06073927\n",
      "| Epoch 110 |   150/  658 batches | lr 0.00006 | ms/batch 14.61 | loss 0.05641885\n",
      "| Epoch 110 |   200/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.05592740\n",
      "| Epoch 110 |   250/  658 batches | lr 0.00006 | ms/batch 16.79 | loss 0.05957353\n",
      "| Epoch 110 |   300/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.06011138\n",
      "| Epoch 110 |   350/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.06049553\n",
      "| Epoch 110 |   400/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05818296\n",
      "| Epoch 110 |   450/  658 batches | lr 0.00006 | ms/batch 14.57 | loss 0.05398384\n",
      "| Epoch 110 |   500/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05907671\n",
      "| Epoch 110 |   550/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.05718299\n",
      "| Epoch 110 |   600/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05986819\n",
      "| Epoch 110 |   650/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05772653\n",
      "\n",
      "Val set: Average loss: 0.05943998\n",
      "\n",
      "| Epoch 111 |    50/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06408512\n",
      "| Epoch 111 |   100/  658 batches | lr 0.00006 | ms/batch 16.03 | loss 0.06092335\n",
      "| Epoch 111 |   150/  658 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05646818\n",
      "| Epoch 111 |   200/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.05599032\n",
      "| Epoch 111 |   250/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.05948601\n",
      "| Epoch 111 |   300/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05995943\n",
      "| Epoch 111 |   350/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.06049349\n",
      "| Epoch 111 |   400/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.05790579\n",
      "| Epoch 111 |   450/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05399406\n",
      "| Epoch 111 |   500/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.05894278\n",
      "| Epoch 111 |   550/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05705138\n",
      "| Epoch 111 |   600/  658 batches | lr 0.00006 | ms/batch 14.50 | loss 0.05983207\n",
      "| Epoch 111 |   650/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05774424\n",
      "\n",
      "Val set: Average loss: 0.05951578\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 112 |    50/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06397892\n",
      "| Epoch 112 |   100/  658 batches | lr 0.00006 | ms/batch 16.03 | loss 0.06093133\n",
      "| Epoch 112 |   150/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05631829\n",
      "| Epoch 112 |   200/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05579653\n",
      "| Epoch 112 |   250/  658 batches | lr 0.00006 | ms/batch 16.17 | loss 0.05949872\n",
      "| Epoch 112 |   300/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05994051\n",
      "| Epoch 112 |   350/  658 batches | lr 0.00006 | ms/batch 14.50 | loss 0.06053544\n",
      "| Epoch 112 |   400/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.05782277\n",
      "| Epoch 112 |   450/  658 batches | lr 0.00006 | ms/batch 14.56 | loss 0.05393660\n",
      "| Epoch 112 |   500/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05890594\n",
      "| Epoch 112 |   550/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05708752\n",
      "| Epoch 112 |   600/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05974027\n",
      "| Epoch 112 |   650/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05774971\n",
      "\n",
      "Val set: Average loss: 0.05936826\n",
      "\n",
      "| Epoch 113 |    50/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.06421764\n",
      "| Epoch 113 |   100/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.06085713\n",
      "| Epoch 113 |   150/  658 batches | lr 0.00006 | ms/batch 14.57 | loss 0.05609018\n",
      "| Epoch 113 |   200/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05571992\n",
      "| Epoch 113 |   250/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05947301\n",
      "| Epoch 113 |   300/  658 batches | lr 0.00006 | ms/batch 14.41 | loss 0.05996812\n",
      "| Epoch 113 |   350/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.06047526\n",
      "| Epoch 113 |   400/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05777260\n",
      "| Epoch 113 |   450/  658 batches | lr 0.00006 | ms/batch 14.61 | loss 0.05391673\n",
      "| Epoch 113 |   500/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.05887577\n",
      "| Epoch 113 |   550/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05717566\n",
      "| Epoch 113 |   600/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05974597\n",
      "| Epoch 113 |   650/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05759466\n",
      "\n",
      "Val set: Average loss: 0.05930206\n",
      "\n",
      "| Epoch 114 |    50/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.06384723\n",
      "| Epoch 114 |   100/  658 batches | lr 0.00006 | ms/batch 16.17 | loss 0.06068493\n",
      "| Epoch 114 |   150/  658 batches | lr 0.00006 | ms/batch 14.50 | loss 0.05617285\n",
      "| Epoch 114 |   200/  658 batches | lr 0.00006 | ms/batch 16.07 | loss 0.05563370\n",
      "| Epoch 114 |   250/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05935806\n",
      "| Epoch 114 |   300/  658 batches | lr 0.00006 | ms/batch 14.52 | loss 0.05994109\n",
      "| Epoch 114 |   350/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.06063354\n",
      "| Epoch 114 |   400/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.05786883\n",
      "| Epoch 114 |   450/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05388030\n",
      "| Epoch 114 |   500/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05881454\n",
      "| Epoch 114 |   550/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.05685688\n",
      "| Epoch 114 |   600/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05973160\n",
      "| Epoch 114 |   650/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05758527\n",
      "\n",
      "Val set: Average loss: 0.05930450\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 115 |    50/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06378519\n",
      "| Epoch 115 |   100/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.06056410\n",
      "| Epoch 115 |   150/  658 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05616673\n",
      "| Epoch 115 |   200/  658 batches | lr 0.00006 | ms/batch 16.17 | loss 0.05559787\n",
      "| Epoch 115 |   250/  658 batches | lr 0.00006 | ms/batch 16.02 | loss 0.05943917\n",
      "| Epoch 115 |   300/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05969124\n",
      "| Epoch 115 |   350/  658 batches | lr 0.00006 | ms/batch 14.50 | loss 0.06065402\n",
      "| Epoch 115 |   400/  658 batches | lr 0.00006 | ms/batch 16.05 | loss 0.05795278\n",
      "| Epoch 115 |   450/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.05377420\n",
      "| Epoch 115 |   500/  658 batches | lr 0.00006 | ms/batch 16.62 | loss 0.05864780\n",
      "| Epoch 115 |   550/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.05689052\n",
      "| Epoch 115 |   600/  658 batches | lr 0.00006 | ms/batch 15.41 | loss 0.05950399\n",
      "| Epoch 115 |   650/  658 batches | lr 0.00006 | ms/batch 15.19 | loss 0.05765822\n",
      "\n",
      "Val set: Average loss: 0.05925712\n",
      "\n",
      "| Epoch 116 |    50/  658 batches | lr 0.00006 | ms/batch 17.43 | loss 0.06362702\n",
      "| Epoch 116 |   100/  658 batches | lr 0.00006 | ms/batch 17.31 | loss 0.06060508\n",
      "| Epoch 116 |   150/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05603020\n",
      "| Epoch 116 |   200/  658 batches | lr 0.00006 | ms/batch 16.93 | loss 0.05561212\n",
      "| Epoch 116 |   250/  658 batches | lr 0.00006 | ms/batch 16.80 | loss 0.05920873\n",
      "| Epoch 116 |   300/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.05963130\n",
      "| Epoch 116 |   350/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.06057539\n",
      "| Epoch 116 |   400/  658 batches | lr 0.00006 | ms/batch 17.87 | loss 0.05795361\n",
      "| Epoch 116 |   450/  658 batches | lr 0.00006 | ms/batch 15.88 | loss 0.05371183\n",
      "| Epoch 116 |   500/  658 batches | lr 0.00006 | ms/batch 18.96 | loss 0.05862615\n",
      "| Epoch 116 |   550/  658 batches | lr 0.00006 | ms/batch 17.35 | loss 0.05689139\n",
      "| Epoch 116 |   600/  658 batches | lr 0.00006 | ms/batch 15.72 | loss 0.05949006\n",
      "| Epoch 116 |   650/  658 batches | lr 0.00006 | ms/batch 17.77 | loss 0.05750922\n",
      "\n",
      "Val set: Average loss: 0.05922793\n",
      "\n",
      "| Epoch 117 |    50/  658 batches | lr 0.00006 | ms/batch 17.98 | loss 0.06342780\n",
      "| Epoch 117 |   100/  658 batches | lr 0.00006 | ms/batch 21.97 | loss 0.06054352\n",
      "| Epoch 117 |   150/  658 batches | lr 0.00006 | ms/batch 19.38 | loss 0.05597680\n",
      "| Epoch 117 |   200/  658 batches | lr 0.00006 | ms/batch 19.18 | loss 0.05553944\n",
      "| Epoch 117 |   250/  658 batches | lr 0.00006 | ms/batch 20.75 | loss 0.05914988\n",
      "| Epoch 117 |   300/  658 batches | lr 0.00006 | ms/batch 18.09 | loss 0.05969222\n",
      "| Epoch 117 |   350/  658 batches | lr 0.00006 | ms/batch 19.09 | loss 0.06042995\n",
      "| Epoch 117 |   400/  658 batches | lr 0.00006 | ms/batch 21.89 | loss 0.05785145\n",
      "| Epoch 117 |   450/  658 batches | lr 0.00006 | ms/batch 18.31 | loss 0.05404668\n",
      "| Epoch 117 |   500/  658 batches | lr 0.00006 | ms/batch 19.78 | loss 0.05829699\n",
      "| Epoch 117 |   550/  658 batches | lr 0.00006 | ms/batch 19.72 | loss 0.05685382\n",
      "| Epoch 117 |   600/  658 batches | lr 0.00006 | ms/batch 17.00 | loss 0.05954677\n",
      "| Epoch 117 |   650/  658 batches | lr 0.00006 | ms/batch 19.45 | loss 0.05746925\n",
      "\n",
      "Val set: Average loss: 0.05915092\n",
      "\n",
      "| Epoch 118 |    50/  658 batches | lr 0.00006 | ms/batch 15.42 | loss 0.06334994\n",
      "| Epoch 118 |   100/  658 batches | lr 0.00006 | ms/batch 18.17 | loss 0.06037076\n",
      "| Epoch 118 |   150/  658 batches | lr 0.00006 | ms/batch 16.54 | loss 0.05592755\n",
      "| Epoch 118 |   200/  658 batches | lr 0.00006 | ms/batch 20.05 | loss 0.05543309\n",
      "| Epoch 118 |   250/  658 batches | lr 0.00006 | ms/batch 22.10 | loss 0.05908865\n",
      "| Epoch 118 |   300/  658 batches | lr 0.00006 | ms/batch 18.24 | loss 0.05958614\n",
      "| Epoch 118 |   350/  658 batches | lr 0.00006 | ms/batch 17.66 | loss 0.06031038\n",
      "| Epoch 118 |   400/  658 batches | lr 0.00006 | ms/batch 19.54 | loss 0.05772041\n",
      "| Epoch 118 |   450/  658 batches | lr 0.00006 | ms/batch 17.96 | loss 0.05392401\n",
      "| Epoch 118 |   500/  658 batches | lr 0.00006 | ms/batch 18.57 | loss 0.05819423\n",
      "| Epoch 118 |   550/  658 batches | lr 0.00006 | ms/batch 20.41 | loss 0.05677012\n",
      "| Epoch 118 |   600/  658 batches | lr 0.00006 | ms/batch 17.13 | loss 0.05957262\n",
      "| Epoch 118 |   650/  658 batches | lr 0.00006 | ms/batch 18.98 | loss 0.05733503\n",
      "\n",
      "Val set: Average loss: 0.05902866\n",
      "\n",
      "| Epoch 119 |    50/  658 batches | lr 0.00006 | ms/batch 23.38 | loss 0.06351721\n",
      "| Epoch 119 |   100/  658 batches | lr 0.00006 | ms/batch 22.52 | loss 0.06025428\n",
      "| Epoch 119 |   150/  658 batches | lr 0.00006 | ms/batch 18.92 | loss 0.05590598\n",
      "| Epoch 119 |   200/  658 batches | lr 0.00006 | ms/batch 19.94 | loss 0.05541012\n",
      "| Epoch 119 |   250/  658 batches | lr 0.00006 | ms/batch 20.10 | loss 0.05903514\n",
      "| Epoch 119 |   300/  658 batches | lr 0.00006 | ms/batch 20.72 | loss 0.05970122\n",
      "| Epoch 119 |   350/  658 batches | lr 0.00006 | ms/batch 18.19 | loss 0.06036543\n",
      "| Epoch 119 |   400/  658 batches | lr 0.00006 | ms/batch 20.24 | loss 0.05768176\n",
      "| Epoch 119 |   450/  658 batches | lr 0.00006 | ms/batch 19.68 | loss 0.05375295\n",
      "| Epoch 119 |   500/  658 batches | lr 0.00006 | ms/batch 20.30 | loss 0.05790457\n",
      "| Epoch 119 |   550/  658 batches | lr 0.00006 | ms/batch 19.40 | loss 0.05661376\n",
      "| Epoch 119 |   600/  658 batches | lr 0.00006 | ms/batch 18.39 | loss 0.05939213\n",
      "| Epoch 119 |   650/  658 batches | lr 0.00006 | ms/batch 18.36 | loss 0.05749286\n",
      "\n",
      "Val set: Average loss: 0.05909457\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 120 |    50/  658 batches | lr 0.00006 | ms/batch 19.08 | loss 0.06317251\n",
      "| Epoch 120 |   100/  658 batches | lr 0.00006 | ms/batch 20.33 | loss 0.06018920\n",
      "| Epoch 120 |   150/  658 batches | lr 0.00006 | ms/batch 18.78 | loss 0.05582911\n",
      "| Epoch 120 |   200/  658 batches | lr 0.00006 | ms/batch 19.74 | loss 0.05539172\n",
      "| Epoch 120 |   250/  658 batches | lr 0.00006 | ms/batch 21.68 | loss 0.05898215\n",
      "| Epoch 120 |   300/  658 batches | lr 0.00006 | ms/batch 17.38 | loss 0.05965630\n",
      "| Epoch 120 |   350/  658 batches | lr 0.00006 | ms/batch 14.94 | loss 0.06014731\n",
      "| Epoch 120 |   400/  658 batches | lr 0.00006 | ms/batch 17.80 | loss 0.05766248\n",
      "| Epoch 120 |   450/  658 batches | lr 0.00006 | ms/batch 16.67 | loss 0.05377071\n",
      "| Epoch 120 |   500/  658 batches | lr 0.00006 | ms/batch 16.81 | loss 0.05787736\n",
      "| Epoch 120 |   550/  658 batches | lr 0.00006 | ms/batch 16.77 | loss 0.05668971\n",
      "| Epoch 120 |   600/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.05935987\n",
      "| Epoch 120 |   650/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.05732061\n",
      "\n",
      "Val set: Average loss: 0.05901124\n",
      "\n",
      "| Epoch 121 |    50/  658 batches | lr 0.00006 | ms/batch 17.12 | loss 0.06336680\n",
      "| Epoch 121 |   100/  658 batches | lr 0.00006 | ms/batch 18.61 | loss 0.06026950\n",
      "| Epoch 121 |   150/  658 batches | lr 0.00006 | ms/batch 14.91 | loss 0.05576786\n",
      "| Epoch 121 |   200/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.05527917\n",
      "| Epoch 121 |   250/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.05892897\n",
      "| Epoch 121 |   300/  658 batches | lr 0.00006 | ms/batch 14.89 | loss 0.05962903\n",
      "| Epoch 121 |   350/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.06001893\n",
      "| Epoch 121 |   400/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.05750727\n",
      "| Epoch 121 |   450/  658 batches | lr 0.00006 | ms/batch 15.36 | loss 0.05358515\n",
      "| Epoch 121 |   500/  658 batches | lr 0.00006 | ms/batch 18.00 | loss 0.05854481\n",
      "| Epoch 121 |   550/  658 batches | lr 0.00006 | ms/batch 16.62 | loss 0.05657745\n",
      "| Epoch 121 |   600/  658 batches | lr 0.00006 | ms/batch 15.08 | loss 0.05908948\n",
      "| Epoch 121 |   650/  658 batches | lr 0.00006 | ms/batch 14.95 | loss 0.05711572\n",
      "\n",
      "Val set: Average loss: 0.05899900\n",
      "\n",
      "| Epoch 122 |    50/  658 batches | lr 0.00006 | ms/batch 17.28 | loss 0.06296339\n",
      "| Epoch 122 |   100/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.06017057\n",
      "| Epoch 122 |   150/  658 batches | lr 0.00006 | ms/batch 15.37 | loss 0.05579751\n",
      "| Epoch 122 |   200/  658 batches | lr 0.00006 | ms/batch 16.55 | loss 0.05523680\n",
      "| Epoch 122 |   250/  658 batches | lr 0.00006 | ms/batch 16.72 | loss 0.05882625\n",
      "| Epoch 122 |   300/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05955651\n",
      "| Epoch 122 |   350/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05984759\n",
      "| Epoch 122 |   400/  658 batches | lr 0.00006 | ms/batch 20.16 | loss 0.05751668\n",
      "| Epoch 122 |   450/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05347155\n",
      "| Epoch 122 |   500/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05838353\n",
      "| Epoch 122 |   550/  658 batches | lr 0.00006 | ms/batch 17.02 | loss 0.05648167\n",
      "| Epoch 122 |   600/  658 batches | lr 0.00006 | ms/batch 18.83 | loss 0.05913904\n",
      "| Epoch 122 |   650/  658 batches | lr 0.00006 | ms/batch 16.98 | loss 0.05705714\n",
      "\n",
      "Val set: Average loss: 0.05888084\n",
      "\n",
      "| Epoch 123 |    50/  658 batches | lr 0.00006 | ms/batch 15.24 | loss 0.06308194\n",
      "| Epoch 123 |   100/  658 batches | lr 0.00006 | ms/batch 16.52 | loss 0.06024815\n",
      "| Epoch 123 |   150/  658 batches | lr 0.00006 | ms/batch 17.68 | loss 0.05571726\n",
      "| Epoch 123 |   200/  658 batches | lr 0.00006 | ms/batch 19.17 | loss 0.05518657\n",
      "| Epoch 123 |   250/  658 batches | lr 0.00006 | ms/batch 19.28 | loss 0.05883752\n",
      "| Epoch 123 |   300/  658 batches | lr 0.00006 | ms/batch 15.86 | loss 0.05949871\n",
      "| Epoch 123 |   350/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.05994743\n",
      "| Epoch 123 |   400/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.05764882\n",
      "| Epoch 123 |   450/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.05374821\n",
      "| Epoch 123 |   500/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05782101\n",
      "| Epoch 123 |   550/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05641936\n",
      "| Epoch 123 |   600/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.05906581\n",
      "| Epoch 123 |   650/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.05713416\n",
      "\n",
      "Val set: Average loss: 0.05890724\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 124 |    50/  658 batches | lr 0.00006 | ms/batch 14.96 | loss 0.06298395\n",
      "| Epoch 124 |   100/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05991048\n",
      "| Epoch 124 |   150/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.05567555\n",
      "| Epoch 124 |   200/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.05514862\n",
      "| Epoch 124 |   250/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.05877346\n",
      "| Epoch 124 |   300/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.05958794\n",
      "| Epoch 124 |   350/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.05992841\n",
      "| Epoch 124 |   400/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.05774752\n",
      "| Epoch 124 |   450/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.05366856\n",
      "| Epoch 124 |   500/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05754369\n",
      "| Epoch 124 |   550/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.05632589\n",
      "| Epoch 124 |   600/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.05910710\n",
      "| Epoch 124 |   650/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.05709165\n",
      "\n",
      "Val set: Average loss: 0.05899395\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch 125 |    50/  658 batches | lr 0.00006 | ms/batch 14.97 | loss 0.06291180\n",
      "| Epoch 125 |   100/  658 batches | lr 0.00006 | ms/batch 16.30 | loss 0.05994526\n",
      "| Epoch 125 |   150/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.05568588\n",
      "| Epoch 125 |   200/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.05512297\n",
      "| Epoch 125 |   250/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.05865784\n",
      "| Epoch 125 |   300/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.05932487\n",
      "| Epoch 125 |   350/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.05976637\n",
      "| Epoch 125 |   400/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05765917\n",
      "| Epoch 125 |   450/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.05358989\n",
      "| Epoch 125 |   500/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.05771651\n",
      "| Epoch 125 |   550/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.05638808\n",
      "| Epoch 125 |   600/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.05892627\n",
      "| Epoch 125 |   650/  658 batches | lr 0.00006 | ms/batch 14.91 | loss 0.05697042\n",
      "\n",
      "Val set: Average loss: 0.05894472\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch 126 |    50/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.06271998\n",
      "| Epoch 126 |   100/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05987098\n",
      "| Epoch 126 |   150/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.05549513\n",
      "| Epoch 126 |   200/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.05502777\n",
      "| Epoch 126 |   250/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.05856755\n",
      "| Epoch 126 |   300/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.05925656\n",
      "| Epoch 126 |   350/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.05969789\n",
      "| Epoch 126 |   400/  658 batches | lr 0.00006 | ms/batch 16.40 | loss 0.05742189\n",
      "| Epoch 126 |   450/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.05365378\n",
      "| Epoch 126 |   500/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.05783938\n",
      "| Epoch 126 |   550/  658 batches | lr 0.00006 | ms/batch 16.51 | loss 0.05652735\n",
      "| Epoch 126 |   600/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.05887709\n",
      "| Epoch 126 |   650/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.05680709\n",
      "\n",
      "Val set: Average loss: 0.05897896\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch 127 |    50/  658 batches | lr 0.00006 | ms/batch 15.02 | loss 0.06274793\n",
      "| Epoch 127 |   100/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.05974940\n",
      "| Epoch 127 |   150/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05553472\n",
      "| Epoch 127 |   200/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.05503279\n",
      "| Epoch 127 |   250/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.05862368\n",
      "| Epoch 127 |   300/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.05924777\n",
      "| Epoch 127 |   350/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.05969045\n",
      "| Epoch 127 |   400/  658 batches | lr 0.00006 | ms/batch 16.40 | loss 0.05742751\n",
      "| Epoch 127 |   450/  658 batches | lr 0.00006 | ms/batch 14.88 | loss 0.05340580\n",
      "| Epoch 127 |   500/  658 batches | lr 0.00006 | ms/batch 16.68 | loss 0.05728006\n",
      "| Epoch 127 |   550/  658 batches | lr 0.00006 | ms/batch 16.54 | loss 0.05619856\n",
      "| Epoch 127 |   600/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.05883829\n",
      "| Epoch 127 |   650/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.05684611\n",
      "\n",
      "Val set: Average loss: 0.05888265\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch 128 |    50/  658 batches | lr 0.00006 | ms/batch 15.00 | loss 0.06281804\n",
      "| Epoch 128 |   100/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.05962903\n",
      "| Epoch 128 |   150/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05550585\n",
      "| Epoch 128 |   200/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.05500658\n",
      "| Epoch 128 |   250/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05858543\n",
      "| Epoch 128 |   300/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.05925181\n",
      "| Epoch 128 |   350/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.05961411\n",
      "| Epoch 128 |   400/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.05761734\n",
      "| Epoch 128 |   450/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.05356950\n",
      "| Epoch 128 |   500/  658 batches | lr 0.00006 | ms/batch 16.40 | loss 0.05725291\n",
      "| Epoch 128 |   550/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.05618364\n",
      "| Epoch 128 |   600/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.05885722\n",
      "| Epoch 128 |   650/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05681636\n",
      "\n",
      "Val set: Average loss: 0.05872775\n",
      "\n",
      "| Epoch 129 |    50/  658 batches | lr 0.00006 | ms/batch 15.01 | loss 0.06265986\n",
      "| Epoch 129 |   100/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.05959078\n",
      "| Epoch 129 |   150/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.05537005\n",
      "| Epoch 129 |   200/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.05487102\n",
      "| Epoch 129 |   250/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.05857274\n",
      "| Epoch 129 |   300/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.05918763\n",
      "| Epoch 129 |   350/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.05956682\n",
      "| Epoch 129 |   400/  658 batches | lr 0.00006 | ms/batch 17.10 | loss 0.05737828\n",
      "| Epoch 129 |   450/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.05345303\n",
      "| Epoch 129 |   500/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.05776598\n",
      "| Epoch 129 |   550/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.05628769\n",
      "| Epoch 129 |   600/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.05886599\n",
      "| Epoch 129 |   650/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.05659008\n",
      "\n",
      "Val set: Average loss: 0.05882477\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 130 |    50/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.06239884\n",
      "| Epoch 130 |   100/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.05961586\n",
      "| Epoch 130 |   150/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.05546132\n",
      "| Epoch 130 |   200/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05485201\n",
      "| Epoch 130 |   250/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.05853770\n",
      "| Epoch 130 |   300/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.05924824\n",
      "| Epoch 130 |   350/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.05960247\n",
      "| Epoch 130 |   400/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.05727582\n",
      "| Epoch 130 |   450/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.05342033\n",
      "| Epoch 130 |   500/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.05736977\n",
      "| Epoch 130 |   550/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.05603900\n",
      "| Epoch 130 |   600/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.05870450\n",
      "| Epoch 130 |   650/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.05662953\n",
      "\n",
      "Val set: Average loss: 0.05882822\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch 131 |    50/  658 batches | lr 0.00006 | ms/batch 15.13 | loss 0.06240172\n",
      "| Epoch 131 |   100/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.05938069\n",
      "| Epoch 131 |   150/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.05527995\n",
      "| Epoch 131 |   200/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.05490290\n",
      "| Epoch 131 |   250/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.05838471\n",
      "| Epoch 131 |   300/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.05899022\n",
      "| Epoch 131 |   350/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.05957559\n",
      "| Epoch 131 |   400/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.05738584\n",
      "| Epoch 131 |   450/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.05334481\n",
      "| Epoch 131 |   500/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05722339\n",
      "| Epoch 131 |   550/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05607601\n",
      "| Epoch 131 |   600/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.05864771\n",
      "| Epoch 131 |   650/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.05660641\n",
      "\n",
      "Val set: Average loss: 0.05868407\n",
      "\n",
      "| Epoch 132 |    50/  658 batches | lr 0.00006 | ms/batch 14.92 | loss 0.06223861\n",
      "| Epoch 132 |   100/  658 batches | lr 0.00006 | ms/batch 16.60 | loss 0.05930504\n",
      "| Epoch 132 |   150/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05517510\n",
      "| Epoch 132 |   200/  658 batches | lr 0.00006 | ms/batch 17.38 | loss 0.05481005\n",
      "| Epoch 132 |   250/  658 batches | lr 0.00006 | ms/batch 17.91 | loss 0.05825817\n",
      "| Epoch 132 |   300/  658 batches | lr 0.00006 | ms/batch 15.48 | loss 0.05902889\n",
      "| Epoch 132 |   350/  658 batches | lr 0.00006 | ms/batch 17.30 | loss 0.05956343\n",
      "| Epoch 132 |   400/  658 batches | lr 0.00006 | ms/batch 17.62 | loss 0.05727365\n",
      "| Epoch 132 |   450/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.05328676\n",
      "| Epoch 132 |   500/  658 batches | lr 0.00006 | ms/batch 17.70 | loss 0.05723625\n",
      "| Epoch 132 |   550/  658 batches | lr 0.00006 | ms/batch 17.31 | loss 0.05606821\n",
      "| Epoch 132 |   600/  658 batches | lr 0.00006 | ms/batch 16.20 | loss 0.05854204\n",
      "| Epoch 132 |   650/  658 batches | lr 0.00006 | ms/batch 16.77 | loss 0.05648400\n",
      "\n",
      "Val set: Average loss: 0.05863142\n",
      "\n",
      "| Epoch 133 |    50/  658 batches | lr 0.00006 | ms/batch 15.82 | loss 0.06216013\n",
      "| Epoch 133 |   100/  658 batches | lr 0.00006 | ms/batch 17.01 | loss 0.05928827\n",
      "| Epoch 133 |   150/  658 batches | lr 0.00006 | ms/batch 15.00 | loss 0.05512561\n",
      "| Epoch 133 |   200/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.05475078\n",
      "| Epoch 133 |   250/  658 batches | lr 0.00006 | ms/batch 16.84 | loss 0.05816217\n",
      "| Epoch 133 |   300/  658 batches | lr 0.00006 | ms/batch 15.05 | loss 0.05898105\n",
      "| Epoch 133 |   350/  658 batches | lr 0.00006 | ms/batch 15.10 | loss 0.05950927\n",
      "| Epoch 133 |   400/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.05713379\n",
      "| Epoch 133 |   450/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.05318304\n",
      "| Epoch 133 |   500/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.05712268\n",
      "| Epoch 133 |   550/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.05592730\n",
      "| Epoch 133 |   600/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.05860047\n",
      "| Epoch 133 |   650/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.05646922\n",
      "\n",
      "Val set: Average loss: 0.05864665\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 134 |    50/  658 batches | lr 0.00006 | ms/batch 15.14 | loss 0.06189124\n",
      "| Epoch 134 |   100/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.05914061\n",
      "| Epoch 134 |   150/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.05508655\n",
      "| Epoch 134 |   200/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.05473278\n",
      "| Epoch 134 |   250/  658 batches | lr 0.00006 | ms/batch 16.78 | loss 0.05818746\n",
      "| Epoch 134 |   300/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.05915666\n",
      "| Epoch 134 |   350/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.05949915\n",
      "| Epoch 134 |   400/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.05716358\n",
      "| Epoch 134 |   450/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.05329625\n",
      "| Epoch 134 |   500/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.05699052\n",
      "| Epoch 134 |   550/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.05586534\n",
      "| Epoch 134 |   600/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.05834202\n",
      "| Epoch 134 |   650/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.05637708\n",
      "\n",
      "Val set: Average loss: 0.05858153\n",
      "\n",
      "| Epoch 135 |    50/  658 batches | lr 0.00006 | ms/batch 14.94 | loss 0.06204891\n",
      "| Epoch 135 |   100/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.05918701\n",
      "| Epoch 135 |   150/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.05499418\n",
      "| Epoch 135 |   200/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.05467344\n",
      "| Epoch 135 |   250/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.05824773\n",
      "| Epoch 135 |   300/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.05920870\n",
      "| Epoch 135 |   350/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.05941741\n",
      "| Epoch 135 |   400/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.05702140\n",
      "| Epoch 135 |   450/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.05324163\n",
      "| Epoch 135 |   500/  658 batches | lr 0.00006 | ms/batch 16.65 | loss 0.05693418\n",
      "| Epoch 135 |   550/  658 batches | lr 0.00006 | ms/batch 16.57 | loss 0.05584675\n",
      "| Epoch 135 |   600/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.05841879\n",
      "| Epoch 135 |   650/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05628050\n",
      "\n",
      "Val set: Average loss: 0.05860567\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 136 |    50/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.06192528\n",
      "| Epoch 136 |   100/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05905303\n",
      "| Epoch 136 |   150/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.05493626\n",
      "| Epoch 136 |   200/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.05456816\n",
      "| Epoch 136 |   250/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05819692\n",
      "| Epoch 136 |   300/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.05892725\n",
      "| Epoch 136 |   350/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05937326\n",
      "| Epoch 136 |   400/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.05698725\n",
      "| Epoch 136 |   450/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.05319467\n",
      "| Epoch 136 |   500/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.05693736\n",
      "| Epoch 136 |   550/  658 batches | lr 0.00006 | ms/batch 16.79 | loss 0.05582746\n",
      "| Epoch 136 |   600/  658 batches | lr 0.00006 | ms/batch 15.43 | loss 0.05840031\n",
      "| Epoch 136 |   650/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05617126\n",
      "\n",
      "Val set: Average loss: 0.05854789\n",
      "\n",
      "| Epoch 137 |    50/  658 batches | lr 0.00006 | ms/batch 15.75 | loss 0.06183120\n",
      "| Epoch 137 |   100/  658 batches | lr 0.00006 | ms/batch 19.60 | loss 0.05894136\n",
      "| Epoch 137 |   150/  658 batches | lr 0.00006 | ms/batch 15.16 | loss 0.05497083\n",
      "| Epoch 137 |   200/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05452470\n",
      "| Epoch 137 |   250/  658 batches | lr 0.00006 | ms/batch 16.58 | loss 0.05825046\n",
      "| Epoch 137 |   300/  658 batches | lr 0.00006 | ms/batch 14.61 | loss 0.05904522\n",
      "| Epoch 137 |   350/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.05922376\n",
      "| Epoch 137 |   400/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05681295\n",
      "| Epoch 137 |   450/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.05312385\n",
      "| Epoch 137 |   500/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.05693033\n",
      "| Epoch 137 |   550/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.05581127\n",
      "| Epoch 137 |   600/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.05823234\n",
      "| Epoch 137 |   650/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.05615954\n",
      "\n",
      "Val set: Average loss: 0.05852812\n",
      "\n",
      "| Epoch 138 |    50/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.06181590\n",
      "| Epoch 138 |   100/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.05898553\n",
      "| Epoch 138 |   150/  658 batches | lr 0.00006 | ms/batch 14.82 | loss 0.05489832\n",
      "| Epoch 138 |   200/  658 batches | lr 0.00006 | ms/batch 17.82 | loss 0.05444436\n",
      "| Epoch 138 |   250/  658 batches | lr 0.00006 | ms/batch 17.62 | loss 0.05807325\n",
      "| Epoch 138 |   300/  658 batches | lr 0.00006 | ms/batch 16.79 | loss 0.05899335\n",
      "| Epoch 138 |   350/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05928495\n",
      "| Epoch 138 |   400/  658 batches | lr 0.00006 | ms/batch 19.56 | loss 0.05669687\n",
      "| Epoch 138 |   450/  658 batches | lr 0.00006 | ms/batch 15.08 | loss 0.05287734\n",
      "| Epoch 138 |   500/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.05671649\n",
      "| Epoch 138 |   550/  658 batches | lr 0.00006 | ms/batch 16.51 | loss 0.05575354\n",
      "| Epoch 138 |   600/  658 batches | lr 0.00006 | ms/batch 17.27 | loss 0.05855230\n",
      "| Epoch 138 |   650/  658 batches | lr 0.00006 | ms/batch 18.97 | loss 0.05609838\n",
      "\n",
      "Val set: Average loss: 0.05847621\n",
      "\n",
      "| Epoch 139 |    50/  658 batches | lr 0.00006 | ms/batch 14.88 | loss 0.06187879\n",
      "| Epoch 139 |   100/  658 batches | lr 0.00006 | ms/batch 16.53 | loss 0.05897254\n",
      "| Epoch 139 |   150/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.05479921\n",
      "| Epoch 139 |   200/  658 batches | lr 0.00006 | ms/batch 16.82 | loss 0.05432055\n",
      "| Epoch 139 |   250/  658 batches | lr 0.00006 | ms/batch 16.60 | loss 0.05832561\n",
      "| Epoch 139 |   300/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.05891497\n",
      "| Epoch 139 |   350/  658 batches | lr 0.00006 | ms/batch 15.65 | loss 0.05914650\n",
      "| Epoch 139 |   400/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.05680276\n",
      "| Epoch 139 |   450/  658 batches | lr 0.00006 | ms/batch 14.99 | loss 0.05279593\n",
      "| Epoch 139 |   500/  658 batches | lr 0.00006 | ms/batch 16.51 | loss 0.05676386\n",
      "| Epoch 139 |   550/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.05573079\n",
      "| Epoch 139 |   600/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.05842661\n",
      "| Epoch 139 |   650/  658 batches | lr 0.00006 | ms/batch 15.19 | loss 0.05592088\n",
      "\n",
      "Val set: Average loss: 0.05847504\n",
      "\n",
      "| Epoch 140 |    50/  658 batches | lr 0.00006 | ms/batch 18.63 | loss 0.06163062\n",
      "| Epoch 140 |   100/  658 batches | lr 0.00006 | ms/batch 17.64 | loss 0.05890160\n",
      "| Epoch 140 |   150/  658 batches | lr 0.00006 | ms/batch 15.32 | loss 0.05485781\n",
      "| Epoch 140 |   200/  658 batches | lr 0.00006 | ms/batch 17.30 | loss 0.05446030\n",
      "| Epoch 140 |   250/  658 batches | lr 0.00006 | ms/batch 22.77 | loss 0.05800553\n",
      "| Epoch 140 |   300/  658 batches | lr 0.00006 | ms/batch 22.07 | loss 0.05886660\n",
      "| Epoch 140 |   350/  658 batches | lr 0.00006 | ms/batch 17.43 | loss 0.05914191\n",
      "| Epoch 140 |   400/  658 batches | lr 0.00006 | ms/batch 17.88 | loss 0.05660861\n",
      "| Epoch 140 |   450/  658 batches | lr 0.00006 | ms/batch 17.16 | loss 0.05290978\n",
      "| Epoch 140 |   500/  658 batches | lr 0.00006 | ms/batch 18.50 | loss 0.05701316\n",
      "| Epoch 140 |   550/  658 batches | lr 0.00006 | ms/batch 18.58 | loss 0.05573988\n",
      "| Epoch 140 |   600/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.05831077\n",
      "| Epoch 140 |   650/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05588820\n",
      "\n",
      "Val set: Average loss: 0.05852634\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 141 |    50/  658 batches | lr 0.00006 | ms/batch 16.78 | loss 0.06156718\n",
      "| Epoch 141 |   100/  658 batches | lr 0.00006 | ms/batch 19.76 | loss 0.05876227\n",
      "| Epoch 141 |   150/  658 batches | lr 0.00006 | ms/batch 17.52 | loss 0.05475003\n",
      "| Epoch 141 |   200/  658 batches | lr 0.00006 | ms/batch 20.10 | loss 0.05450853\n",
      "| Epoch 141 |   250/  658 batches | lr 0.00006 | ms/batch 19.07 | loss 0.05790325\n",
      "| Epoch 141 |   300/  658 batches | lr 0.00006 | ms/batch 17.08 | loss 0.05882158\n",
      "| Epoch 141 |   350/  658 batches | lr 0.00006 | ms/batch 18.62 | loss 0.05908601\n",
      "| Epoch 141 |   400/  658 batches | lr 0.00006 | ms/batch 19.77 | loss 0.05658397\n",
      "| Epoch 141 |   450/  658 batches | lr 0.00006 | ms/batch 19.81 | loss 0.05275844\n",
      "| Epoch 141 |   500/  658 batches | lr 0.00006 | ms/batch 18.30 | loss 0.05678810\n",
      "| Epoch 141 |   550/  658 batches | lr 0.00006 | ms/batch 17.47 | loss 0.05568266\n",
      "| Epoch 141 |   600/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.05820648\n",
      "| Epoch 141 |   650/  658 batches | lr 0.00006 | ms/batch 17.36 | loss 0.05569846\n",
      "\n",
      "Val set: Average loss: 0.05843061\n",
      "\n",
      "| Epoch 142 |    50/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.06141998\n",
      "| Epoch 142 |   100/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.05865305\n",
      "| Epoch 142 |   150/  658 batches | lr 0.00006 | ms/batch 14.60 | loss 0.05469802\n",
      "| Epoch 142 |   200/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05436583\n",
      "| Epoch 142 |   250/  658 batches | lr 0.00006 | ms/batch 16.44 | loss 0.05787861\n",
      "| Epoch 142 |   300/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.05863968\n",
      "| Epoch 142 |   350/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.05913558\n",
      "| Epoch 142 |   400/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.05657925\n",
      "| Epoch 142 |   450/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05287866\n",
      "| Epoch 142 |   500/  658 batches | lr 0.00006 | ms/batch 16.66 | loss 0.05670083\n",
      "| Epoch 142 |   550/  658 batches | lr 0.00006 | ms/batch 16.17 | loss 0.05570362\n",
      "| Epoch 142 |   600/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05811953\n",
      "| Epoch 142 |   650/  658 batches | lr 0.00006 | ms/batch 14.60 | loss 0.05566974\n",
      "\n",
      "Val set: Average loss: 0.05849240\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 143 |    50/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.06141268\n",
      "| Epoch 143 |   100/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05864673\n",
      "| Epoch 143 |   150/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05457490\n",
      "| Epoch 143 |   200/  658 batches | lr 0.00006 | ms/batch 16.85 | loss 0.05423246\n",
      "| Epoch 143 |   250/  658 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05771499\n",
      "| Epoch 143 |   300/  658 batches | lr 0.00006 | ms/batch 14.41 | loss 0.05873735\n",
      "| Epoch 143 |   350/  658 batches | lr 0.00006 | ms/batch 14.52 | loss 0.05905487\n",
      "| Epoch 143 |   400/  658 batches | lr 0.00006 | ms/batch 16.20 | loss 0.05655764\n",
      "| Epoch 143 |   450/  658 batches | lr 0.00006 | ms/batch 14.60 | loss 0.05276591\n",
      "| Epoch 143 |   500/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.05672058\n",
      "| Epoch 143 |   550/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.05555517\n",
      "| Epoch 143 |   600/  658 batches | lr 0.00006 | ms/batch 14.52 | loss 0.05813916\n",
      "| Epoch 143 |   650/  658 batches | lr 0.00006 | ms/batch 14.57 | loss 0.05575171\n",
      "\n",
      "Val set: Average loss: 0.05846629\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch 144 |    50/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.06143747\n",
      "| Epoch 144 |   100/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05869633\n",
      "| Epoch 144 |   150/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05450257\n",
      "| Epoch 144 |   200/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05413730\n",
      "| Epoch 144 |   250/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.05794770\n",
      "| Epoch 144 |   300/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.05884590\n",
      "| Epoch 144 |   350/  658 batches | lr 0.00006 | ms/batch 14.60 | loss 0.05905429\n",
      "| Epoch 144 |   400/  658 batches | lr 0.00006 | ms/batch 16.20 | loss 0.05650162\n",
      "| Epoch 144 |   450/  658 batches | lr 0.00006 | ms/batch 14.61 | loss 0.05276629\n",
      "| Epoch 144 |   500/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.05667218\n",
      "| Epoch 144 |   550/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05549737\n",
      "| Epoch 144 |   600/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05815988\n",
      "| Epoch 144 |   650/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05569951\n",
      "\n",
      "Val set: Average loss: 0.05841368\n",
      "\n",
      "| Epoch 145 |    50/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.06132711\n",
      "| Epoch 145 |   100/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.05867122\n",
      "| Epoch 145 |   150/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05442914\n",
      "| Epoch 145 |   200/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.05409131\n",
      "| Epoch 145 |   250/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05785382\n",
      "| Epoch 145 |   300/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.05881606\n",
      "| Epoch 145 |   350/  658 batches | lr 0.00006 | ms/batch 14.58 | loss 0.05908291\n",
      "| Epoch 145 |   400/  658 batches | lr 0.00006 | ms/batch 16.21 | loss 0.05685213\n",
      "| Epoch 145 |   450/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.05268448\n",
      "| Epoch 145 |   500/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.05658239\n",
      "| Epoch 145 |   550/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.05552445\n",
      "| Epoch 145 |   600/  658 batches | lr 0.00006 | ms/batch 14.52 | loss 0.05803966\n",
      "| Epoch 145 |   650/  658 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05556040\n",
      "\n",
      "Val set: Average loss: 0.05841000\n",
      "\n",
      "| Epoch 146 |    50/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.06123951\n",
      "| Epoch 146 |   100/  658 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05840408\n",
      "| Epoch 146 |   150/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05438227\n",
      "| Epoch 146 |   200/  658 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05410493\n",
      "| Epoch 146 |   250/  658 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05779209\n",
      "| Epoch 146 |   300/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05860734\n",
      "| Epoch 146 |   350/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05888119\n",
      "| Epoch 146 |   400/  658 batches | lr 0.00006 | ms/batch 16.01 | loss 0.05646458\n",
      "| Epoch 146 |   450/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.05268791\n",
      "| Epoch 146 |   500/  658 batches | lr 0.00006 | ms/batch 16.21 | loss 0.05655449\n",
      "| Epoch 146 |   550/  658 batches | lr 0.00006 | ms/batch 16.03 | loss 0.05537931\n",
      "| Epoch 146 |   600/  658 batches | lr 0.00006 | ms/batch 14.50 | loss 0.05795833\n",
      "| Epoch 146 |   650/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05550861\n",
      "\n",
      "Val set: Average loss: 0.05843917\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 147 |    50/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06120505\n",
      "| Epoch 147 |   100/  658 batches | lr 0.00006 | ms/batch 16.06 | loss 0.05844266\n",
      "| Epoch 147 |   150/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.05431547\n",
      "| Epoch 147 |   200/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.05394578\n",
      "| Epoch 147 |   250/  658 batches | lr 0.00006 | ms/batch 17.00 | loss 0.05766888\n",
      "| Epoch 147 |   300/  658 batches | lr 0.00006 | ms/batch 14.61 | loss 0.05870080\n",
      "| Epoch 147 |   350/  658 batches | lr 0.00006 | ms/batch 14.58 | loss 0.05887084\n",
      "| Epoch 147 |   400/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.05659241\n",
      "| Epoch 147 |   450/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05265805\n",
      "| Epoch 147 |   500/  658 batches | lr 0.00006 | ms/batch 16.17 | loss 0.05656356\n",
      "| Epoch 147 |   550/  658 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05527929\n",
      "| Epoch 147 |   600/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05788312\n",
      "| Epoch 147 |   650/  658 batches | lr 0.00006 | ms/batch 14.60 | loss 0.05542033\n",
      "\n",
      "Val set: Average loss: 0.05835776\n",
      "\n",
      "| Epoch 148 |    50/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06119677\n",
      "| Epoch 148 |   100/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05844378\n",
      "| Epoch 148 |   150/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.05419686\n",
      "| Epoch 148 |   200/  658 batches | lr 0.00006 | ms/batch 16.20 | loss 0.05382307\n",
      "| Epoch 148 |   250/  658 batches | lr 0.00006 | ms/batch 16.56 | loss 0.05784164\n",
      "| Epoch 148 |   300/  658 batches | lr 0.00006 | ms/batch 14.39 | loss 0.05859550\n",
      "| Epoch 148 |   350/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05903207\n",
      "| Epoch 148 |   400/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.05652345\n",
      "| Epoch 148 |   450/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05232923\n",
      "| Epoch 148 |   500/  658 batches | lr 0.00006 | ms/batch 16.30 | loss 0.05676440\n",
      "| Epoch 148 |   550/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.05525412\n",
      "| Epoch 148 |   600/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05789074\n",
      "| Epoch 148 |   650/  658 batches | lr 0.00006 | ms/batch 14.57 | loss 0.05533779\n",
      "\n",
      "Val set: Average loss: 0.05838562\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 149 |    50/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.06162288\n",
      "| Epoch 149 |   100/  658 batches | lr 0.00006 | ms/batch 16.06 | loss 0.05874347\n",
      "| Epoch 149 |   150/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05435774\n",
      "| Epoch 149 |   200/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05393417\n",
      "| Epoch 149 |   250/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05747730\n",
      "| Epoch 149 |   300/  658 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05846614\n",
      "| Epoch 149 |   350/  658 batches | lr 0.00006 | ms/batch 14.60 | loss 0.05885927\n",
      "| Epoch 149 |   400/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.05640595\n",
      "| Epoch 149 |   450/  658 batches | lr 0.00006 | ms/batch 14.57 | loss 0.05241831\n",
      "| Epoch 149 |   500/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.05660573\n",
      "| Epoch 149 |   550/  658 batches | lr 0.00006 | ms/batch 16.04 | loss 0.05523059\n",
      "| Epoch 149 |   600/  658 batches | lr 0.00006 | ms/batch 14.41 | loss 0.05782819\n",
      "| Epoch 149 |   650/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05528587\n",
      "\n",
      "Val set: Average loss: 0.05842344\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch 150 |    50/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06145647\n",
      "| Epoch 150 |   100/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.05843133\n",
      "| Epoch 150 |   150/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05421937\n",
      "| Epoch 150 |   200/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05402146\n",
      "| Epoch 150 |   250/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.05754884\n",
      "| Epoch 150 |   300/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05843073\n",
      "| Epoch 150 |   350/  658 batches | lr 0.00006 | ms/batch 14.37 | loss 0.05875276\n",
      "| Epoch 150 |   400/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05642463\n",
      "| Epoch 150 |   450/  658 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05226748\n",
      "| Epoch 150 |   500/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.05653373\n",
      "| Epoch 150 |   550/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05511448\n",
      "| Epoch 150 |   600/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05780577\n",
      "| Epoch 150 |   650/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05521556\n",
      "\n",
      "Val set: Average loss: 0.05846177\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch 151 |    50/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06145251\n",
      "| Epoch 151 |   100/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.05851207\n",
      "| Epoch 151 |   150/  658 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05431725\n",
      "| Epoch 151 |   200/  658 batches | lr 0.00006 | ms/batch 16.19 | loss 0.05396184\n",
      "| Epoch 151 |   250/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.05756435\n",
      "| Epoch 151 |   300/  658 batches | lr 0.00006 | ms/batch 14.50 | loss 0.05853119\n",
      "| Epoch 151 |   350/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05883616\n",
      "| Epoch 151 |   400/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.05635505\n",
      "| Epoch 151 |   450/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05191342\n",
      "| Epoch 151 |   500/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.05799766\n",
      "| Epoch 151 |   550/  658 batches | lr 0.00006 | ms/batch 16.17 | loss 0.05565167\n",
      "| Epoch 151 |   600/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05816974\n",
      "| Epoch 151 |   650/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05544016\n",
      "\n",
      "Val set: Average loss: 0.05838287\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch 152 |    50/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.06113617\n",
      "| Epoch 152 |   100/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05838619\n",
      "| Epoch 152 |   150/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05408740\n",
      "| Epoch 152 |   200/  658 batches | lr 0.00006 | ms/batch 16.29 | loss 0.05377119\n",
      "| Epoch 152 |   250/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.05761420\n",
      "| Epoch 152 |   300/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.05837843\n",
      "| Epoch 152 |   350/  658 batches | lr 0.00006 | ms/batch 14.58 | loss 0.05873908\n",
      "| Epoch 152 |   400/  658 batches | lr 0.00006 | ms/batch 16.20 | loss 0.05634133\n",
      "| Epoch 152 |   450/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.05188522\n",
      "| Epoch 152 |   500/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.05814446\n",
      "| Epoch 152 |   550/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05572014\n",
      "| Epoch 152 |   600/  658 batches | lr 0.00006 | ms/batch 14.36 | loss 0.05833510\n",
      "| Epoch 152 |   650/  658 batches | lr 0.00006 | ms/batch 14.36 | loss 0.05522932\n",
      "\n",
      "Val set: Average loss: 0.05850220\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch 153 |    50/  658 batches | lr 0.00006 | ms/batch 14.89 | loss 0.06171918\n",
      "| Epoch 153 |   100/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05856779\n",
      "| Epoch 153 |   150/  658 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05419676\n",
      "| Epoch 153 |   200/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05405921\n",
      "| Epoch 153 |   250/  658 batches | lr 0.00006 | ms/batch 16.29 | loss 0.05742002\n",
      "| Epoch 153 |   300/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05839075\n",
      "| Epoch 153 |   350/  658 batches | lr 0.00006 | ms/batch 14.52 | loss 0.05880087\n",
      "| Epoch 153 |   400/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05640317\n",
      "| Epoch 153 |   450/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.05198071\n",
      "| Epoch 153 |   500/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.05650897\n",
      "| Epoch 153 |   550/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.05506855\n",
      "| Epoch 153 |   600/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05777085\n",
      "| Epoch 153 |   650/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.05513832\n",
      "\n",
      "Val set: Average loss: 0.05843125\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch 154 |    50/  658 batches | lr 0.00006 | ms/batch 14.90 | loss 0.06130090\n",
      "| Epoch 154 |   100/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05833532\n",
      "| Epoch 154 |   150/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.05412537\n",
      "| Epoch 154 |   200/  658 batches | lr 0.00006 | ms/batch 16.46 | loss 0.05347191\n",
      "| Epoch 154 |   250/  658 batches | lr 0.00006 | ms/batch 16.51 | loss 0.05735493\n",
      "| Epoch 154 |   300/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.05807144\n",
      "| Epoch 154 |   350/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.05847587\n",
      "| Epoch 154 |   400/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.05618441\n",
      "| Epoch 154 |   450/  658 batches | lr 0.00006 | ms/batch 14.84 | loss 0.05195796\n",
      "| Epoch 154 |   500/  658 batches | lr 0.00006 | ms/batch 16.51 | loss 0.05625792\n",
      "| Epoch 154 |   550/  658 batches | lr 0.00006 | ms/batch 16.60 | loss 0.05487427\n",
      "| Epoch 154 |   600/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.05768265\n",
      "| Epoch 154 |   650/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.05515836\n",
      "\n",
      "Val set: Average loss: 0.05849476\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch 155 |    50/  658 batches | lr 0.00006 | ms/batch 14.88 | loss 0.06118255\n",
      "| Epoch 155 |   100/  658 batches | lr 0.00006 | ms/batch 16.20 | loss 0.05823062\n",
      "| Epoch 155 |   150/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.05396586\n",
      "| Epoch 155 |   200/  658 batches | lr 0.00006 | ms/batch 17.05 | loss 0.05346510\n",
      "| Epoch 155 |   250/  658 batches | lr 0.00006 | ms/batch 16.97 | loss 0.05728579\n",
      "| Epoch 155 |   300/  658 batches | lr 0.00006 | ms/batch 15.38 | loss 0.05817120\n",
      "| Epoch 155 |   350/  658 batches | lr 0.00006 | ms/batch 15.33 | loss 0.05845267\n",
      "| Epoch 155 |   400/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.05610690\n",
      "| Epoch 155 |   450/  658 batches | lr 0.00006 | ms/batch 14.56 | loss 0.05184140\n",
      "| Epoch 155 |   500/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.05641383\n",
      "| Epoch 155 |   550/  658 batches | lr 0.00006 | ms/batch 17.94 | loss 0.05482573\n",
      "| Epoch 155 |   600/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.05764096\n",
      "| Epoch 155 |   650/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.05515652\n",
      "\n",
      "Val set: Average loss: 0.05847365\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch 156 |    50/  658 batches | lr 0.00006 | ms/batch 15.46 | loss 0.06126123\n",
      "| Epoch 156 |   100/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.05845847\n",
      "| Epoch 156 |   150/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.05380348\n",
      "| Epoch 156 |   200/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.05348322\n",
      "| Epoch 156 |   250/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.05746594\n",
      "| Epoch 156 |   300/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.05813089\n",
      "| Epoch 156 |   350/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.05845285\n",
      "| Epoch 156 |   400/  658 batches | lr 0.00006 | ms/batch 17.56 | loss 0.05586429\n",
      "| Epoch 156 |   450/  658 batches | lr 0.00006 | ms/batch 15.85 | loss 0.05190095\n",
      "| Epoch 156 |   500/  658 batches | lr 0.00006 | ms/batch 19.69 | loss 0.05787321\n",
      "| Epoch 156 |   550/  658 batches | lr 0.00006 | ms/batch 17.70 | loss 0.05546710\n",
      "| Epoch 156 |   600/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.05794714\n",
      "| Epoch 156 |   650/  658 batches | lr 0.00006 | ms/batch 14.52 | loss 0.05505044\n",
      "\n",
      "Val set: Average loss: 0.05844719\n",
      "\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch 157 |    50/  658 batches | lr 0.00006 | ms/batch 17.74 | loss 0.06071360\n",
      "| Epoch 157 |   100/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.05798722\n",
      "| Epoch 157 |   150/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.05373046\n",
      "| Epoch 157 |   200/  658 batches | lr 0.00006 | ms/batch 17.80 | loss 0.05357877\n",
      "| Epoch 157 |   250/  658 batches | lr 0.00006 | ms/batch 16.48 | loss 0.05716763\n",
      "| Epoch 157 |   300/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.05810393\n",
      "| Epoch 157 |   350/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.05826592\n",
      "| Epoch 157 |   400/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.05616680\n",
      "| Epoch 157 |   450/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.05165155\n",
      "| Epoch 157 |   500/  658 batches | lr 0.00006 | ms/batch 16.82 | loss 0.05791996\n",
      "| Epoch 157 |   550/  658 batches | lr 0.00006 | ms/batch 16.61 | loss 0.05539969\n",
      "| Epoch 157 |   600/  658 batches | lr 0.00006 | ms/batch 14.57 | loss 0.05794807\n",
      "| Epoch 157 |   650/  658 batches | lr 0.00006 | ms/batch 14.50 | loss 0.05513558\n",
      "\n",
      "Val set: Average loss: 0.05851979\n",
      "\n",
      "EarlyStopping counter: 10 out of 20\n",
      "| Epoch 158 |    50/  658 batches | lr 0.00006 | ms/batch 14.97 | loss 0.06075741\n",
      "| Epoch 158 |   100/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05791588\n",
      "| Epoch 158 |   150/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.05374226\n",
      "| Epoch 158 |   200/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.05333471\n",
      "| Epoch 158 |   250/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.05719019\n",
      "| Epoch 158 |   300/  658 batches | lr 0.00006 | ms/batch 14.52 | loss 0.05806373\n",
      "| Epoch 158 |   350/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.05832823\n",
      "| Epoch 158 |   400/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05598097\n",
      "| Epoch 158 |   450/  658 batches | lr 0.00006 | ms/batch 16.97 | loss 0.05147761\n",
      "| Epoch 158 |   500/  658 batches | lr 0.00006 | ms/batch 16.81 | loss 0.05764994\n",
      "| Epoch 158 |   550/  658 batches | lr 0.00006 | ms/batch 18.15 | loss 0.05502577\n",
      "| Epoch 158 |   600/  658 batches | lr 0.00006 | ms/batch 17.18 | loss 0.05764943\n",
      "| Epoch 158 |   650/  658 batches | lr 0.00006 | ms/batch 17.33 | loss 0.05509683\n",
      "\n",
      "Val set: Average loss: 0.05841025\n",
      "\n",
      "EarlyStopping counter: 11 out of 20\n",
      "| Epoch 159 |    50/  658 batches | lr 0.00006 | ms/batch 21.16 | loss 0.06126798\n",
      "| Epoch 159 |   100/  658 batches | lr 0.00006 | ms/batch 22.08 | loss 0.05823253\n",
      "| Epoch 159 |   150/  658 batches | lr 0.00006 | ms/batch 17.29 | loss 0.05367816\n",
      "| Epoch 159 |   200/  658 batches | lr 0.00006 | ms/batch 20.08 | loss 0.05322170\n",
      "| Epoch 159 |   250/  658 batches | lr 0.00006 | ms/batch 20.95 | loss 0.05738254\n",
      "| Epoch 159 |   300/  658 batches | lr 0.00006 | ms/batch 22.86 | loss 0.05800628\n",
      "| Epoch 159 |   350/  658 batches | lr 0.00006 | ms/batch 24.81 | loss 0.05841487\n",
      "| Epoch 159 |   400/  658 batches | lr 0.00006 | ms/batch 25.10 | loss 0.05610322\n",
      "| Epoch 159 |   450/  658 batches | lr 0.00006 | ms/batch 24.60 | loss 0.05155163\n",
      "| Epoch 159 |   500/  658 batches | lr 0.00006 | ms/batch 25.47 | loss 0.05773643\n",
      "| Epoch 159 |   550/  658 batches | lr 0.00006 | ms/batch 17.03 | loss 0.05515484\n",
      "| Epoch 159 |   600/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05762865\n",
      "| Epoch 159 |   650/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.05487153\n",
      "\n",
      "Val set: Average loss: 0.05843905\n",
      "\n",
      "EarlyStopping counter: 12 out of 20\n",
      "| Epoch 160 |    50/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.06052096\n",
      "| Epoch 160 |   100/  658 batches | lr 0.00006 | ms/batch 17.00 | loss 0.05783568\n",
      "| Epoch 160 |   150/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.05362066\n",
      "| Epoch 160 |   200/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05310702\n",
      "| Epoch 160 |   250/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05711940\n",
      "| Epoch 160 |   300/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05795196\n",
      "| Epoch 160 |   350/  658 batches | lr 0.00006 | ms/batch 14.52 | loss 0.05824551\n",
      "| Epoch 160 |   400/  658 batches | lr 0.00006 | ms/batch 16.30 | loss 0.05574799\n",
      "| Epoch 160 |   450/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.05160297\n",
      "| Epoch 160 |   500/  658 batches | lr 0.00006 | ms/batch 16.45 | loss 0.05766595\n",
      "| Epoch 160 |   550/  658 batches | lr 0.00006 | ms/batch 18.99 | loss 0.05507092\n",
      "| Epoch 160 |   600/  658 batches | lr 0.00006 | ms/batch 16.06 | loss 0.05768929\n",
      "| Epoch 160 |   650/  658 batches | lr 0.00006 | ms/batch 16.68 | loss 0.05492737\n",
      "\n",
      "Val set: Average loss: 0.05846978\n",
      "\n",
      "EarlyStopping counter: 13 out of 20\n",
      "| Epoch 161 |    50/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.06043260\n",
      "| Epoch 161 |   100/  658 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05774940\n",
      "| Epoch 161 |   150/  658 batches | lr 0.00006 | ms/batch 14.56 | loss 0.05363419\n",
      "| Epoch 161 |   200/  658 batches | lr 0.00006 | ms/batch 16.29 | loss 0.05326838\n",
      "| Epoch 161 |   250/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05703188\n",
      "| Epoch 161 |   300/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.05787629\n",
      "| Epoch 161 |   350/  658 batches | lr 0.00006 | ms/batch 15.07 | loss 0.05822102\n",
      "| Epoch 161 |   400/  658 batches | lr 0.00006 | ms/batch 16.79 | loss 0.05583746\n",
      "| Epoch 161 |   450/  658 batches | lr 0.00006 | ms/batch 14.50 | loss 0.05152318\n",
      "| Epoch 161 |   500/  658 batches | lr 0.00006 | ms/batch 16.30 | loss 0.05741050\n",
      "| Epoch 161 |   550/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.05526388\n",
      "| Epoch 161 |   600/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.05781532\n",
      "| Epoch 161 |   650/  658 batches | lr 0.00006 | ms/batch 15.84 | loss 0.05491635\n",
      "\n",
      "Val set: Average loss: 0.05839426\n",
      "\n",
      "EarlyStopping counter: 14 out of 20\n",
      "| Epoch 162 |    50/  658 batches | lr 0.00006 | ms/batch 18.56 | loss 0.06025212\n",
      "| Epoch 162 |   100/  658 batches | lr 0.00006 | ms/batch 21.47 | loss 0.05773001\n",
      "| Epoch 162 |   150/  658 batches | lr 0.00006 | ms/batch 17.89 | loss 0.05361171\n",
      "| Epoch 162 |   200/  658 batches | lr 0.00006 | ms/batch 16.49 | loss 0.05323379\n",
      "| Epoch 162 |   250/  658 batches | lr 0.00006 | ms/batch 18.36 | loss 0.05719649\n",
      "| Epoch 162 |   300/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05779825\n",
      "| Epoch 162 |   350/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05821511\n",
      "| Epoch 162 |   400/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05575057\n",
      "| Epoch 162 |   450/  658 batches | lr 0.00006 | ms/batch 14.60 | loss 0.05148444\n",
      "| Epoch 162 |   500/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.05734415\n",
      "| Epoch 162 |   550/  658 batches | lr 0.00006 | ms/batch 16.91 | loss 0.05495212\n",
      "| Epoch 162 |   600/  658 batches | lr 0.00006 | ms/batch 14.98 | loss 0.05765282\n",
      "| Epoch 162 |   650/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05481761\n",
      "\n",
      "Val set: Average loss: 0.05833775\n",
      "\n",
      "| Epoch 163 |    50/  658 batches | lr 0.00006 | ms/batch 14.89 | loss 0.06118301\n",
      "| Epoch 163 |   100/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.05769024\n",
      "| Epoch 163 |   150/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.05355454\n",
      "| Epoch 163 |   200/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.05323190\n",
      "| Epoch 163 |   250/  658 batches | lr 0.00006 | ms/batch 16.81 | loss 0.05707931\n",
      "| Epoch 163 |   300/  658 batches | lr 0.00006 | ms/batch 15.00 | loss 0.05774767\n",
      "| Epoch 163 |   350/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.05804034\n",
      "| Epoch 163 |   400/  658 batches | lr 0.00006 | ms/batch 16.97 | loss 0.05564155\n",
      "| Epoch 163 |   450/  658 batches | lr 0.00006 | ms/batch 16.00 | loss 0.05139555\n",
      "| Epoch 163 |   500/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05745357\n",
      "| Epoch 163 |   550/  658 batches | lr 0.00006 | ms/batch 18.53 | loss 0.05494273\n",
      "| Epoch 163 |   600/  658 batches | lr 0.00006 | ms/batch 14.68 | loss 0.05764220\n",
      "| Epoch 163 |   650/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05480305\n",
      "\n",
      "Val set: Average loss: 0.05839435\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 164 |    50/  658 batches | lr 0.00006 | ms/batch 15.05 | loss 0.06052497\n",
      "| Epoch 164 |   100/  658 batches | lr 0.00006 | ms/batch 17.65 | loss 0.05759796\n",
      "| Epoch 164 |   150/  658 batches | lr 0.00006 | ms/batch 16.04 | loss 0.05347217\n",
      "| Epoch 164 |   200/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05316861\n",
      "| Epoch 164 |   250/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05687793\n",
      "| Epoch 164 |   300/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05772519\n",
      "| Epoch 164 |   350/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05807301\n",
      "| Epoch 164 |   400/  658 batches | lr 0.00006 | ms/batch 16.19 | loss 0.05561360\n",
      "| Epoch 164 |   450/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.05141025\n",
      "| Epoch 164 |   500/  658 batches | lr 0.00006 | ms/batch 17.29 | loss 0.05699978\n",
      "| Epoch 164 |   550/  658 batches | lr 0.00006 | ms/batch 18.97 | loss 0.05473782\n",
      "| Epoch 164 |   600/  658 batches | lr 0.00006 | ms/batch 15.80 | loss 0.05758872\n",
      "| Epoch 164 |   650/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.05489504\n",
      "\n",
      "Val set: Average loss: 0.05816902\n",
      "\n",
      "| Epoch 165 |    50/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.06077959\n",
      "| Epoch 165 |   100/  658 batches | lr 0.00006 | ms/batch 18.51 | loss 0.05780104\n",
      "| Epoch 165 |   150/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.05373860\n",
      "| Epoch 165 |   200/  658 batches | lr 0.00006 | ms/batch 17.39 | loss 0.05305755\n",
      "| Epoch 165 |   250/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05712343\n",
      "| Epoch 165 |   300/  658 batches | lr 0.00006 | ms/batch 14.94 | loss 0.05756613\n",
      "| Epoch 165 |   350/  658 batches | lr 0.00006 | ms/batch 14.92 | loss 0.05793860\n",
      "| Epoch 165 |   400/  658 batches | lr 0.00006 | ms/batch 16.59 | loss 0.05563228\n",
      "| Epoch 165 |   450/  658 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05134443\n",
      "| Epoch 165 |   500/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05736101\n",
      "| Epoch 165 |   550/  658 batches | lr 0.00006 | ms/batch 15.94 | loss 0.05506608\n",
      "| Epoch 165 |   600/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05753117\n",
      "| Epoch 165 |   650/  658 batches | lr 0.00006 | ms/batch 14.61 | loss 0.05503018\n",
      "\n",
      "Val set: Average loss: 0.05807921\n",
      "\n",
      "| Epoch 166 |    50/  658 batches | lr 0.00006 | ms/batch 15.07 | loss 0.06003766\n",
      "| Epoch 166 |   100/  658 batches | lr 0.00006 | ms/batch 16.07 | loss 0.05742156\n",
      "| Epoch 166 |   150/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05335736\n",
      "| Epoch 166 |   200/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05298210\n",
      "| Epoch 166 |   250/  658 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05687761\n",
      "| Epoch 166 |   300/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05749518\n",
      "| Epoch 166 |   350/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05791909\n",
      "| Epoch 166 |   400/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05548287\n",
      "| Epoch 166 |   450/  658 batches | lr 0.00006 | ms/batch 14.50 | loss 0.05121502\n",
      "| Epoch 166 |   500/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.05717288\n",
      "| Epoch 166 |   550/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.05478203\n",
      "| Epoch 166 |   600/  658 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05740893\n",
      "| Epoch 166 |   650/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.05466212\n",
      "\n",
      "Val set: Average loss: 0.05848702\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 167 |    50/  658 batches | lr 0.00006 | ms/batch 14.76 | loss 0.05986061\n",
      "| Epoch 167 |   100/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05739941\n",
      "| Epoch 167 |   150/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05324219\n",
      "| Epoch 167 |   200/  658 batches | lr 0.00006 | ms/batch 16.20 | loss 0.05284814\n",
      "| Epoch 167 |   250/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.05700098\n",
      "| Epoch 167 |   300/  658 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05743297\n",
      "| Epoch 167 |   350/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.05784233\n",
      "| Epoch 167 |   400/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05547171\n",
      "| Epoch 167 |   450/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05117158\n",
      "| Epoch 167 |   500/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.05714052\n",
      "| Epoch 167 |   550/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.05467532\n",
      "| Epoch 167 |   600/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05747827\n",
      "| Epoch 167 |   650/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05463389\n",
      "\n",
      "Val set: Average loss: 0.05835717\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch 168 |    50/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.05993795\n",
      "| Epoch 168 |   100/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05738107\n",
      "| Epoch 168 |   150/  658 batches | lr 0.00006 | ms/batch 14.39 | loss 0.05324822\n",
      "| Epoch 168 |   200/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.05285349\n",
      "| Epoch 168 |   250/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05691556\n",
      "| Epoch 168 |   300/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05737411\n",
      "| Epoch 168 |   350/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.05772789\n",
      "| Epoch 168 |   400/  658 batches | lr 0.00006 | ms/batch 16.01 | loss 0.05549235\n",
      "| Epoch 168 |   450/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05126314\n",
      "| Epoch 168 |   500/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05709684\n",
      "| Epoch 168 |   550/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.05470521\n",
      "| Epoch 168 |   600/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05731484\n",
      "| Epoch 168 |   650/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05462741\n",
      "\n",
      "Val set: Average loss: 0.05832480\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch 169 |    50/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.05990789\n",
      "| Epoch 169 |   100/  658 batches | lr 0.00006 | ms/batch 16.07 | loss 0.05734702\n",
      "| Epoch 169 |   150/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05332376\n",
      "| Epoch 169 |   200/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05275740\n",
      "| Epoch 169 |   250/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05685652\n",
      "| Epoch 169 |   300/  658 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05714177\n",
      "| Epoch 169 |   350/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.05777426\n",
      "| Epoch 169 |   400/  658 batches | lr 0.00006 | ms/batch 16.17 | loss 0.05544660\n",
      "| Epoch 169 |   450/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05127653\n",
      "| Epoch 169 |   500/  658 batches | lr 0.00006 | ms/batch 16.21 | loss 0.05710519\n",
      "| Epoch 169 |   550/  658 batches | lr 0.00006 | ms/batch 16.04 | loss 0.05494686\n",
      "| Epoch 169 |   600/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05764267\n",
      "| Epoch 169 |   650/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05460201\n",
      "\n",
      "Val set: Average loss: 0.05826608\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch 170 |    50/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.06050377\n",
      "| Epoch 170 |   100/  658 batches | lr 0.00006 | ms/batch 16.06 | loss 0.05754651\n",
      "| Epoch 170 |   150/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05340136\n",
      "| Epoch 170 |   200/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.05285914\n",
      "| Epoch 170 |   250/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.05691855\n",
      "| Epoch 170 |   300/  658 batches | lr 0.00006 | ms/batch 14.52 | loss 0.05726867\n",
      "| Epoch 170 |   350/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05787496\n",
      "| Epoch 170 |   400/  658 batches | lr 0.00006 | ms/batch 16.05 | loss 0.05540173\n",
      "| Epoch 170 |   450/  658 batches | lr 0.00006 | ms/batch 14.57 | loss 0.05108968\n",
      "| Epoch 170 |   500/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.05685775\n",
      "| Epoch 170 |   550/  658 batches | lr 0.00006 | ms/batch 16.06 | loss 0.05440551\n",
      "| Epoch 170 |   600/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05718355\n",
      "| Epoch 170 |   650/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05445704\n",
      "\n",
      "Val set: Average loss: 0.05814653\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch 171 |    50/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06026380\n",
      "| Epoch 171 |   100/  658 batches | lr 0.00006 | ms/batch 15.99 | loss 0.05715313\n",
      "| Epoch 171 |   150/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05333078\n",
      "| Epoch 171 |   200/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.05283501\n",
      "| Epoch 171 |   250/  658 batches | lr 0.00006 | ms/batch 16.21 | loss 0.05659470\n",
      "| Epoch 171 |   300/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05746383\n",
      "| Epoch 171 |   350/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05788443\n",
      "| Epoch 171 |   400/  658 batches | lr 0.00006 | ms/batch 16.07 | loss 0.05544669\n",
      "| Epoch 171 |   450/  658 batches | lr 0.00006 | ms/batch 14.67 | loss 0.05122938\n",
      "| Epoch 171 |   500/  658 batches | lr 0.00006 | ms/batch 16.06 | loss 0.05693592\n",
      "| Epoch 171 |   550/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.05455080\n",
      "| Epoch 171 |   600/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.05717904\n",
      "| Epoch 171 |   650/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05433519\n",
      "\n",
      "Val set: Average loss: 0.05827774\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch 172 |    50/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.06041784\n",
      "| Epoch 172 |   100/  658 batches | lr 0.00006 | ms/batch 16.19 | loss 0.05707669\n",
      "| Epoch 172 |   150/  658 batches | lr 0.00006 | ms/batch 14.56 | loss 0.05338832\n",
      "| Epoch 172 |   200/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05305880\n",
      "| Epoch 172 |   250/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.05663789\n",
      "| Epoch 172 |   300/  658 batches | lr 0.00006 | ms/batch 14.29 | loss 0.05720270\n",
      "| Epoch 172 |   350/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05759013\n",
      "| Epoch 172 |   400/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05540820\n",
      "| Epoch 172 |   450/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.05125529\n",
      "| Epoch 172 |   500/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05697542\n",
      "| Epoch 172 |   550/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05451812\n",
      "| Epoch 172 |   600/  658 batches | lr 0.00006 | ms/batch 14.35 | loss 0.05702015\n",
      "| Epoch 172 |   650/  658 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05429353\n",
      "\n",
      "Val set: Average loss: 0.05831578\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch 173 |    50/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.06098439\n",
      "| Epoch 173 |   100/  658 batches | lr 0.00006 | ms/batch 16.07 | loss 0.05718501\n",
      "| Epoch 173 |   150/  658 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05346126\n",
      "| Epoch 173 |   200/  658 batches | lr 0.00006 | ms/batch 16.36 | loss 0.05338797\n",
      "| Epoch 173 |   250/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.05680232\n",
      "| Epoch 173 |   300/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05717075\n",
      "| Epoch 173 |   350/  658 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05770952\n",
      "| Epoch 173 |   400/  658 batches | lr 0.00006 | ms/batch 16.50 | loss 0.05526593\n",
      "| Epoch 173 |   450/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05118392\n",
      "| Epoch 173 |   500/  658 batches | lr 0.00006 | ms/batch 16.06 | loss 0.05695556\n",
      "| Epoch 173 |   550/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05453149\n",
      "| Epoch 173 |   600/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05746452\n",
      "| Epoch 173 |   650/  658 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05440389\n",
      "\n",
      "Val set: Average loss: 0.05831248\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch 174 |    50/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06023690\n",
      "| Epoch 174 |   100/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05746384\n",
      "| Epoch 174 |   150/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.05382412\n",
      "| Epoch 174 |   200/  658 batches | lr 0.00006 | ms/batch 16.20 | loss 0.05286732\n",
      "| Epoch 174 |   250/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.05682744\n",
      "| Epoch 174 |   300/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05719867\n",
      "| Epoch 174 |   350/  658 batches | lr 0.00006 | ms/batch 14.58 | loss 0.05780249\n",
      "| Epoch 174 |   400/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.05534460\n",
      "| Epoch 174 |   450/  658 batches | lr 0.00006 | ms/batch 14.60 | loss 0.05107019\n",
      "| Epoch 174 |   500/  658 batches | lr 0.00006 | ms/batch 16.37 | loss 0.05670070\n",
      "| Epoch 174 |   550/  658 batches | lr 0.00006 | ms/batch 16.63 | loss 0.05424625\n",
      "| Epoch 174 |   600/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05705455\n",
      "| Epoch 174 |   650/  658 batches | lr 0.00006 | ms/batch 14.58 | loss 0.05429310\n",
      "\n",
      "Val set: Average loss: 0.05818058\n",
      "\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch 175 |    50/  658 batches | lr 0.00006 | ms/batch 14.94 | loss 0.06040544\n",
      "| Epoch 175 |   100/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05689911\n",
      "| Epoch 175 |   150/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05333488\n",
      "| Epoch 175 |   200/  658 batches | lr 0.00006 | ms/batch 16.21 | loss 0.05295387\n",
      "| Epoch 175 |   250/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05665557\n",
      "| Epoch 175 |   300/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05702724\n",
      "| Epoch 175 |   350/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05746249\n",
      "| Epoch 175 |   400/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05538357\n",
      "| Epoch 175 |   450/  658 batches | lr 0.00006 | ms/batch 14.57 | loss 0.05106159\n",
      "| Epoch 175 |   500/  658 batches | lr 0.00006 | ms/batch 16.19 | loss 0.05671989\n",
      "| Epoch 175 |   550/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.05422470\n",
      "| Epoch 175 |   600/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.05710821\n",
      "| Epoch 175 |   650/  658 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05420914\n",
      "\n",
      "Val set: Average loss: 0.05813394\n",
      "\n",
      "EarlyStopping counter: 10 out of 20\n",
      "| Epoch 176 |    50/  658 batches | lr 0.00006 | ms/batch 14.80 | loss 0.06017598\n",
      "| Epoch 176 |   100/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.05688820\n",
      "| Epoch 176 |   150/  658 batches | lr 0.00006 | ms/batch 14.58 | loss 0.05354018\n",
      "| Epoch 176 |   200/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.05278630\n",
      "| Epoch 176 |   250/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05651758\n",
      "| Epoch 176 |   300/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05689249\n",
      "| Epoch 176 |   350/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.05736999\n",
      "| Epoch 176 |   400/  658 batches | lr 0.00006 | ms/batch 16.33 | loss 0.05537171\n",
      "| Epoch 176 |   450/  658 batches | lr 0.00006 | ms/batch 14.69 | loss 0.05108630\n",
      "| Epoch 176 |   500/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05661983\n",
      "| Epoch 176 |   550/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.05447690\n",
      "| Epoch 176 |   600/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05711008\n",
      "| Epoch 176 |   650/  658 batches | lr 0.00006 | ms/batch 14.56 | loss 0.05429789\n",
      "\n",
      "Val set: Average loss: 0.05829077\n",
      "\n",
      "EarlyStopping counter: 11 out of 20\n",
      "| Epoch 177 |    50/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.05929052\n",
      "| Epoch 177 |   100/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05694458\n",
      "| Epoch 177 |   150/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.05336572\n",
      "| Epoch 177 |   200/  658 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05246073\n",
      "| Epoch 177 |   250/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05648735\n",
      "| Epoch 177 |   300/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05689890\n",
      "| Epoch 177 |   350/  658 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05717446\n",
      "| Epoch 177 |   400/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.05516765\n",
      "| Epoch 177 |   450/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.05098411\n",
      "| Epoch 177 |   500/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.05652144\n",
      "| Epoch 177 |   550/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.05435429\n",
      "| Epoch 177 |   600/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05696103\n",
      "| Epoch 177 |   650/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05403099\n",
      "\n",
      "Val set: Average loss: 0.05811192\n",
      "\n",
      "EarlyStopping counter: 12 out of 20\n",
      "| Epoch 178 |    50/  658 batches | lr 0.00006 | ms/batch 15.23 | loss 0.06049139\n",
      "| Epoch 178 |   100/  658 batches | lr 0.00006 | ms/batch 16.84 | loss 0.05678803\n",
      "| Epoch 178 |   150/  658 batches | lr 0.00006 | ms/batch 14.85 | loss 0.05323803\n",
      "| Epoch 178 |   200/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05274700\n",
      "| Epoch 178 |   250/  658 batches | lr 0.00006 | ms/batch 16.58 | loss 0.05635474\n",
      "| Epoch 178 |   300/  658 batches | lr 0.00006 | ms/batch 14.91 | loss 0.05674083\n",
      "| Epoch 178 |   350/  658 batches | lr 0.00006 | ms/batch 16.69 | loss 0.05712605\n",
      "| Epoch 178 |   400/  658 batches | lr 0.00006 | ms/batch 18.42 | loss 0.05528817\n",
      "| Epoch 178 |   450/  658 batches | lr 0.00006 | ms/batch 15.33 | loss 0.05114550\n",
      "| Epoch 178 |   500/  658 batches | lr 0.00006 | ms/batch 16.93 | loss 0.05661307\n",
      "| Epoch 178 |   550/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05405746\n",
      "| Epoch 178 |   600/  658 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05682989\n",
      "| Epoch 178 |   650/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05404847\n",
      "\n",
      "Val set: Average loss: 0.05844082\n",
      "\n",
      "EarlyStopping counter: 13 out of 20\n",
      "| Epoch 179 |    50/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.05983582\n",
      "| Epoch 179 |   100/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05715560\n",
      "| Epoch 179 |   150/  658 batches | lr 0.00006 | ms/batch 14.57 | loss 0.05339223\n",
      "| Epoch 179 |   200/  658 batches | lr 0.00006 | ms/batch 16.40 | loss 0.05259846\n",
      "| Epoch 179 |   250/  658 batches | lr 0.00006 | ms/batch 16.39 | loss 0.05630266\n",
      "| Epoch 179 |   300/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05657786\n",
      "| Epoch 179 |   350/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05708014\n",
      "| Epoch 179 |   400/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05546261\n",
      "| Epoch 179 |   450/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05082563\n",
      "| Epoch 179 |   500/  658 batches | lr 0.00006 | ms/batch 16.83 | loss 0.05657861\n",
      "| Epoch 179 |   550/  658 batches | lr 0.00006 | ms/batch 16.84 | loss 0.05410747\n",
      "| Epoch 179 |   600/  658 batches | lr 0.00006 | ms/batch 15.05 | loss 0.05689439\n",
      "| Epoch 179 |   650/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.05396179\n",
      "\n",
      "Val set: Average loss: 0.05819304\n",
      "\n",
      "EarlyStopping counter: 14 out of 20\n",
      "| Epoch 180 |    50/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.06005265\n",
      "| Epoch 180 |   100/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.05660840\n",
      "| Epoch 180 |   150/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05310955\n",
      "| Epoch 180 |   200/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.05260506\n",
      "| Epoch 180 |   250/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05633268\n",
      "| Epoch 180 |   300/  658 batches | lr 0.00006 | ms/batch 14.41 | loss 0.05670327\n",
      "| Epoch 180 |   350/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05717347\n",
      "| Epoch 180 |   400/  658 batches | lr 0.00006 | ms/batch 16.06 | loss 0.05511910\n",
      "| Epoch 180 |   450/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05072759\n",
      "| Epoch 180 |   500/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.05651287\n",
      "| Epoch 180 |   550/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05414445\n",
      "| Epoch 180 |   600/  658 batches | lr 0.00006 | ms/batch 14.56 | loss 0.05692968\n",
      "| Epoch 180 |   650/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05392792\n",
      "\n",
      "Val set: Average loss: 0.05800620\n",
      "\n",
      "| Epoch 181 |    50/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.06027624\n",
      "| Epoch 181 |   100/  658 batches | lr 0.00006 | ms/batch 16.01 | loss 0.05648122\n",
      "| Epoch 181 |   150/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.05307397\n",
      "| Epoch 181 |   200/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05255921\n",
      "| Epoch 181 |   250/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05625594\n",
      "| Epoch 181 |   300/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05677125\n",
      "| Epoch 181 |   350/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05707560\n",
      "| Epoch 181 |   400/  658 batches | lr 0.00006 | ms/batch 16.07 | loss 0.05510805\n",
      "| Epoch 181 |   450/  658 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05068395\n",
      "| Epoch 181 |   500/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.05640663\n",
      "| Epoch 181 |   550/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.05401158\n",
      "| Epoch 181 |   600/  658 batches | lr 0.00006 | ms/batch 14.56 | loss 0.05661714\n",
      "| Epoch 181 |   650/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05377538\n",
      "\n",
      "Val set: Average loss: 0.05816907\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 182 |    50/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.05988664\n",
      "| Epoch 182 |   100/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05676800\n",
      "| Epoch 182 |   150/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05291433\n",
      "| Epoch 182 |   200/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05265207\n",
      "| Epoch 182 |   250/  658 batches | lr 0.00006 | ms/batch 16.19 | loss 0.05611594\n",
      "| Epoch 182 |   300/  658 batches | lr 0.00006 | ms/batch 14.52 | loss 0.05642599\n",
      "| Epoch 182 |   350/  658 batches | lr 0.00006 | ms/batch 14.39 | loss 0.05699778\n",
      "| Epoch 182 |   400/  658 batches | lr 0.00006 | ms/batch 15.97 | loss 0.05523631\n",
      "| Epoch 182 |   450/  658 batches | lr 0.00006 | ms/batch 14.50 | loss 0.05103352\n",
      "| Epoch 182 |   500/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05646225\n",
      "| Epoch 182 |   550/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05389760\n",
      "| Epoch 182 |   600/  658 batches | lr 0.00006 | ms/batch 14.50 | loss 0.05677611\n",
      "| Epoch 182 |   650/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05374989\n",
      "\n",
      "Val set: Average loss: 0.05809998\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch 183 |    50/  658 batches | lr 0.00006 | ms/batch 14.75 | loss 0.06002142\n",
      "| Epoch 183 |   100/  658 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05627265\n",
      "| Epoch 183 |   150/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.05301835\n",
      "| Epoch 183 |   200/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.05253709\n",
      "| Epoch 183 |   250/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.05598922\n",
      "| Epoch 183 |   300/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.05661120\n",
      "| Epoch 183 |   350/  658 batches | lr 0.00006 | ms/batch 14.41 | loss 0.05693331\n",
      "| Epoch 183 |   400/  658 batches | lr 0.00006 | ms/batch 16.17 | loss 0.05492551\n",
      "| Epoch 183 |   450/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05069002\n",
      "| Epoch 183 |   500/  658 batches | lr 0.00006 | ms/batch 16.17 | loss 0.05637861\n",
      "| Epoch 183 |   550/  658 batches | lr 0.00006 | ms/batch 16.38 | loss 0.05408724\n",
      "| Epoch 183 |   600/  658 batches | lr 0.00006 | ms/batch 15.17 | loss 0.05661448\n",
      "| Epoch 183 |   650/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05376251\n",
      "\n",
      "Val set: Average loss: 0.05802536\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch 184 |    50/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.05977260\n",
      "| Epoch 184 |   100/  658 batches | lr 0.00006 | ms/batch 16.19 | loss 0.05632197\n",
      "| Epoch 184 |   150/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.05290779\n",
      "| Epoch 184 |   200/  658 batches | lr 0.00006 | ms/batch 16.25 | loss 0.05242061\n",
      "| Epoch 184 |   250/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.05620922\n",
      "| Epoch 184 |   300/  658 batches | lr 0.00006 | ms/batch 14.57 | loss 0.05661204\n",
      "| Epoch 184 |   350/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05693854\n",
      "| Epoch 184 |   400/  658 batches | lr 0.00006 | ms/batch 16.19 | loss 0.05481395\n",
      "| Epoch 184 |   450/  658 batches | lr 0.00006 | ms/batch 15.51 | loss 0.05079936\n",
      "| Epoch 184 |   500/  658 batches | lr 0.00006 | ms/batch 17.21 | loss 0.05619996\n",
      "| Epoch 184 |   550/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05385681\n",
      "| Epoch 184 |   600/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05660752\n",
      "| Epoch 184 |   650/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05374708\n",
      "\n",
      "Val set: Average loss: 0.05799259\n",
      "\n",
      "| Epoch 185 |    50/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.05937185\n",
      "| Epoch 185 |   100/  658 batches | lr 0.00006 | ms/batch 16.40 | loss 0.05631962\n",
      "| Epoch 185 |   150/  658 batches | lr 0.00006 | ms/batch 14.60 | loss 0.05281196\n",
      "| Epoch 185 |   200/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.05237627\n",
      "| Epoch 185 |   250/  658 batches | lr 0.00006 | ms/batch 16.28 | loss 0.05593197\n",
      "| Epoch 185 |   300/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05637429\n",
      "| Epoch 185 |   350/  658 batches | lr 0.00006 | ms/batch 14.39 | loss 0.05686867\n",
      "| Epoch 185 |   400/  658 batches | lr 0.00006 | ms/batch 16.04 | loss 0.05486671\n",
      "| Epoch 185 |   450/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05089324\n",
      "| Epoch 185 |   500/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.05611948\n",
      "| Epoch 185 |   550/  658 batches | lr 0.00006 | ms/batch 16.20 | loss 0.05377820\n",
      "| Epoch 185 |   600/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05647966\n",
      "| Epoch 185 |   650/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.05363033\n",
      "\n",
      "Val set: Average loss: 0.05789655\n",
      "\n",
      "| Epoch 186 |    50/  658 batches | lr 0.00006 | ms/batch 14.96 | loss 0.06081217\n",
      "| Epoch 186 |   100/  658 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05664626\n",
      "| Epoch 186 |   150/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.05266455\n",
      "| Epoch 186 |   200/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.05227844\n",
      "| Epoch 186 |   250/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.05599973\n",
      "| Epoch 186 |   300/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05636800\n",
      "| Epoch 186 |   350/  658 batches | lr 0.00006 | ms/batch 14.52 | loss 0.05678701\n",
      "| Epoch 186 |   400/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05485763\n",
      "| Epoch 186 |   450/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.05067610\n",
      "| Epoch 186 |   500/  658 batches | lr 0.00006 | ms/batch 16.29 | loss 0.05611838\n",
      "| Epoch 186 |   550/  658 batches | lr 0.00006 | ms/batch 16.21 | loss 0.05363914\n",
      "| Epoch 186 |   600/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.05650287\n",
      "| Epoch 186 |   650/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05354495\n",
      "\n",
      "Val set: Average loss: 0.05805711\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch 187 |    50/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.05921065\n",
      "| Epoch 187 |   100/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.05639928\n",
      "| Epoch 187 |   150/  658 batches | lr 0.00006 | ms/batch 14.56 | loss 0.05269105\n",
      "| Epoch 187 |   200/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.05215690\n",
      "| Epoch 187 |   250/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.05586468\n",
      "| Epoch 187 |   300/  658 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05625775\n",
      "| Epoch 187 |   350/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05678046\n",
      "| Epoch 187 |   400/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05470825\n",
      "| Epoch 187 |   450/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05053651\n",
      "| Epoch 187 |   500/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.05597675\n",
      "| Epoch 187 |   550/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05375432\n",
      "| Epoch 187 |   600/  658 batches | lr 0.00006 | ms/batch 14.41 | loss 0.05635918\n",
      "| Epoch 187 |   650/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05355700\n",
      "\n",
      "Val set: Average loss: 0.05823681\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch 188 |    50/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.05912916\n",
      "| Epoch 188 |   100/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.05631656\n",
      "| Epoch 188 |   150/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05255541\n",
      "| Epoch 188 |   200/  658 batches | lr 0.00006 | ms/batch 16.47 | loss 0.05187177\n",
      "| Epoch 188 |   250/  658 batches | lr 0.00006 | ms/batch 18.21 | loss 0.05581584\n",
      "| Epoch 188 |   300/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.05610529\n",
      "| Epoch 188 |   350/  658 batches | lr 0.00006 | ms/batch 14.58 | loss 0.05667761\n",
      "| Epoch 188 |   400/  658 batches | lr 0.00006 | ms/batch 16.19 | loss 0.05469945\n",
      "| Epoch 188 |   450/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.05041275\n",
      "| Epoch 188 |   500/  658 batches | lr 0.00006 | ms/batch 16.19 | loss 0.05600785\n",
      "| Epoch 188 |   550/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.05368146\n",
      "| Epoch 188 |   600/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05637035\n",
      "| Epoch 188 |   650/  658 batches | lr 0.00006 | ms/batch 14.41 | loss 0.05355096\n",
      "\n",
      "Val set: Average loss: 0.05830189\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch 189 |    50/  658 batches | lr 0.00006 | ms/batch 14.87 | loss 0.05944098\n",
      "| Epoch 189 |   100/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05622475\n",
      "| Epoch 189 |   150/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05270015\n",
      "| Epoch 189 |   200/  658 batches | lr 0.00006 | ms/batch 16.21 | loss 0.05228054\n",
      "| Epoch 189 |   250/  658 batches | lr 0.00006 | ms/batch 16.63 | loss 0.05587639\n",
      "| Epoch 189 |   300/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05619181\n",
      "| Epoch 189 |   350/  658 batches | lr 0.00006 | ms/batch 14.78 | loss 0.05663624\n",
      "| Epoch 189 |   400/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05472175\n",
      "| Epoch 189 |   450/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05037958\n",
      "| Epoch 189 |   500/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.05598008\n",
      "| Epoch 189 |   550/  658 batches | lr 0.00006 | ms/batch 16.41 | loss 0.05373091\n",
      "| Epoch 189 |   600/  658 batches | lr 0.00006 | ms/batch 14.58 | loss 0.05641219\n",
      "| Epoch 189 |   650/  658 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05374372\n",
      "\n",
      "Val set: Average loss: 0.05839655\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch 190 |    50/  658 batches | lr 0.00006 | ms/batch 14.92 | loss 0.05873009\n",
      "| Epoch 190 |   100/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05638316\n",
      "| Epoch 190 |   150/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05265878\n",
      "| Epoch 190 |   200/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.05186837\n",
      "| Epoch 190 |   250/  658 batches | lr 0.00006 | ms/batch 16.17 | loss 0.05573554\n",
      "| Epoch 190 |   300/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05607407\n",
      "| Epoch 190 |   350/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05662628\n",
      "| Epoch 190 |   400/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05465429\n",
      "| Epoch 190 |   450/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05025640\n",
      "| Epoch 190 |   500/  658 batches | lr 0.00006 | ms/batch 16.31 | loss 0.05587251\n",
      "| Epoch 190 |   550/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05333208\n",
      "| Epoch 190 |   600/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05637410\n",
      "| Epoch 190 |   650/  658 batches | lr 0.00006 | ms/batch 14.60 | loss 0.05360925\n",
      "\n",
      "Val set: Average loss: 0.05816465\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch 191 |    50/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.05925576\n",
      "| Epoch 191 |   100/  658 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05613209\n",
      "| Epoch 191 |   150/  658 batches | lr 0.00006 | ms/batch 14.61 | loss 0.05255680\n",
      "| Epoch 191 |   200/  658 batches | lr 0.00006 | ms/batch 16.29 | loss 0.05183966\n",
      "| Epoch 191 |   250/  658 batches | lr 0.00006 | ms/batch 16.20 | loss 0.05560493\n",
      "| Epoch 191 |   300/  658 batches | lr 0.00006 | ms/batch 14.61 | loss 0.05579637\n",
      "| Epoch 191 |   350/  658 batches | lr 0.00006 | ms/batch 14.41 | loss 0.05654537\n",
      "| Epoch 191 |   400/  658 batches | lr 0.00006 | ms/batch 16.21 | loss 0.05472330\n",
      "| Epoch 191 |   450/  658 batches | lr 0.00006 | ms/batch 14.64 | loss 0.05018769\n",
      "| Epoch 191 |   500/  658 batches | lr 0.00006 | ms/batch 16.32 | loss 0.05598161\n",
      "| Epoch 191 |   550/  658 batches | lr 0.00006 | ms/batch 16.23 | loss 0.05340024\n",
      "| Epoch 191 |   600/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05631912\n",
      "| Epoch 191 |   650/  658 batches | lr 0.00006 | ms/batch 14.60 | loss 0.05354763\n",
      "\n",
      "Val set: Average loss: 0.05821252\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch 192 |    50/  658 batches | lr 0.00006 | ms/batch 14.86 | loss 0.05932932\n",
      "| Epoch 192 |   100/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05600143\n",
      "| Epoch 192 |   150/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.05245464\n",
      "| Epoch 192 |   200/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.05171212\n",
      "| Epoch 192 |   250/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05550963\n",
      "| Epoch 192 |   300/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.05607213\n",
      "| Epoch 192 |   350/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05673591\n",
      "| Epoch 192 |   400/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.05466183\n",
      "| Epoch 192 |   450/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.05015257\n",
      "| Epoch 192 |   500/  658 batches | lr 0.00006 | ms/batch 16.34 | loss 0.05599842\n",
      "| Epoch 192 |   550/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05323955\n",
      "| Epoch 192 |   600/  658 batches | lr 0.00006 | ms/batch 14.72 | loss 0.05627291\n",
      "| Epoch 192 |   650/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.05347571\n",
      "\n",
      "Val set: Average loss: 0.05866255\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch 193 |    50/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05833615\n",
      "| Epoch 193 |   100/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05607788\n",
      "| Epoch 193 |   150/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05245978\n",
      "| Epoch 193 |   200/  658 batches | lr 0.00006 | ms/batch 16.24 | loss 0.05166218\n",
      "| Epoch 193 |   250/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05560113\n",
      "| Epoch 193 |   300/  658 batches | lr 0.00006 | ms/batch 14.60 | loss 0.05535634\n",
      "| Epoch 193 |   350/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05662003\n",
      "| Epoch 193 |   400/  658 batches | lr 0.00006 | ms/batch 16.03 | loss 0.05465980\n",
      "| Epoch 193 |   450/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.04995419\n",
      "| Epoch 193 |   500/  658 batches | lr 0.00006 | ms/batch 16.21 | loss 0.05582325\n",
      "| Epoch 193 |   550/  658 batches | lr 0.00006 | ms/batch 16.18 | loss 0.05320138\n",
      "| Epoch 193 |   600/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05610677\n",
      "| Epoch 193 |   650/  658 batches | lr 0.00006 | ms/batch 14.56 | loss 0.05340208\n",
      "\n",
      "Val set: Average loss: 0.05824989\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch 194 |    50/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05875915\n",
      "| Epoch 194 |   100/  658 batches | lr 0.00006 | ms/batch 16.43 | loss 0.05587196\n",
      "| Epoch 194 |   150/  658 batches | lr 0.00006 | ms/batch 14.81 | loss 0.05247341\n",
      "| Epoch 194 |   200/  658 batches | lr 0.00006 | ms/batch 16.80 | loss 0.05230210\n",
      "| Epoch 194 |   250/  658 batches | lr 0.00006 | ms/batch 16.98 | loss 0.05567321\n",
      "| Epoch 194 |   300/  658 batches | lr 0.00006 | ms/batch 15.40 | loss 0.05593242\n",
      "| Epoch 194 |   350/  658 batches | lr 0.00006 | ms/batch 15.28 | loss 0.05645117\n",
      "| Epoch 194 |   400/  658 batches | lr 0.00006 | ms/batch 16.70 | loss 0.05452564\n",
      "| Epoch 194 |   450/  658 batches | lr 0.00006 | ms/batch 15.02 | loss 0.05004254\n",
      "| Epoch 194 |   500/  658 batches | lr 0.00006 | ms/batch 16.81 | loss 0.05587232\n",
      "| Epoch 194 |   550/  658 batches | lr 0.00006 | ms/batch 16.57 | loss 0.05312801\n",
      "| Epoch 194 |   600/  658 batches | lr 0.00006 | ms/batch 14.63 | loss 0.05617133\n",
      "| Epoch 194 |   650/  658 batches | lr 0.00006 | ms/batch 14.38 | loss 0.05358960\n",
      "\n",
      "Val set: Average loss: 0.05849871\n",
      "\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch 195 |    50/  658 batches | lr 0.00006 | ms/batch 14.94 | loss 0.05840991\n",
      "| Epoch 195 |   100/  658 batches | lr 0.00006 | ms/batch 16.02 | loss 0.05649750\n",
      "| Epoch 195 |   150/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05282754\n",
      "| Epoch 195 |   200/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05168285\n",
      "| Epoch 195 |   250/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05552085\n",
      "| Epoch 195 |   300/  658 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05524467\n",
      "| Epoch 195 |   350/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05651840\n",
      "| Epoch 195 |   400/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.05460542\n",
      "| Epoch 195 |   450/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.04961635\n",
      "| Epoch 195 |   500/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05575179\n",
      "| Epoch 195 |   550/  658 batches | lr 0.00006 | ms/batch 16.42 | loss 0.05328704\n",
      "| Epoch 195 |   600/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05622586\n",
      "| Epoch 195 |   650/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05384124\n",
      "\n",
      "Val set: Average loss: 0.05816866\n",
      "\n",
      "EarlyStopping counter: 10 out of 20\n",
      "| Epoch 196 |    50/  658 batches | lr 0.00006 | ms/batch 14.73 | loss 0.05833211\n",
      "| Epoch 196 |   100/  658 batches | lr 0.00006 | ms/batch 15.99 | loss 0.05603376\n",
      "| Epoch 196 |   150/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.05242226\n",
      "| Epoch 196 |   200/  658 batches | lr 0.00006 | ms/batch 16.20 | loss 0.05164767\n",
      "| Epoch 196 |   250/  658 batches | lr 0.00006 | ms/batch 16.05 | loss 0.05535382\n",
      "| Epoch 196 |   300/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05536487\n",
      "| Epoch 196 |   350/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05660261\n",
      "| Epoch 196 |   400/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05433289\n",
      "| Epoch 196 |   450/  658 batches | lr 0.00006 | ms/batch 14.46 | loss 0.04980733\n",
      "| Epoch 196 |   500/  658 batches | lr 0.00006 | ms/batch 16.21 | loss 0.05557966\n",
      "| Epoch 196 |   550/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05291965\n",
      "| Epoch 196 |   600/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05604959\n",
      "| Epoch 196 |   650/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05338310\n",
      "\n",
      "Val set: Average loss: 0.05833950\n",
      "\n",
      "EarlyStopping counter: 11 out of 20\n",
      "| Epoch 197 |    50/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.05813873\n",
      "| Epoch 197 |   100/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.05587012\n",
      "| Epoch 197 |   150/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05237162\n",
      "| Epoch 197 |   200/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.05139033\n",
      "| Epoch 197 |   250/  658 batches | lr 0.00006 | ms/batch 16.83 | loss 0.05528570\n",
      "| Epoch 197 |   300/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05514070\n",
      "| Epoch 197 |   350/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05639186\n",
      "| Epoch 197 |   400/  658 batches | lr 0.00006 | ms/batch 16.19 | loss 0.05445389\n",
      "| Epoch 197 |   450/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.04960615\n",
      "| Epoch 197 |   500/  658 batches | lr 0.00006 | ms/batch 16.01 | loss 0.05569857\n",
      "| Epoch 197 |   550/  658 batches | lr 0.00006 | ms/batch 16.00 | loss 0.05320555\n",
      "| Epoch 197 |   600/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05585174\n",
      "| Epoch 197 |   650/  658 batches | lr 0.00006 | ms/batch 14.38 | loss 0.05338003\n",
      "\n",
      "Val set: Average loss: 0.05848721\n",
      "\n",
      "EarlyStopping counter: 12 out of 20\n",
      "| Epoch 198 |    50/  658 batches | lr 0.00006 | ms/batch 14.83 | loss 0.05781308\n",
      "| Epoch 198 |   100/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.05583886\n",
      "| Epoch 198 |   150/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05253958\n",
      "| Epoch 198 |   200/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05173770\n",
      "| Epoch 198 |   250/  658 batches | lr 0.00006 | ms/batch 16.05 | loss 0.05537563\n",
      "| Epoch 198 |   300/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05576670\n",
      "| Epoch 198 |   350/  658 batches | lr 0.00006 | ms/batch 14.57 | loss 0.05634079\n",
      "| Epoch 198 |   400/  658 batches | lr 0.00006 | ms/batch 16.01 | loss 0.05406034\n",
      "| Epoch 198 |   450/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.04983683\n",
      "| Epoch 198 |   500/  658 batches | lr 0.00006 | ms/batch 16.20 | loss 0.05545652\n",
      "| Epoch 198 |   550/  658 batches | lr 0.00006 | ms/batch 16.15 | loss 0.05295210\n",
      "| Epoch 198 |   600/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05575843\n",
      "| Epoch 198 |   650/  658 batches | lr 0.00006 | ms/batch 14.35 | loss 0.05316442\n",
      "\n",
      "Val set: Average loss: 0.05858403\n",
      "\n",
      "EarlyStopping counter: 13 out of 20\n",
      "| Epoch 199 |    50/  658 batches | lr 0.00006 | ms/batch 14.66 | loss 0.05784710\n",
      "| Epoch 199 |   100/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05577541\n",
      "| Epoch 199 |   150/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05249530\n",
      "| Epoch 199 |   200/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.05158194\n",
      "| Epoch 199 |   250/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.05534449\n",
      "| Epoch 199 |   300/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05534001\n",
      "| Epoch 199 |   350/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05627557\n",
      "| Epoch 199 |   400/  658 batches | lr 0.00006 | ms/batch 16.00 | loss 0.05409678\n",
      "| Epoch 199 |   450/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.04985263\n",
      "| Epoch 199 |   500/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05562821\n",
      "| Epoch 199 |   550/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05280244\n",
      "| Epoch 199 |   600/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05601019\n",
      "| Epoch 199 |   650/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05312579\n",
      "\n",
      "Val set: Average loss: 0.05836467\n",
      "\n",
      "EarlyStopping counter: 14 out of 20\n",
      "| Epoch 200 |    50/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.05804091\n",
      "| Epoch 200 |   100/  658 batches | lr 0.00006 | ms/batch 16.00 | loss 0.05568778\n",
      "| Epoch 200 |   150/  658 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05234872\n",
      "| Epoch 200 |   200/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05155475\n",
      "| Epoch 200 |   250/  658 batches | lr 0.00006 | ms/batch 16.07 | loss 0.05512153\n",
      "| Epoch 200 |   300/  658 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05556996\n",
      "| Epoch 200 |   350/  658 batches | lr 0.00006 | ms/batch 14.36 | loss 0.05637402\n",
      "| Epoch 200 |   400/  658 batches | lr 0.00006 | ms/batch 16.04 | loss 0.05406813\n",
      "| Epoch 200 |   450/  658 batches | lr 0.00006 | ms/batch 14.65 | loss 0.04972463\n",
      "| Epoch 200 |   500/  658 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05555415\n",
      "| Epoch 200 |   550/  658 batches | lr 0.00006 | ms/batch 16.04 | loss 0.05295277\n",
      "| Epoch 200 |   600/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05596106\n",
      "| Epoch 200 |   650/  658 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05326886\n",
      "\n",
      "Val set: Average loss: 0.05855457\n",
      "\n",
      "EarlyStopping counter: 15 out of 20\n",
      "| Epoch 201 |    50/  658 batches | lr 0.00006 | ms/batch 14.79 | loss 0.05783642\n",
      "| Epoch 201 |   100/  658 batches | lr 0.00006 | ms/batch 16.03 | loss 0.05571664\n",
      "| Epoch 201 |   150/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05219175\n",
      "| Epoch 201 |   200/  658 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05142337\n",
      "| Epoch 201 |   250/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05506700\n",
      "| Epoch 201 |   300/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05473323\n",
      "| Epoch 201 |   350/  658 batches | lr 0.00006 | ms/batch 14.41 | loss 0.05634759\n",
      "| Epoch 201 |   400/  658 batches | lr 0.00006 | ms/batch 16.04 | loss 0.05417090\n",
      "| Epoch 201 |   450/  658 batches | lr 0.00006 | ms/batch 14.51 | loss 0.04955306\n",
      "| Epoch 201 |   500/  658 batches | lr 0.00006 | ms/batch 16.35 | loss 0.05536718\n",
      "| Epoch 201 |   550/  658 batches | lr 0.00006 | ms/batch 16.53 | loss 0.05295964\n",
      "| Epoch 201 |   600/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05555363\n",
      "| Epoch 201 |   650/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.05302383\n",
      "\n",
      "Val set: Average loss: 0.05859691\n",
      "\n",
      "EarlyStopping counter: 16 out of 20\n",
      "| Epoch 202 |    50/  658 batches | lr 0.00006 | ms/batch 14.70 | loss 0.05756278\n",
      "| Epoch 202 |   100/  658 batches | lr 0.00006 | ms/batch 16.06 | loss 0.05557232\n",
      "| Epoch 202 |   150/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.05231518\n",
      "| Epoch 202 |   200/  658 batches | lr 0.00006 | ms/batch 16.29 | loss 0.05129624\n",
      "| Epoch 202 |   250/  658 batches | lr 0.00006 | ms/batch 16.02 | loss 0.05503587\n",
      "| Epoch 202 |   300/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05482099\n",
      "| Epoch 202 |   350/  658 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05624679\n",
      "| Epoch 202 |   400/  658 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05411489\n",
      "| Epoch 202 |   450/  658 batches | lr 0.00006 | ms/batch 14.53 | loss 0.04954991\n",
      "| Epoch 202 |   500/  658 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05541232\n",
      "| Epoch 202 |   550/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05282442\n",
      "| Epoch 202 |   600/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05547497\n",
      "| Epoch 202 |   650/  658 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05287461\n",
      "\n",
      "Val set: Average loss: 0.05871092\n",
      "\n",
      "EarlyStopping counter: 17 out of 20\n",
      "| Epoch 203 |    50/  658 batches | lr 0.00006 | ms/batch 14.77 | loss 0.05771830\n",
      "| Epoch 203 |   100/  658 batches | lr 0.00006 | ms/batch 16.06 | loss 0.05554788\n",
      "| Epoch 203 |   150/  658 batches | lr 0.00006 | ms/batch 14.62 | loss 0.05225783\n",
      "| Epoch 203 |   200/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.05141555\n",
      "| Epoch 203 |   250/  658 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05492916\n",
      "| Epoch 203 |   300/  658 batches | lr 0.00006 | ms/batch 14.61 | loss 0.05498882\n",
      "| Epoch 203 |   350/  658 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05616275\n",
      "| Epoch 203 |   400/  658 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05392150\n",
      "| Epoch 203 |   450/  658 batches | lr 0.00006 | ms/batch 14.56 | loss 0.04958991\n",
      "| Epoch 203 |   500/  658 batches | lr 0.00006 | ms/batch 16.16 | loss 0.05550588\n",
      "| Epoch 203 |   550/  658 batches | lr 0.00006 | ms/batch 16.07 | loss 0.05288647\n",
      "| Epoch 203 |   600/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05540575\n",
      "| Epoch 203 |   650/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05302924\n",
      "\n",
      "Val set: Average loss: 0.05892800\n",
      "\n",
      "EarlyStopping counter: 18 out of 20\n",
      "| Epoch 204 |    50/  658 batches | lr 0.00006 | ms/batch 14.71 | loss 0.05752559\n",
      "| Epoch 204 |   100/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.05560719\n",
      "| Epoch 204 |   150/  658 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05203167\n",
      "| Epoch 204 |   200/  658 batches | lr 0.00006 | ms/batch 16.05 | loss 0.05115368\n",
      "| Epoch 204 |   250/  658 batches | lr 0.00006 | ms/batch 16.21 | loss 0.05486908\n",
      "| Epoch 204 |   300/  658 batches | lr 0.00006 | ms/batch 14.50 | loss 0.05467408\n",
      "| Epoch 204 |   350/  658 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05636779\n",
      "| Epoch 204 |   400/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05394653\n",
      "| Epoch 204 |   450/  658 batches | lr 0.00006 | ms/batch 14.49 | loss 0.04951223\n",
      "| Epoch 204 |   500/  658 batches | lr 0.00006 | ms/batch 16.27 | loss 0.05549620\n",
      "| Epoch 204 |   550/  658 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05297578\n",
      "| Epoch 204 |   600/  658 batches | lr 0.00006 | ms/batch 14.59 | loss 0.05542227\n",
      "| Epoch 204 |   650/  658 batches | lr 0.00006 | ms/batch 14.39 | loss 0.05303054\n",
      "\n",
      "Val set: Average loss: 0.05875593\n",
      "\n",
      "EarlyStopping counter: 19 out of 20\n",
      "| Epoch 205 |    50/  658 batches | lr 0.00006 | ms/batch 14.74 | loss 0.05756394\n",
      "| Epoch 205 |   100/  658 batches | lr 0.00006 | ms/batch 16.08 | loss 0.05563871\n",
      "| Epoch 205 |   150/  658 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05201015\n",
      "| Epoch 205 |   200/  658 batches | lr 0.00006 | ms/batch 16.26 | loss 0.05107220\n",
      "| Epoch 205 |   250/  658 batches | lr 0.00006 | ms/batch 16.17 | loss 0.05495215\n",
      "| Epoch 205 |   300/  658 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05470139\n",
      "| Epoch 205 |   350/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.05612706\n",
      "| Epoch 205 |   400/  658 batches | lr 0.00006 | ms/batch 16.09 | loss 0.05379078\n",
      "| Epoch 205 |   450/  658 batches | lr 0.00006 | ms/batch 14.50 | loss 0.04950311\n",
      "| Epoch 205 |   500/  658 batches | lr 0.00006 | ms/batch 16.19 | loss 0.05526251\n",
      "| Epoch 205 |   550/  658 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05281272\n",
      "| Epoch 205 |   600/  658 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05544017\n",
      "| Epoch 205 |   650/  658 batches | lr 0.00006 | ms/batch 14.40 | loss 0.05290019\n",
      "\n",
      "Val set: Average loss: 0.05860401\n",
      "\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Stopping at Epoch: 205\n"
     ]
    }
   ],
   "source": [
    "load = False\n",
    "save_model_path = '../models/final_real_data_model.chkpt'\n",
    "val_err_df_path = '../results/val_final_real_data_model.csv'\n",
    "\n",
    "if not load:\n",
    "  train_losses, val_losses = train(\n",
    "      epochs,\n",
    "      batch_size,\n",
    "      model,\n",
    "      optimizer,\n",
    "      loss_fn,\n",
    "      X_train,\n",
    "      y_train,\n",
    "      X_val,\n",
    "      y_val)\n",
    "  val_err_df = pd.DataFrame({\n",
    "      'Training': train_losses,\n",
    "      'Validation': val_losses})\n",
    "  val_err_df.to_csv(val_err_df_path)\n",
    "  torch.save(model.state_dict(), save_model_path)\n",
    "else:\n",
    "  model = Net(input_size, output_size, hidden_size, num_layers, act_fn)\n",
    "  model.load_state_dict(torch.load(save_model_path, map_location=device))\n",
    "  model = model.to(device)\n",
    "  val_err_df = pd.read_csv(val_err_df_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650894219921,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "KkenWSRYvDpl",
    "outputId": "37746aa5-0b46-4a80-d14d-397868e89361",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_err_df[1:].plot(xlabel='epoch', ylabel='MSE')\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlG5Ky25vDpm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test the model\n",
    "\n",
    "Now that the model has been trained, let's verify the accuracy of the model's prediction on a small sample of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1650894219922,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "TBZ4eHAdvDpn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 30\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_out = model(X_test[0:test_size].to(device))\n",
    "\n",
    "test_out = output_sc.inverse_transform(test_out.cpu().detach().numpy())\n",
    "real_out = output_sc.inverse_transform(y_test[0:test_size].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1650894219926,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "wK1WeGIZvDpo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['strike', 'close', 'hv_21', 'moneyness', 'tau', 'r',\n",
    "       'call', 'put']\n",
    "test_options = pd.DataFrame(input_sc.inverse_transform(X_test[0:test_size].detach().cpu().numpy()), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1650894220542,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "_dvE5LGXvDpq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         strike        close     hv_21  moneyness       tau         r  \\\n0    724.942017  1078.057373  0.601565   0.672724  0.291270  0.005249   \n1    119.779816   186.636154  1.620926   0.643852  0.010965  0.010750   \n2    970.000977   952.522156  0.653796   1.018581  0.120999  0.008331   \n3   2159.910889  2146.113281  0.751449   0.993920  0.002666  0.010322   \n4   3176.765381  2890.473389  0.405984   0.910914  0.099046  0.008408   \n5    102.653839   169.737167  0.838465   0.604154  1.063380  0.010719   \n6    104.794586   197.499741  0.826699   1.882022  0.181236  0.009094   \n7   1559.966797  2518.695557  0.633307   1.614246  0.716814  0.008331   \n8    719.991516   193.073837  0.660636   0.267920  1.639517  0.011406   \n9    249.830109   199.511520  1.524059   0.797834  0.387449  0.008408   \n10   129.948334   195.890305  0.824784   0.663701  0.387449  0.011924   \n11  1119.909668  1057.134888  0.610801   1.059783  2.214584  0.006981   \n12   334.924774   169.737167  0.726566   0.505509  0.530279  0.009071   \n13  3200.313477  2434.200684  0.594725   0.761143  0.543932  0.011406   \n14    64.120392   709.498657  0.834498  11.086348  0.079770  0.010322   \n15   399.682343   156.459396  0.277649   2.554791  0.140275  0.008232   \n16   294.785797   290.041779  0.417067   0.983093  0.871156  0.008331   \n17   174.904022    90.875229  0.547864   0.519945  0.085124  0.008812   \n18   140.116913   149.216965  0.464679   1.066400  0.066116  0.010437   \n19   182.396637   156.459396  0.277649   1.165044  0.005343  0.008232   \n20  3150.006104  2327.978516  0.671394   1.353462  0.714271  0.008530   \n21    38.966648   176.174850  0.742180   0.219800  0.354452  0.009094   \n22  1579.768677  2486.507080  0.643534   0.636033  0.123676  0.008812   \n23   569.871704   876.476685  0.712405   0.650468  0.392870  0.008331   \n24   390.048981   169.737167  0.838465   2.296751  0.409469  0.010719   \n25    95.161224   191.062057  0.837918   2.013749  0.348830  0.009880   \n26  3255.972900  2763.328613  0.360834   1.177976  0.044163  0.008232   \n27  2979.816650  2151.746338  0.751449   1.384739  0.030241  0.010673   \n28   639.981140   172.955978  1.544856   0.270326  0.030241  0.009071   \n29   394.865692   724.486450  0.807613   0.545809  0.684018  0.010750   \n\n        call       put   Prediction         Real  \n0   0.000239  0.999761    17.500103    20.074718  \n1   0.000239  0.999761   -11.966465    -0.095158  \n2   0.000239  0.999761   119.551216   152.008652  \n3   0.999798  0.000202    28.637005    21.010876  \n4   0.999798  0.000202    69.055504    53.350750  \n5   0.000239  0.999761     0.535946    12.840801  \n6   0.999798  0.000202    92.170250    97.350006  \n7   0.999798  0.000202  1399.254883  1360.902954  \n8   0.999798  0.000202   -13.858265     1.266521  \n9   0.999798  0.000202     5.889431     9.436596  \n10  0.000239  0.999761     0.776928     4.500515  \n11  0.000239  0.999761   360.103546   374.026154  \n12  0.999798  0.000202     0.926128     1.777157  \n13  0.999798  0.000202    51.637798    78.456711  \n14  0.999798  0.000202   884.893372   995.292175  \n15  0.000239  0.999761    48.914501    51.223125  \n16  0.999798  0.000202    29.669750    28.244793  \n17  0.999798  0.000202    -6.181278    -0.095158  \n18  0.999798  0.000202    13.568111    11.308910  \n19  0.000239  0.999761    14.167812    23.819332  \n20  0.000239  0.999761   789.339172   707.977905  \n21  0.000239  0.999761    -5.888508     0.075054  \n22  0.000239  0.999761     8.891216     6.032406  \n23  0.000239  0.999761    25.075893    29.010738  \n24  0.000239  0.999761   163.490723   132.030258  \n25  0.999798  0.000202   127.403572   108.243439  \n26  0.000239  0.999761   405.275543   359.983826  \n27  0.000239  0.999761   752.860779   803.976318  \n28  0.999798  0.000202    -4.660969    -0.095158  \n29  0.000239  0.999761    28.317318    31.393673  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>strike</th>\n      <th>close</th>\n      <th>hv_21</th>\n      <th>moneyness</th>\n      <th>tau</th>\n      <th>r</th>\n      <th>call</th>\n      <th>put</th>\n      <th>Prediction</th>\n      <th>Real</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>724.942017</td>\n      <td>1078.057373</td>\n      <td>0.601565</td>\n      <td>0.672724</td>\n      <td>0.291270</td>\n      <td>0.005249</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>17.500103</td>\n      <td>20.074718</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>119.779816</td>\n      <td>186.636154</td>\n      <td>1.620926</td>\n      <td>0.643852</td>\n      <td>0.010965</td>\n      <td>0.010750</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>-11.966465</td>\n      <td>-0.095158</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>970.000977</td>\n      <td>952.522156</td>\n      <td>0.653796</td>\n      <td>1.018581</td>\n      <td>0.120999</td>\n      <td>0.008331</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>119.551216</td>\n      <td>152.008652</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2159.910889</td>\n      <td>2146.113281</td>\n      <td>0.751449</td>\n      <td>0.993920</td>\n      <td>0.002666</td>\n      <td>0.010322</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>28.637005</td>\n      <td>21.010876</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3176.765381</td>\n      <td>2890.473389</td>\n      <td>0.405984</td>\n      <td>0.910914</td>\n      <td>0.099046</td>\n      <td>0.008408</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>69.055504</td>\n      <td>53.350750</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>102.653839</td>\n      <td>169.737167</td>\n      <td>0.838465</td>\n      <td>0.604154</td>\n      <td>1.063380</td>\n      <td>0.010719</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>0.535946</td>\n      <td>12.840801</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>104.794586</td>\n      <td>197.499741</td>\n      <td>0.826699</td>\n      <td>1.882022</td>\n      <td>0.181236</td>\n      <td>0.009094</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>92.170250</td>\n      <td>97.350006</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1559.966797</td>\n      <td>2518.695557</td>\n      <td>0.633307</td>\n      <td>1.614246</td>\n      <td>0.716814</td>\n      <td>0.008331</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>1399.254883</td>\n      <td>1360.902954</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>719.991516</td>\n      <td>193.073837</td>\n      <td>0.660636</td>\n      <td>0.267920</td>\n      <td>1.639517</td>\n      <td>0.011406</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>-13.858265</td>\n      <td>1.266521</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>249.830109</td>\n      <td>199.511520</td>\n      <td>1.524059</td>\n      <td>0.797834</td>\n      <td>0.387449</td>\n      <td>0.008408</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>5.889431</td>\n      <td>9.436596</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>129.948334</td>\n      <td>195.890305</td>\n      <td>0.824784</td>\n      <td>0.663701</td>\n      <td>0.387449</td>\n      <td>0.011924</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>0.776928</td>\n      <td>4.500515</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1119.909668</td>\n      <td>1057.134888</td>\n      <td>0.610801</td>\n      <td>1.059783</td>\n      <td>2.214584</td>\n      <td>0.006981</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>360.103546</td>\n      <td>374.026154</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>334.924774</td>\n      <td>169.737167</td>\n      <td>0.726566</td>\n      <td>0.505509</td>\n      <td>0.530279</td>\n      <td>0.009071</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>0.926128</td>\n      <td>1.777157</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3200.313477</td>\n      <td>2434.200684</td>\n      <td>0.594725</td>\n      <td>0.761143</td>\n      <td>0.543932</td>\n      <td>0.011406</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>51.637798</td>\n      <td>78.456711</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>64.120392</td>\n      <td>709.498657</td>\n      <td>0.834498</td>\n      <td>11.086348</td>\n      <td>0.079770</td>\n      <td>0.010322</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>884.893372</td>\n      <td>995.292175</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>399.682343</td>\n      <td>156.459396</td>\n      <td>0.277649</td>\n      <td>2.554791</td>\n      <td>0.140275</td>\n      <td>0.008232</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>48.914501</td>\n      <td>51.223125</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>294.785797</td>\n      <td>290.041779</td>\n      <td>0.417067</td>\n      <td>0.983093</td>\n      <td>0.871156</td>\n      <td>0.008331</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>29.669750</td>\n      <td>28.244793</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>174.904022</td>\n      <td>90.875229</td>\n      <td>0.547864</td>\n      <td>0.519945</td>\n      <td>0.085124</td>\n      <td>0.008812</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>-6.181278</td>\n      <td>-0.095158</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>140.116913</td>\n      <td>149.216965</td>\n      <td>0.464679</td>\n      <td>1.066400</td>\n      <td>0.066116</td>\n      <td>0.010437</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>13.568111</td>\n      <td>11.308910</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>182.396637</td>\n      <td>156.459396</td>\n      <td>0.277649</td>\n      <td>1.165044</td>\n      <td>0.005343</td>\n      <td>0.008232</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>14.167812</td>\n      <td>23.819332</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>3150.006104</td>\n      <td>2327.978516</td>\n      <td>0.671394</td>\n      <td>1.353462</td>\n      <td>0.714271</td>\n      <td>0.008530</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>789.339172</td>\n      <td>707.977905</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>38.966648</td>\n      <td>176.174850</td>\n      <td>0.742180</td>\n      <td>0.219800</td>\n      <td>0.354452</td>\n      <td>0.009094</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>-5.888508</td>\n      <td>0.075054</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1579.768677</td>\n      <td>2486.507080</td>\n      <td>0.643534</td>\n      <td>0.636033</td>\n      <td>0.123676</td>\n      <td>0.008812</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>8.891216</td>\n      <td>6.032406</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>569.871704</td>\n      <td>876.476685</td>\n      <td>0.712405</td>\n      <td>0.650468</td>\n      <td>0.392870</td>\n      <td>0.008331</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>25.075893</td>\n      <td>29.010738</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>390.048981</td>\n      <td>169.737167</td>\n      <td>0.838465</td>\n      <td>2.296751</td>\n      <td>0.409469</td>\n      <td>0.010719</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>163.490723</td>\n      <td>132.030258</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>95.161224</td>\n      <td>191.062057</td>\n      <td>0.837918</td>\n      <td>2.013749</td>\n      <td>0.348830</td>\n      <td>0.009880</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>127.403572</td>\n      <td>108.243439</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>3255.972900</td>\n      <td>2763.328613</td>\n      <td>0.360834</td>\n      <td>1.177976</td>\n      <td>0.044163</td>\n      <td>0.008232</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>405.275543</td>\n      <td>359.983826</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>2979.816650</td>\n      <td>2151.746338</td>\n      <td>0.751449</td>\n      <td>1.384739</td>\n      <td>0.030241</td>\n      <td>0.010673</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>752.860779</td>\n      <td>803.976318</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>639.981140</td>\n      <td>172.955978</td>\n      <td>1.544856</td>\n      <td>0.270326</td>\n      <td>0.030241</td>\n      <td>0.009071</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>-4.660969</td>\n      <td>-0.095158</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>394.865692</td>\n      <td>724.486450</td>\n      <td>0.807613</td>\n      <td>0.545809</td>\n      <td>0.684018</td>\n      <td>0.010750</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>28.317318</td>\n      <td>31.393673</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_options['Prediction'] = test_out\n",
    "test_options['Real'] = real_out\n",
    "test_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1650894220545,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "TNw5-1jgvDpr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_options['Abs Error'] = np.abs(test_options.Prediction - test_options.Real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1650894220546,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "lAC6AdeVvDpr",
    "outputId": "78bd13cc-c5a0-4746-f402-9e84d8b78696",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         strike        close     hv_21  moneyness       tau         r  \\\n12   334.924774   169.737167  0.726566   0.505509  0.530279  0.009071   \n16   294.785797   290.041779  0.417067   0.983093  0.871156  0.008331   \n18   140.116913   149.216965  0.464679   1.066400  0.066116  0.010437   \n15   399.682343   156.459396  0.277649   2.554791  0.140275  0.008232   \n0    724.942017  1078.057373  0.601565   0.672724  0.291270  0.005249   \n22  1579.768677  2486.507080  0.643534   0.636033  0.123676  0.008812   \n29   394.865692   724.486450  0.807613   0.545809  0.684018  0.010750   \n9    249.830109   199.511520  1.524059   0.797834  0.387449  0.008408   \n10   129.948334   195.890305  0.824784   0.663701  0.387449  0.011924   \n23   569.871704   876.476685  0.712405   0.650468  0.392870  0.008331   \n28   639.981140   172.955978  1.544856   0.270326  0.030241  0.009071   \n6    104.794586   197.499741  0.826699   1.882022  0.181236  0.009094   \n21    38.966648   176.174850  0.742180   0.219800  0.354452  0.009094   \n17   174.904022    90.875229  0.547864   0.519945  0.085124  0.008812   \n3   2159.910889  2146.113281  0.751449   0.993920  0.002666  0.010322   \n19   182.396637   156.459396  0.277649   1.165044  0.005343  0.008232   \n1    119.779816   186.636154  1.620926   0.643852  0.010965  0.010750   \n5    102.653839   169.737167  0.838465   0.604154  1.063380  0.010719   \n11  1119.909668  1057.134888  0.610801   1.059783  2.214584  0.006981   \n8    719.991516   193.073837  0.660636   0.267920  1.639517  0.011406   \n4   3176.765381  2890.473389  0.405984   0.910914  0.099046  0.008408   \n25    95.161224   191.062057  0.837918   2.013749  0.348830  0.009880   \n13  3200.313477  2434.200684  0.594725   0.761143  0.543932  0.011406   \n24   390.048981   169.737167  0.838465   2.296751  0.409469  0.010719   \n2    970.000977   952.522156  0.653796   1.018581  0.120999  0.008331   \n7   1559.966797  2518.695557  0.633307   1.614246  0.716814  0.008331   \n26  3255.972900  2763.328613  0.360834   1.177976  0.044163  0.008232   \n27  2979.816650  2151.746338  0.751449   1.384739  0.030241  0.010673   \n20  3150.006104  2327.978516  0.671394   1.353462  0.714271  0.008530   \n14    64.120392   709.498657  0.834498  11.086348  0.079770  0.010322   \n\n        call       put   Prediction         Real   Abs Error  \n12  0.999798  0.000202     0.926128     1.777157    0.851028  \n16  0.999798  0.000202    29.669750    28.244793    1.424957  \n18  0.999798  0.000202    13.568111    11.308910    2.259201  \n15  0.000239  0.999761    48.914501    51.223125    2.308624  \n0   0.000239  0.999761    17.500103    20.074718    2.574615  \n22  0.000239  0.999761     8.891216     6.032406    2.858810  \n29  0.000239  0.999761    28.317318    31.393673    3.076355  \n9   0.999798  0.000202     5.889431     9.436596    3.547165  \n10  0.000239  0.999761     0.776928     4.500515    3.723587  \n23  0.000239  0.999761    25.075893    29.010738    3.934845  \n28  0.999798  0.000202    -4.660969    -0.095158    4.565811  \n6   0.999798  0.000202    92.170250    97.350006    5.179756  \n21  0.000239  0.999761    -5.888508     0.075054    5.963562  \n17  0.999798  0.000202    -6.181278    -0.095158    6.086121  \n3   0.999798  0.000202    28.637005    21.010876    7.626129  \n19  0.000239  0.999761    14.167812    23.819332    9.651520  \n1   0.000239  0.999761   -11.966465    -0.095158   11.871307  \n5   0.000239  0.999761     0.535946    12.840801   12.304855  \n11  0.000239  0.999761   360.103546   374.026154   13.922607  \n8   0.999798  0.000202   -13.858265     1.266521   15.124786  \n4   0.999798  0.000202    69.055504    53.350750   15.704754  \n25  0.999798  0.000202   127.403572   108.243439   19.160133  \n13  0.999798  0.000202    51.637798    78.456711   26.818913  \n24  0.000239  0.999761   163.490723   132.030258   31.460464  \n2   0.000239  0.999761   119.551216   152.008652   32.457436  \n7   0.999798  0.000202  1399.254883  1360.902954   38.351929  \n26  0.000239  0.999761   405.275543   359.983826   45.291718  \n27  0.000239  0.999761   752.860779   803.976318   51.115540  \n20  0.000239  0.999761   789.339172   707.977905   81.361267  \n14  0.999798  0.000202   884.893372   995.292175  110.398804  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>strike</th>\n      <th>close</th>\n      <th>hv_21</th>\n      <th>moneyness</th>\n      <th>tau</th>\n      <th>r</th>\n      <th>call</th>\n      <th>put</th>\n      <th>Prediction</th>\n      <th>Real</th>\n      <th>Abs Error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>334.924774</td>\n      <td>169.737167</td>\n      <td>0.726566</td>\n      <td>0.505509</td>\n      <td>0.530279</td>\n      <td>0.009071</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>0.926128</td>\n      <td>1.777157</td>\n      <td>0.851028</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>294.785797</td>\n      <td>290.041779</td>\n      <td>0.417067</td>\n      <td>0.983093</td>\n      <td>0.871156</td>\n      <td>0.008331</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>29.669750</td>\n      <td>28.244793</td>\n      <td>1.424957</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>140.116913</td>\n      <td>149.216965</td>\n      <td>0.464679</td>\n      <td>1.066400</td>\n      <td>0.066116</td>\n      <td>0.010437</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>13.568111</td>\n      <td>11.308910</td>\n      <td>2.259201</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>399.682343</td>\n      <td>156.459396</td>\n      <td>0.277649</td>\n      <td>2.554791</td>\n      <td>0.140275</td>\n      <td>0.008232</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>48.914501</td>\n      <td>51.223125</td>\n      <td>2.308624</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>724.942017</td>\n      <td>1078.057373</td>\n      <td>0.601565</td>\n      <td>0.672724</td>\n      <td>0.291270</td>\n      <td>0.005249</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>17.500103</td>\n      <td>20.074718</td>\n      <td>2.574615</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1579.768677</td>\n      <td>2486.507080</td>\n      <td>0.643534</td>\n      <td>0.636033</td>\n      <td>0.123676</td>\n      <td>0.008812</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>8.891216</td>\n      <td>6.032406</td>\n      <td>2.858810</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>394.865692</td>\n      <td>724.486450</td>\n      <td>0.807613</td>\n      <td>0.545809</td>\n      <td>0.684018</td>\n      <td>0.010750</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>28.317318</td>\n      <td>31.393673</td>\n      <td>3.076355</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>249.830109</td>\n      <td>199.511520</td>\n      <td>1.524059</td>\n      <td>0.797834</td>\n      <td>0.387449</td>\n      <td>0.008408</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>5.889431</td>\n      <td>9.436596</td>\n      <td>3.547165</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>129.948334</td>\n      <td>195.890305</td>\n      <td>0.824784</td>\n      <td>0.663701</td>\n      <td>0.387449</td>\n      <td>0.011924</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>0.776928</td>\n      <td>4.500515</td>\n      <td>3.723587</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>569.871704</td>\n      <td>876.476685</td>\n      <td>0.712405</td>\n      <td>0.650468</td>\n      <td>0.392870</td>\n      <td>0.008331</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>25.075893</td>\n      <td>29.010738</td>\n      <td>3.934845</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>639.981140</td>\n      <td>172.955978</td>\n      <td>1.544856</td>\n      <td>0.270326</td>\n      <td>0.030241</td>\n      <td>0.009071</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>-4.660969</td>\n      <td>-0.095158</td>\n      <td>4.565811</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>104.794586</td>\n      <td>197.499741</td>\n      <td>0.826699</td>\n      <td>1.882022</td>\n      <td>0.181236</td>\n      <td>0.009094</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>92.170250</td>\n      <td>97.350006</td>\n      <td>5.179756</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>38.966648</td>\n      <td>176.174850</td>\n      <td>0.742180</td>\n      <td>0.219800</td>\n      <td>0.354452</td>\n      <td>0.009094</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>-5.888508</td>\n      <td>0.075054</td>\n      <td>5.963562</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>174.904022</td>\n      <td>90.875229</td>\n      <td>0.547864</td>\n      <td>0.519945</td>\n      <td>0.085124</td>\n      <td>0.008812</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>-6.181278</td>\n      <td>-0.095158</td>\n      <td>6.086121</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2159.910889</td>\n      <td>2146.113281</td>\n      <td>0.751449</td>\n      <td>0.993920</td>\n      <td>0.002666</td>\n      <td>0.010322</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>28.637005</td>\n      <td>21.010876</td>\n      <td>7.626129</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>182.396637</td>\n      <td>156.459396</td>\n      <td>0.277649</td>\n      <td>1.165044</td>\n      <td>0.005343</td>\n      <td>0.008232</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>14.167812</td>\n      <td>23.819332</td>\n      <td>9.651520</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>119.779816</td>\n      <td>186.636154</td>\n      <td>1.620926</td>\n      <td>0.643852</td>\n      <td>0.010965</td>\n      <td>0.010750</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>-11.966465</td>\n      <td>-0.095158</td>\n      <td>11.871307</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>102.653839</td>\n      <td>169.737167</td>\n      <td>0.838465</td>\n      <td>0.604154</td>\n      <td>1.063380</td>\n      <td>0.010719</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>0.535946</td>\n      <td>12.840801</td>\n      <td>12.304855</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1119.909668</td>\n      <td>1057.134888</td>\n      <td>0.610801</td>\n      <td>1.059783</td>\n      <td>2.214584</td>\n      <td>0.006981</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>360.103546</td>\n      <td>374.026154</td>\n      <td>13.922607</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>719.991516</td>\n      <td>193.073837</td>\n      <td>0.660636</td>\n      <td>0.267920</td>\n      <td>1.639517</td>\n      <td>0.011406</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>-13.858265</td>\n      <td>1.266521</td>\n      <td>15.124786</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3176.765381</td>\n      <td>2890.473389</td>\n      <td>0.405984</td>\n      <td>0.910914</td>\n      <td>0.099046</td>\n      <td>0.008408</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>69.055504</td>\n      <td>53.350750</td>\n      <td>15.704754</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>95.161224</td>\n      <td>191.062057</td>\n      <td>0.837918</td>\n      <td>2.013749</td>\n      <td>0.348830</td>\n      <td>0.009880</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>127.403572</td>\n      <td>108.243439</td>\n      <td>19.160133</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3200.313477</td>\n      <td>2434.200684</td>\n      <td>0.594725</td>\n      <td>0.761143</td>\n      <td>0.543932</td>\n      <td>0.011406</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>51.637798</td>\n      <td>78.456711</td>\n      <td>26.818913</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>390.048981</td>\n      <td>169.737167</td>\n      <td>0.838465</td>\n      <td>2.296751</td>\n      <td>0.409469</td>\n      <td>0.010719</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>163.490723</td>\n      <td>132.030258</td>\n      <td>31.460464</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>970.000977</td>\n      <td>952.522156</td>\n      <td>0.653796</td>\n      <td>1.018581</td>\n      <td>0.120999</td>\n      <td>0.008331</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>119.551216</td>\n      <td>152.008652</td>\n      <td>32.457436</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1559.966797</td>\n      <td>2518.695557</td>\n      <td>0.633307</td>\n      <td>1.614246</td>\n      <td>0.716814</td>\n      <td>0.008331</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>1399.254883</td>\n      <td>1360.902954</td>\n      <td>38.351929</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>3255.972900</td>\n      <td>2763.328613</td>\n      <td>0.360834</td>\n      <td>1.177976</td>\n      <td>0.044163</td>\n      <td>0.008232</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>405.275543</td>\n      <td>359.983826</td>\n      <td>45.291718</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>2979.816650</td>\n      <td>2151.746338</td>\n      <td>0.751449</td>\n      <td>1.384739</td>\n      <td>0.030241</td>\n      <td>0.010673</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>752.860779</td>\n      <td>803.976318</td>\n      <td>51.115540</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>3150.006104</td>\n      <td>2327.978516</td>\n      <td>0.671394</td>\n      <td>1.353462</td>\n      <td>0.714271</td>\n      <td>0.008530</td>\n      <td>0.000239</td>\n      <td>0.999761</td>\n      <td>789.339172</td>\n      <td>707.977905</td>\n      <td>81.361267</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>64.120392</td>\n      <td>709.498657</td>\n      <td>0.834498</td>\n      <td>11.086348</td>\n      <td>0.079770</td>\n      <td>0.010322</td>\n      <td>0.999798</td>\n      <td>0.000202</td>\n      <td>884.893372</td>\n      <td>995.292175</td>\n      <td>110.398804</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_options.sort_values('Abs Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Performance\n",
    "\n",
    "Let's now test the model's accuracy on different sets (training, validation and testing) using different loss metrics."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDJ-hkrSAqVh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MSE (Mean squared error)\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def get_mse(model, X, y, batch_size):\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X, y), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = loss_fn(out, batch_labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1650895213041,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "GG64Xp3vAib-",
    "outputId": "f846ee65-029d-4a76-b006-d65e4ac07a48",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE on the train set is:  0.05613133750606815\n",
      "The MSE on the val set is:  0.05860401061644037\n",
      "The MSE on the test set is:  0.06286119817789779\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The MSE on the train set is: ', get_mse(model, X_train, y_train, batch_size).mean())\n",
    "print('The MSE on the val set is: ', get_mse(model, X_val, y_val, batch_size).mean())\n",
    "print('The MSE on the test set is: ', get_mse(model, X_test, y_test, batch_size).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCLf2KQJAvoE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MAE (Mean absolute error)\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum |y_i - \\hat{y}_i |\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def get_mae(model, X, y, batch_size):\n",
    "    mae_loss = nn.L1Loss()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X, y), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = mae_loss(out, batch_labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1650895213494,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "mZohsEyLAu7S",
    "outputId": "4d6bfba3-5751-478d-d53e-73751702414e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE on the train set is:  0.08577484345363628\n",
      "The MAE on the val set is:  0.08773241896765778\n",
      "The MAE on the test set is:  0.09019139929708228\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The MAE on the train set is: ', get_mae(model, X_train, y_train, batch_size).mean())\n",
    "print('The MAE on the val set is: ', get_mae(model, X_val, y_val, batch_size).mean())\n",
    "print('The MAE on the test set is: ', get_mae(model, X_test, y_test, batch_size).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmsZ4aPaA1SJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### RSME (Root mean squared error)\n",
    "\n",
    "$$\n",
    "RSME = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the train set is:  0.23530011324634922\n",
      "The RMSE on the val set is:  0.24030440824538915\n",
      "The RMSE on the test set is:  0.24927726172479708\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The RMSE on the train set is: ', np.sqrt(get_mse(model, X_train, y_train, batch_size)).mean())\n",
    "print('The RMSE on the val set is: ', np.sqrt(get_mse(model, X_val, y_val, batch_size)).mean())\n",
    "print('The RMSE on the test set is: ', np.sqrt(get_mse(model, X_test, y_test, batch_size)).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8Jxp0L4A5zb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MAPE (Mean absolute percentage error)\n",
    "\n",
    "$$\n",
    "MAPE = \\frac{1}{n} \\sum \\frac{|y_i - \\hat{y}_i|}{y_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def get_mape(model, X, y, batch_size):\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X, y), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = MAPELoss(out, batch_labels.to(device)).detach().cpu().item()\n",
    "            losses.append(loss)\n",
    "\n",
    "    return np.array(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1650895213818,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "fHghkyflA79o",
    "outputId": "37838323-11f0-4eb9-d7d7-50303bf4e9ce",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAPE on the train set is:  0.5376367306048022\n",
      "The MAPE on the val set is:  0.44358209780899877\n",
      "The MAPE on the test set is:  0.6988782046071018\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The MAPE on the train set is: ', get_mape(model, X_train, y_train, batch_size).mean())\n",
    "print('The MAPE on the val set is: ', get_mape(model, X_val, y_val, batch_size).mean())\n",
    "print('The MAPE on the test set is: ', get_mape(model, X_test, y_test, batch_size).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLLLqkX6BEle",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650895213820,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "sf8FmLxxBGcu",
    "outputId": "764313a9-78fc-46ca-dd08-9fb8834e4374",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the R^2 score is:  0.9518547107612501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(X_test[0:batch_size].to(device)).squeeze().cpu().detach().numpy()\n",
    "\n",
    "y_true = y_test[0:batch_size].cpu().squeeze().detach().numpy()\n",
    "\n",
    "r2 = r2_score(y_pred=out, y_true=y_true)\n",
    "\n",
    "print('the R^2 score is: ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1650895214553,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "awvWu7WxBIHB",
    "outputId": "1b3dccd3-90ef-47d5-9895-267fdf93020c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 648x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.scatter(\n",
    "    y=output_sc.inverse_transform(out.reshape(-1, 1)),\n",
    "    x=output_sc.inverse_transform(y_true.squeeze().reshape(-1, 1))\n",
    ")\n",
    "ax.set_xlabel('Actual Value')\n",
    "ax.set_ylabel('Predicted Value')\n",
    "\n",
    "ax.text(20, 80, f'$R^2$ = {np.round(r2, 6)}', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model-Testing-Heston.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}