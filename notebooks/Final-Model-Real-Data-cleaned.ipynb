{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ju-WUaXnY5cr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Final model training on real data\n",
    "\n",
    "## Contents\n",
    "\n",
    "- Load the data\n",
    "- Preprocessing\n",
    "- Model\n",
    "- Training\n",
    "- Testing the model\n",
    "    - Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1650891681415,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "hH3-v8y-AuXg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650891681417,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "QXEzFQ3iAyj_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891681418,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "gsH02HCWA0gC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "options_path = '../data/real_options_cleaned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1650891681421,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "jT4n95T6A4S6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3413,
     "status": "ok",
     "timestamp": 1650891684820,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "7H8lEQPCA5IS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "options_df = pd.read_csv(options_path, index_col=0)\n",
    "options_df = reduce_mem_usage(options_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1650891684821,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "9TKRgvuOA8pC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "options_df = shuffle(options_df, random_state=0)\n",
    "options_df = options_df.reset_index()\n",
    "options_df['r'] = options_df['r'] / 100\n",
    "options_df = options_df.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1650891684822,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "3HXliAsq132p",
    "outputId": "09bc2eb1-d20d-4471-ed50-5fa0eba727fb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             contractSymbol              lastTradeDate  strike   lastPrice  \\\n0       TSLA220506P00945000        2022-04-05 17:43:35   945.0   22.000000   \n1       NFLX220624C00175000        2022-06-01 17:49:08   175.0   22.625000   \n2       TSLA220520P00700000        2022-05-05 19:58:46   700.0    7.648438   \n3       AAPL221021C00300000  2022-03-21 16:59:05+00:00   300.0    0.090027   \n4       NFLX220617C01050000        2022-05-18 19:44:23  1050.0    0.020004   \n...                     ...                        ...     ...         ...   \n539590  NFLX220715P00485000        2022-05-04 19:07:40   485.0  284.500000   \n539591  AMZN220520C02740000        2022-05-05 19:40:36  2740.0    5.500000   \n539592  NVDA230120C00142500        2022-05-04 19:56:26   142.5   73.000000   \n539593  NVDA221021C00260000        2022-06-02 19:39:24   260.0    5.621094   \n539594  AMZN220520P02890000        2022-05-18 13:35:43  2890.0  652.000000   \n\n               bid         ask  volume  openInterest  impliedVolatility  type  \\\n0        20.203125   22.703125    16.0          25.0           0.602051   put   \n1        21.906250   22.906250     1.0          58.0           0.598633  call   \n2         7.000000    7.500000  1621.0        7180.0           0.854004   put   \n3         0.059998    0.119995    50.0         639.0           0.314941  call   \n4         0.000000    0.000000    20.0           0.0           0.500000  call   \n...            ...         ...     ...           ...                ...   ...   \n539590  295.000000  299.500000   100.0           0.0           0.968750   put   \n539591    4.351562    5.351562    50.0          80.0           0.475342  call   \n539592   71.125000   74.250000    36.0        1108.0           0.592773  call   \n539593    0.000000    0.000000    34.0           0.0           0.125000  call   \n539594  742.000000  746.500000     1.0          13.0           1.626953   put   \n\n        expiryDate downloadDate     close       vol  moneyness       tau  \\\n0       2022-05-06   2022-04-05  1091.000  0.590332   0.866211  0.085144   \n1       2022-06-24   2022-06-01   192.875  0.660645   1.102539  0.063171   \n2       2022-05-20   2022-05-05   873.500  0.694824   0.801270  0.041199   \n3       2022-10-21   2022-03-21   165.375  0.330078   0.551270  0.587891   \n4       2022-06-17   2022-06-02   205.125  0.696289   0.195312  0.041199   \n...            ...          ...       ...       ...        ...       ...   \n539590  2022-07-15   2022-05-25   187.875  0.739258   2.582031  0.140137   \n539591  2022-05-20   2022-05-05  2328.000  0.671387   0.849609  0.041199   \n539592  2023-01-20   2022-05-04   203.375  0.664551   1.426758  0.716797   \n539593  2022-10-21   2022-06-02   195.875  0.824707   0.753418  0.387451   \n539594  2022-05-20   2022-05-19  2146.000  0.751465   1.346680  0.002747   \n\n               r  ba_spread  \n0       0.006832   2.500000  \n1       0.011406   1.000000  \n2       0.008530   0.500000  \n3       0.005409   0.059998  \n4       0.011925   0.000000  \n...          ...        ...  \n539590  0.010719   4.500000  \n539591  0.008530   1.000000  \n539592  0.008331   3.125000  \n539593  0.011925   0.000000  \n539594  0.010323   4.500000  \n\n[539595 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contractSymbol</th>\n      <th>lastTradeDate</th>\n      <th>strike</th>\n      <th>lastPrice</th>\n      <th>bid</th>\n      <th>ask</th>\n      <th>volume</th>\n      <th>openInterest</th>\n      <th>impliedVolatility</th>\n      <th>type</th>\n      <th>expiryDate</th>\n      <th>downloadDate</th>\n      <th>close</th>\n      <th>vol</th>\n      <th>moneyness</th>\n      <th>tau</th>\n      <th>r</th>\n      <th>ba_spread</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TSLA220506P00945000</td>\n      <td>2022-04-05 17:43:35</td>\n      <td>945.0</td>\n      <td>22.000000</td>\n      <td>20.203125</td>\n      <td>22.703125</td>\n      <td>16.0</td>\n      <td>25.0</td>\n      <td>0.602051</td>\n      <td>put</td>\n      <td>2022-05-06</td>\n      <td>2022-04-05</td>\n      <td>1091.000</td>\n      <td>0.590332</td>\n      <td>0.866211</td>\n      <td>0.085144</td>\n      <td>0.006832</td>\n      <td>2.500000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NFLX220624C00175000</td>\n      <td>2022-06-01 17:49:08</td>\n      <td>175.0</td>\n      <td>22.625000</td>\n      <td>21.906250</td>\n      <td>22.906250</td>\n      <td>1.0</td>\n      <td>58.0</td>\n      <td>0.598633</td>\n      <td>call</td>\n      <td>2022-06-24</td>\n      <td>2022-06-01</td>\n      <td>192.875</td>\n      <td>0.660645</td>\n      <td>1.102539</td>\n      <td>0.063171</td>\n      <td>0.011406</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TSLA220520P00700000</td>\n      <td>2022-05-05 19:58:46</td>\n      <td>700.0</td>\n      <td>7.648438</td>\n      <td>7.000000</td>\n      <td>7.500000</td>\n      <td>1621.0</td>\n      <td>7180.0</td>\n      <td>0.854004</td>\n      <td>put</td>\n      <td>2022-05-20</td>\n      <td>2022-05-05</td>\n      <td>873.500</td>\n      <td>0.694824</td>\n      <td>0.801270</td>\n      <td>0.041199</td>\n      <td>0.008530</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AAPL221021C00300000</td>\n      <td>2022-03-21 16:59:05+00:00</td>\n      <td>300.0</td>\n      <td>0.090027</td>\n      <td>0.059998</td>\n      <td>0.119995</td>\n      <td>50.0</td>\n      <td>639.0</td>\n      <td>0.314941</td>\n      <td>call</td>\n      <td>2022-10-21</td>\n      <td>2022-03-21</td>\n      <td>165.375</td>\n      <td>0.330078</td>\n      <td>0.551270</td>\n      <td>0.587891</td>\n      <td>0.005409</td>\n      <td>0.059998</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NFLX220617C01050000</td>\n      <td>2022-05-18 19:44:23</td>\n      <td>1050.0</td>\n      <td>0.020004</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>call</td>\n      <td>2022-06-17</td>\n      <td>2022-06-02</td>\n      <td>205.125</td>\n      <td>0.696289</td>\n      <td>0.195312</td>\n      <td>0.041199</td>\n      <td>0.011925</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>539590</th>\n      <td>NFLX220715P00485000</td>\n      <td>2022-05-04 19:07:40</td>\n      <td>485.0</td>\n      <td>284.500000</td>\n      <td>295.000000</td>\n      <td>299.500000</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.968750</td>\n      <td>put</td>\n      <td>2022-07-15</td>\n      <td>2022-05-25</td>\n      <td>187.875</td>\n      <td>0.739258</td>\n      <td>2.582031</td>\n      <td>0.140137</td>\n      <td>0.010719</td>\n      <td>4.500000</td>\n    </tr>\n    <tr>\n      <th>539591</th>\n      <td>AMZN220520C02740000</td>\n      <td>2022-05-05 19:40:36</td>\n      <td>2740.0</td>\n      <td>5.500000</td>\n      <td>4.351562</td>\n      <td>5.351562</td>\n      <td>50.0</td>\n      <td>80.0</td>\n      <td>0.475342</td>\n      <td>call</td>\n      <td>2022-05-20</td>\n      <td>2022-05-05</td>\n      <td>2328.000</td>\n      <td>0.671387</td>\n      <td>0.849609</td>\n      <td>0.041199</td>\n      <td>0.008530</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>539592</th>\n      <td>NVDA230120C00142500</td>\n      <td>2022-05-04 19:56:26</td>\n      <td>142.5</td>\n      <td>73.000000</td>\n      <td>71.125000</td>\n      <td>74.250000</td>\n      <td>36.0</td>\n      <td>1108.0</td>\n      <td>0.592773</td>\n      <td>call</td>\n      <td>2023-01-20</td>\n      <td>2022-05-04</td>\n      <td>203.375</td>\n      <td>0.664551</td>\n      <td>1.426758</td>\n      <td>0.716797</td>\n      <td>0.008331</td>\n      <td>3.125000</td>\n    </tr>\n    <tr>\n      <th>539593</th>\n      <td>NVDA221021C00260000</td>\n      <td>2022-06-02 19:39:24</td>\n      <td>260.0</td>\n      <td>5.621094</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>34.0</td>\n      <td>0.0</td>\n      <td>0.125000</td>\n      <td>call</td>\n      <td>2022-10-21</td>\n      <td>2022-06-02</td>\n      <td>195.875</td>\n      <td>0.824707</td>\n      <td>0.753418</td>\n      <td>0.387451</td>\n      <td>0.011925</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>539594</th>\n      <td>AMZN220520P02890000</td>\n      <td>2022-05-18 13:35:43</td>\n      <td>2890.0</td>\n      <td>652.000000</td>\n      <td>742.000000</td>\n      <td>746.500000</td>\n      <td>1.0</td>\n      <td>13.0</td>\n      <td>1.626953</td>\n      <td>put</td>\n      <td>2022-05-20</td>\n      <td>2022-05-19</td>\n      <td>2146.000</td>\n      <td>0.751465</td>\n      <td>1.346680</td>\n      <td>0.002747</td>\n      <td>0.010323</td>\n      <td>4.500000</td>\n    </tr>\n  </tbody>\n</table>\n<p>539595 rows Ã— 18 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pn28_RUMBAFH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "The preprocessing phase consist in the following steps\n",
    "\n",
    "- remove all the columns in the dataset that are not going to be use as the neural net's input\n",
    "- hot-encode all the dummy variables, which in this case, is the `type` feature\n",
    "- Standardize the input and the output data\n",
    "\n",
    "The training dataset will made of 80% of the total data, while the validation and the test set are going to represent 10% of the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1650891685322,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "32oPe6XUBCTF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop = ['impliedVolatility',\n",
    "                  'bid',\n",
    "                  'ask',\n",
    "                  'volume',\n",
    "                  'openInterest',\n",
    "                  'contractSymbol',\n",
    "                  'lastTradeDate',\n",
    "                    'ba_spread',\n",
    "                  'expiryDate',\n",
    "                  'downloadDate']\n",
    "options_df = options_df.drop(cols_to_drop, axis=1)\n",
    "options_df = pd.get_dummies(options_df, prefix='', prefix_sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1650891686346,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "_Tlc7k7xBDPV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_sc = StandardScaler()\n",
    "output_sc = StandardScaler()\n",
    "input_data = input_sc.fit_transform(options_df.drop(['lastPrice'], axis=1))\n",
    "output_data = output_sc.fit_transform(options_df['lastPrice'].values.reshape(-1, 1))\n",
    "\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "\n",
    "last_train_idx = int(np.round(len(input_data) * train_size))\n",
    "last_val_idx = last_train_idx + int(np.round(len(input_data) * val_size))\n",
    "\n",
    "X_train = input_data[0:last_train_idx]\n",
    "X_val = input_data[last_train_idx:last_val_idx]\n",
    "X_test = input_data[last_val_idx:]\n",
    "\n",
    "y_train = output_data[0:last_train_idx]\n",
    "y_val = output_data[last_train_idx:last_val_idx]\n",
    "y_test = output_data[last_val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1650891686349,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "IwMVtvfABIhR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = Variable(torch.Tensor(X_train))\n",
    "X_val = Variable(torch.Tensor(X_val))\n",
    "X_test = Variable(torch.Tensor(X_test))\n",
    "\n",
    "y_train = Variable(torch.Tensor(y_train))\n",
    "y_val = Variable(torch.Tensor(y_val))\n",
    "y_test = Variable(torch.Tensor(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ra2l5P1nBVCz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model\n",
    "\n",
    "The deep learning model is a fully connected neural network with the following architecture:\n",
    "\n",
    "- 4 hidden layers\n",
    "- 800 neurons for each layer\n",
    "- $LeakyReLU$ used as activation function\n",
    "- a residual connection every two hidden layers (helps to stabilize the gradient)\n",
    "\n",
    "The values of these hyperparameters are being obtained after executing the Bayesian Optimization.\n",
    "\n",
    "The weights of the network are initialized using the Glorot uniform distribution described in *Understanding the difficulty of training deep feedforward neural networks* - Glorot, X. & Bengio, Y. (2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891686350,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "xTLMLFoSBWDy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "device = 'cuda:0' if CUDA else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1650891686352,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "CQ9Gl3bUBWsj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, module):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.module = module\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.module(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891686353,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "YL3SicQPBYq0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, layer_size, act_fn):\n",
    "      super(HiddenLayer, self).__init__()\n",
    "      \n",
    "      if act_fn == 'ReLU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.ReLU())\n",
    "      elif act_fn == 'LeakyReLU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.LeakyReLU())\n",
    "      elif act_fn == 'ELU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.ELU())\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1650891686792,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "lHUFGf9xBawS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, output_size, hidden_size, num_layers, act_fn):\n",
    "    super(Net, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.output_size = output_size\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if act_fn == 'ReLU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ReLU())\n",
    "    elif act_fn == 'LeakyReLU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.LeakyReLU())\n",
    "    elif act_fn == 'ELU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ELU())\n",
    "\n",
    "    self.hidden_layers_list = []\n",
    "\n",
    "    for i in range(num_layers // 2):\n",
    "      self.hidden_layers_list.append(\n",
    "          ResBlock(\n",
    "            nn.Sequential(\n",
    "                HiddenLayer(self.hidden_size, act_fn),\n",
    "                HiddenLayer(self.hidden_size, act_fn)\n",
    "            )\n",
    "        )\n",
    "      )\n",
    "\n",
    "    self.hidden_layers = nn.Sequential(*self.hidden_layers_list)\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        self.initial_layer,\n",
    "        self.hidden_layers,\n",
    "        nn.Linear(self.hidden_size, self.output_size)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650891686793,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "zcq_lQrHBdH8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(m, init_m: str):\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_uniform(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.uniform_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_normal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.normal_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_xuniform(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.xavier_uniform_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_xnormal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.xavier_normal_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  if init_m == 'uniform':\n",
    "    m.apply(init_uniform)\n",
    "  elif init_m == 'normal':\n",
    "    m.apply(init_normal)\n",
    "  elif init_m == 'xaiver uniform':\n",
    "    m.apply(init_xuniform)\n",
    "  elif init_m == 'xavier normal':\n",
    "    m.apply(init_xnormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuCpyycNCKEZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training\n",
    "\n",
    "Regarding the training phase the number of max epochs is set to 2000, but since running 2000 epochs could (as well as take a long time) **overfit** the neural network, an *Early Stopping* technique was used to stop the neural network training if the validation error no longer improves. In particular, if for 20 epochs the validation error does not improve with respect to the minimum obtained then the training is stopped.\n",
    "\n",
    "The loss function used to train the neural net is the Mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650891686794,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "YbCOnCSNCL25",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 8\n",
    "output_size = 1\n",
    "num_layers = 4\n",
    "hidden_size = 800\n",
    "batch_size = 774\n",
    "epochs = 2000\n",
    "lr = 5.973524887918111e-05\n",
    "init_method = 'xaiver uniform'\n",
    "act_fn = 'LeakyReLU'\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers, act_fn)\n",
    "init_weights(model, init_method)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 7351,
     "status": "ok",
     "timestamp": 1650891694138,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "QqZbxrppvDpZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1650891694142,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "Q-9T0GArCMgp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class OptDataset(Dataset):\n",
    "\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X[idx], self.y[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6-hCH2ivDpb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Losses Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1650891694144,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "mVaO8TruHW4M",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def MAPELoss(output, target):\n",
    "  return torch.mean(torch.abs((target - output) / target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1650891694149,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "tVLW5dHhvDpe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, X_val, y_val):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X_val, y_val), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = loss_fn(out, batch_labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    print('\\nVal set: Average loss: {:.8f}\\n'.format(\n",
    "                losses.mean()))\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68eF5_EovDpf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Early Stopping class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650891694150,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "n2So2ffSvDpg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code took form: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, \n",
    "                 patience=10, \n",
    "                 verbose=False, \n",
    "                 delta=0, \n",
    "                 path='../models/final_heston_model.chkpt',\n",
    "                 trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL_xmnKXvDph",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1650891694152,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "DrBBTGKJvDph",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val\n",
    "):\n",
    "\n",
    "  training_losses = []\n",
    "  validation_losses = []\n",
    "\n",
    "  early_stopping = EarlyStopping(patience=20)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "\n",
    "    for batch, batch_labels in DataLoader(OptDataset(X_train, y_train), batch_size=batch_size):\n",
    "      out = model(batch.to(device))\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss = loss_fn(out, batch_labels.to(device))\n",
    "      epoch_losses.append(loss.item())\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if i > 0 and i % 50 == 0:\n",
    "        avg_loss = total_loss / 50\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| Epoch {:3d} | {:5d}/{:5d} batches | lr {:2.5f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.8f}'.format(\n",
    "              epoch, i, len(X_train) // batch_size+1, lr, elapsed * 1000 / 50,\n",
    "              avg_loss))\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "      i += 1\n",
    "\n",
    "    training_losses.append(np.array(epoch_losses).mean())\n",
    "    val_loss = evaluate(model, loss_fn, X_val, y_val)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Stopping at Epoch: {epoch}\")\n",
    "        break\n",
    "\n",
    "  return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2525796,
     "status": "ok",
     "timestamp": 1650894219916,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "hBETWoCfvDpj",
    "outputId": "30acf99f-3a1e-4d87-d1ff-13068ebe4cd4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 |    50/  558 batches | lr 0.00006 | ms/batch 17.30 | loss 0.37621312\n",
      "| Epoch   0 |   100/  558 batches | lr 0.00006 | ms/batch 15.69 | loss 0.12560805\n",
      "| Epoch   0 |   150/  558 batches | lr 0.00006 | ms/batch 14.10 | loss 0.08252220\n",
      "| Epoch   0 |   200/  558 batches | lr 0.00006 | ms/batch 15.80 | loss 0.08641968\n",
      "| Epoch   0 |   250/  558 batches | lr 0.00006 | ms/batch 18.01 | loss 0.07730174\n",
      "| Epoch   0 |   300/  558 batches | lr 0.00006 | ms/batch 15.54 | loss 0.07943854\n",
      "| Epoch   0 |   350/  558 batches | lr 0.00006 | ms/batch 16.82 | loss 0.07612933\n",
      "| Epoch   0 |   400/  558 batches | lr 0.00006 | ms/batch 17.18 | loss 0.07528757\n",
      "| Epoch   0 |   450/  558 batches | lr 0.00006 | ms/batch 15.46 | loss 0.07570924\n",
      "| Epoch   0 |   500/  558 batches | lr 0.00006 | ms/batch 14.83 | loss 0.07461556\n",
      "| Epoch   0 |   550/  558 batches | lr 0.00006 | ms/batch 16.44 | loss 0.06511319\n",
      "\n",
      "Val set: Average loss: 0.07375153\n",
      "\n",
      "| Epoch   1 |    50/  558 batches | lr 0.00006 | ms/batch 15.00 | loss 0.06573239\n",
      "| Epoch   1 |   100/  558 batches | lr 0.00006 | ms/batch 16.15 | loss 0.06574900\n",
      "| Epoch   1 |   150/  558 batches | lr 0.00006 | ms/batch 16.12 | loss 0.06021814\n",
      "| Epoch   1 |   200/  558 batches | lr 0.00006 | ms/batch 14.43 | loss 0.06690474\n",
      "| Epoch   1 |   250/  558 batches | lr 0.00006 | ms/batch 14.40 | loss 0.06228533\n",
      "| Epoch   1 |   300/  558 batches | lr 0.00006 | ms/batch 16.06 | loss 0.06536405\n",
      "| Epoch   1 |   350/  558 batches | lr 0.00006 | ms/batch 14.52 | loss 0.06309499\n",
      "| Epoch   1 |   400/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.06393073\n",
      "| Epoch   1 |   450/  558 batches | lr 0.00006 | ms/batch 16.05 | loss 0.06527617\n",
      "| Epoch   1 |   500/  558 batches | lr 0.00006 | ms/batch 14.45 | loss 0.06586403\n",
      "| Epoch   1 |   550/  558 batches | lr 0.00006 | ms/batch 14.49 | loss 0.06137712\n",
      "\n",
      "Val set: Average loss: 0.06541756\n",
      "\n",
      "| Epoch   2 |    50/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05903768\n",
      "| Epoch   2 |   100/  558 batches | lr 0.00006 | ms/batch 15.97 | loss 0.05858365\n",
      "| Epoch   2 |   150/  558 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05593052\n",
      "| Epoch   2 |   200/  558 batches | lr 0.00006 | ms/batch 16.02 | loss 0.06147590\n",
      "| Epoch   2 |   250/  558 batches | lr 0.00006 | ms/batch 14.39 | loss 0.05793373\n",
      "| Epoch   2 |   300/  558 batches | lr 0.00006 | ms/batch 14.40 | loss 0.06124024\n",
      "| Epoch   2 |   350/  558 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05890987\n",
      "| Epoch   2 |   400/  558 batches | lr 0.00006 | ms/batch 16.11 | loss 0.05969480\n",
      "| Epoch   2 |   450/  558 batches | lr 0.00006 | ms/batch 14.48 | loss 0.06107358\n",
      "| Epoch   2 |   500/  558 batches | lr 0.00006 | ms/batch 16.06 | loss 0.06255891\n",
      "| Epoch   2 |   550/  558 batches | lr 0.00006 | ms/batch 14.36 | loss 0.05931909\n",
      "\n",
      "Val set: Average loss: 0.06148084\n",
      "\n",
      "| Epoch   3 |    50/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.05564493\n",
      "| Epoch   3 |   100/  558 batches | lr 0.00006 | ms/batch 15.98 | loss 0.05521929\n",
      "| Epoch   3 |   150/  558 batches | lr 0.00006 | ms/batch 16.07 | loss 0.05317978\n",
      "| Epoch   3 |   200/  558 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05839251\n",
      "| Epoch   3 |   250/  558 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05599076\n",
      "| Epoch   3 |   300/  558 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05809380\n",
      "| Epoch   3 |   350/  558 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05580056\n",
      "| Epoch   3 |   400/  558 batches | lr 0.00006 | ms/batch 15.96 | loss 0.05678017\n",
      "| Epoch   3 |   450/  558 batches | lr 0.00006 | ms/batch 16.04 | loss 0.05799054\n",
      "| Epoch   3 |   500/  558 batches | lr 0.00006 | ms/batch 14.48 | loss 0.06042481\n",
      "| Epoch   3 |   550/  558 batches | lr 0.00006 | ms/batch 16.03 | loss 0.05724020\n",
      "\n",
      "Val set: Average loss: 0.05896470\n",
      "\n",
      "| Epoch   4 |    50/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.05314828\n",
      "| Epoch   4 |   100/  558 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05237797\n",
      "| Epoch   4 |   150/  558 batches | lr 0.00006 | ms/batch 16.07 | loss 0.05106843\n",
      "| Epoch   4 |   200/  558 batches | lr 0.00006 | ms/batch 16.05 | loss 0.05663191\n",
      "| Epoch   4 |   250/  558 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05398201\n",
      "| Epoch   4 |   300/  558 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05586368\n",
      "| Epoch   4 |   350/  558 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05382686\n",
      "| Epoch   4 |   400/  558 batches | lr 0.00006 | ms/batch 14.44 | loss 0.05450120\n",
      "| Epoch   4 |   450/  558 batches | lr 0.00006 | ms/batch 16.05 | loss 0.05570124\n",
      "| Epoch   4 |   500/  558 batches | lr 0.00006 | ms/batch 16.09 | loss 0.05890414\n",
      "| Epoch   4 |   550/  558 batches | lr 0.00006 | ms/batch 14.41 | loss 0.05581159\n",
      "\n",
      "Val set: Average loss: 0.05713003\n",
      "\n",
      "| Epoch   5 |    50/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.05088950\n",
      "| Epoch   5 |   100/  558 batches | lr 0.00006 | ms/batch 14.35 | loss 0.05019281\n",
      "| Epoch   5 |   150/  558 batches | lr 0.00006 | ms/batch 14.37 | loss 0.04944779\n",
      "| Epoch   5 |   200/  558 batches | lr 0.00006 | ms/batch 16.03 | loss 0.05523668\n",
      "| Epoch   5 |   250/  558 batches | lr 0.00006 | ms/batch 16.03 | loss 0.05221910\n",
      "| Epoch   5 |   300/  558 batches | lr 0.00006 | ms/batch 14.52 | loss 0.05388855\n",
      "| Epoch   5 |   350/  558 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05211215\n",
      "| Epoch   5 |   400/  558 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05276724\n",
      "| Epoch   5 |   450/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.05398834\n",
      "| Epoch   5 |   500/  558 batches | lr 0.00006 | ms/batch 16.08 | loss 0.05747447\n",
      "| Epoch   5 |   550/  558 batches | lr 0.00006 | ms/batch 14.54 | loss 0.05459897\n",
      "\n",
      "Val set: Average loss: 0.05535768\n",
      "\n",
      "| Epoch   6 |    50/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.04918969\n",
      "| Epoch   6 |   100/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.04868609\n",
      "| Epoch   6 |   150/  558 batches | lr 0.00006 | ms/batch 15.99 | loss 0.04826556\n",
      "| Epoch   6 |   200/  558 batches | lr 0.00006 | ms/batch 14.33 | loss 0.05409736\n",
      "| Epoch   6 |   250/  558 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05090228\n",
      "| Epoch   6 |   300/  558 batches | lr 0.00006 | ms/batch 16.06 | loss 0.05250313\n",
      "| Epoch   6 |   350/  558 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05102267\n",
      "| Epoch   6 |   400/  558 batches | lr 0.00006 | ms/batch 14.52 | loss 0.05166267\n",
      "| Epoch   6 |   450/  558 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05283568\n",
      "| Epoch   6 |   500/  558 batches | lr 0.00006 | ms/batch 14.45 | loss 0.05608312\n",
      "| Epoch   6 |   550/  558 batches | lr 0.00006 | ms/batch 14.48 | loss 0.05342166\n",
      "\n",
      "Val set: Average loss: 0.05391103\n",
      "\n",
      "| Epoch   7 |    50/  558 batches | lr 0.00006 | ms/batch 14.86 | loss 0.04801441\n",
      "| Epoch   7 |   100/  558 batches | lr 0.00006 | ms/batch 16.09 | loss 0.04765958\n",
      "| Epoch   7 |   150/  558 batches | lr 0.00006 | ms/batch 16.50 | loss 0.04750360\n",
      "| Epoch   7 |   200/  558 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05314411\n",
      "| Epoch   7 |   250/  558 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05010943\n",
      "| Epoch   7 |   300/  558 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05151529\n",
      "| Epoch   7 |   350/  558 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05025047\n",
      "| Epoch   7 |   400/  558 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05086277\n",
      "| Epoch   7 |   450/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.05208508\n",
      "| Epoch   7 |   500/  558 batches | lr 0.00006 | ms/batch 14.42 | loss 0.05490833\n",
      "| Epoch   7 |   550/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.05240002\n",
      "\n",
      "Val set: Average loss: 0.05295460\n",
      "\n",
      "| Epoch   8 |    50/  558 batches | lr 0.00006 | ms/batch 16.48 | loss 0.04718611\n",
      "| Epoch   8 |   100/  558 batches | lr 0.00006 | ms/batch 14.48 | loss 0.04691545\n",
      "| Epoch   8 |   150/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04698550\n",
      "| Epoch   8 |   200/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.05249771\n",
      "| Epoch   8 |   250/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.04945207\n",
      "| Epoch   8 |   300/  558 batches | lr 0.00006 | ms/batch 14.46 | loss 0.05086427\n",
      "| Epoch   8 |   350/  558 batches | lr 0.00006 | ms/batch 16.16 | loss 0.04970088\n",
      "| Epoch   8 |   400/  558 batches | lr 0.00006 | ms/batch 14.55 | loss 0.05015366\n",
      "| Epoch   8 |   450/  558 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05150586\n",
      "| Epoch   8 |   500/  558 batches | lr 0.00006 | ms/batch 16.10 | loss 0.05399944\n",
      "| Epoch   8 |   550/  558 batches | lr 0.00006 | ms/batch 14.53 | loss 0.05149013\n",
      "\n",
      "Val set: Average loss: 0.05216220\n",
      "\n",
      "| Epoch   9 |    50/  558 batches | lr 0.00006 | ms/batch 14.81 | loss 0.04648229\n",
      "| Epoch   9 |   100/  558 batches | lr 0.00006 | ms/batch 16.04 | loss 0.04635497\n",
      "| Epoch   9 |   150/  558 batches | lr 0.00006 | ms/batch 14.54 | loss 0.04658937\n",
      "| Epoch   9 |   200/  558 batches | lr 0.00006 | ms/batch 16.12 | loss 0.05200897\n",
      "| Epoch   9 |   250/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04890153\n",
      "| Epoch   9 |   300/  558 batches | lr 0.00006 | ms/batch 14.43 | loss 0.05032219\n",
      "| Epoch   9 |   350/  558 batches | lr 0.00006 | ms/batch 14.46 | loss 0.04926144\n",
      "| Epoch   9 |   400/  558 batches | lr 0.00006 | ms/batch 16.11 | loss 0.04961969\n",
      "| Epoch   9 |   450/  558 batches | lr 0.00006 | ms/batch 14.56 | loss 0.05112011\n",
      "| Epoch   9 |   500/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.05325226\n",
      "| Epoch   9 |   550/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.05076799\n",
      "\n",
      "Val set: Average loss: 0.05177062\n",
      "\n",
      "| Epoch  10 |    50/  558 batches | lr 0.00006 | ms/batch 14.76 | loss 0.04594468\n",
      "| Epoch  10 |   100/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.04591718\n",
      "| Epoch  10 |   150/  558 batches | lr 0.00006 | ms/batch 16.15 | loss 0.04622375\n",
      "| Epoch  10 |   200/  558 batches | lr 0.00006 | ms/batch 14.49 | loss 0.05156906\n",
      "| Epoch  10 |   250/  558 batches | lr 0.00006 | ms/batch 16.11 | loss 0.04849182\n",
      "| Epoch  10 |   300/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.04997224\n",
      "| Epoch  10 |   350/  558 batches | lr 0.00006 | ms/batch 14.45 | loss 0.04887416\n",
      "| Epoch  10 |   400/  558 batches | lr 0.00006 | ms/batch 14.54 | loss 0.04913569\n",
      "| Epoch  10 |   450/  558 batches | lr 0.00006 | ms/batch 16.06 | loss 0.05079317\n",
      "| Epoch  10 |   500/  558 batches | lr 0.00006 | ms/batch 14.50 | loss 0.05250182\n",
      "| Epoch  10 |   550/  558 batches | lr 0.00006 | ms/batch 16.07 | loss 0.05012612\n",
      "\n",
      "Val set: Average loss: 0.05194950\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  11 |    50/  558 batches | lr 0.00006 | ms/batch 16.63 | loss 0.04551861\n",
      "| Epoch  11 |   100/  558 batches | lr 0.00006 | ms/batch 16.05 | loss 0.04551675\n",
      "| Epoch  11 |   150/  558 batches | lr 0.00006 | ms/batch 14.43 | loss 0.04593161\n",
      "| Epoch  11 |   200/  558 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05116417\n",
      "| Epoch  11 |   250/  558 batches | lr 0.00006 | ms/batch 16.12 | loss 0.04807528\n",
      "| Epoch  11 |   300/  558 batches | lr 0.00006 | ms/batch 14.53 | loss 0.04955913\n",
      "| Epoch  11 |   350/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04848087\n",
      "| Epoch  11 |   400/  558 batches | lr 0.00006 | ms/batch 16.16 | loss 0.04876038\n",
      "| Epoch  11 |   450/  558 batches | lr 0.00006 | ms/batch 14.51 | loss 0.05054918\n",
      "| Epoch  11 |   500/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.05190714\n",
      "| Epoch  11 |   550/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.04963262\n",
      "\n",
      "Val set: Average loss: 0.05232108\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  12 |    50/  558 batches | lr 0.00006 | ms/batch 14.87 | loss 0.04527266\n",
      "| Epoch  12 |   100/  558 batches | lr 0.00006 | ms/batch 16.14 | loss 0.04525235\n",
      "| Epoch  12 |   150/  558 batches | lr 0.00006 | ms/batch 16.04 | loss 0.04566836\n",
      "| Epoch  12 |   200/  558 batches | lr 0.00006 | ms/batch 14.47 | loss 0.05068942\n",
      "| Epoch  12 |   250/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04767942\n",
      "| Epoch  12 |   300/  558 batches | lr 0.00006 | ms/batch 16.12 | loss 0.04927958\n",
      "| Epoch  12 |   350/  558 batches | lr 0.00006 | ms/batch 14.52 | loss 0.04819277\n",
      "| Epoch  12 |   400/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.04851862\n",
      "| Epoch  12 |   450/  558 batches | lr 0.00006 | ms/batch 16.13 | loss 0.05027777\n",
      "| Epoch  12 |   500/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.05129206\n",
      "| Epoch  12 |   550/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04917594\n",
      "\n",
      "Val set: Average loss: 0.05264184\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  13 |    50/  558 batches | lr 0.00006 | ms/batch 16.41 | loss 0.04516779\n",
      "| Epoch  13 |   100/  558 batches | lr 0.00006 | ms/batch 16.17 | loss 0.04494610\n",
      "| Epoch  13 |   150/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.04544566\n",
      "| Epoch  13 |   200/  558 batches | lr 0.00006 | ms/batch 16.07 | loss 0.05030804\n",
      "| Epoch  13 |   250/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.04736579\n",
      "| Epoch  13 |   300/  558 batches | lr 0.00006 | ms/batch 14.49 | loss 0.04890885\n",
      "| Epoch  13 |   350/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.04792787\n",
      "| Epoch  13 |   400/  558 batches | lr 0.00006 | ms/batch 16.17 | loss 0.04827763\n",
      "| Epoch  13 |   450/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04993311\n",
      "| Epoch  13 |   500/  558 batches | lr 0.00006 | ms/batch 16.00 | loss 0.05077316\n",
      "| Epoch  13 |   550/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.04869575\n",
      "\n",
      "Val set: Average loss: 0.05240605\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  14 |    50/  558 batches | lr 0.00006 | ms/batch 14.75 | loss 0.04501881\n",
      "| Epoch  14 |   100/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.04476524\n",
      "| Epoch  14 |   150/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.04527800\n",
      "| Epoch  14 |   200/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04997523\n",
      "| Epoch  14 |   250/  558 batches | lr 0.00006 | ms/batch 14.48 | loss 0.04704000\n",
      "| Epoch  14 |   300/  558 batches | lr 0.00006 | ms/batch 14.54 | loss 0.04833812\n",
      "| Epoch  14 |   350/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04769831\n",
      "| Epoch  14 |   400/  558 batches | lr 0.00006 | ms/batch 16.13 | loss 0.04803268\n",
      "| Epoch  14 |   450/  558 batches | lr 0.00006 | ms/batch 16.10 | loss 0.04971191\n",
      "| Epoch  14 |   500/  558 batches | lr 0.00006 | ms/batch 16.14 | loss 0.05041545\n",
      "| Epoch  14 |   550/  558 batches | lr 0.00006 | ms/batch 14.45 | loss 0.04840361\n",
      "\n",
      "Val set: Average loss: 0.05203894\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  15 |    50/  558 batches | lr 0.00006 | ms/batch 14.80 | loss 0.04492843\n",
      "| Epoch  15 |   100/  558 batches | lr 0.00006 | ms/batch 14.40 | loss 0.04447604\n",
      "| Epoch  15 |   150/  558 batches | lr 0.00006 | ms/batch 16.10 | loss 0.04507035\n",
      "| Epoch  15 |   200/  558 batches | lr 0.00006 | ms/batch 16.08 | loss 0.04959456\n",
      "| Epoch  15 |   250/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.04676562\n",
      "| Epoch  15 |   300/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04792124\n",
      "| Epoch  15 |   350/  558 batches | lr 0.00006 | ms/batch 14.50 | loss 0.04752675\n",
      "| Epoch  15 |   400/  558 batches | lr 0.00006 | ms/batch 14.53 | loss 0.04779520\n",
      "| Epoch  15 |   450/  558 batches | lr 0.00006 | ms/batch 16.39 | loss 0.04942038\n",
      "| Epoch  15 |   500/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.05010881\n",
      "| Epoch  15 |   550/  558 batches | lr 0.00006 | ms/batch 16.40 | loss 0.04807316\n",
      "\n",
      "Val set: Average loss: 0.05138609\n",
      "\n",
      "| Epoch  16 |    50/  558 batches | lr 0.00006 | ms/batch 16.46 | loss 0.04470686\n",
      "| Epoch  16 |   100/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.04411880\n",
      "| Epoch  16 |   150/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04481878\n",
      "| Epoch  16 |   200/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.04939690\n",
      "| Epoch  16 |   250/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.04645777\n",
      "| Epoch  16 |   300/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.04746285\n",
      "| Epoch  16 |   350/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.04737180\n",
      "| Epoch  16 |   400/  558 batches | lr 0.00006 | ms/batch 14.55 | loss 0.04755704\n",
      "| Epoch  16 |   450/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.04923935\n",
      "| Epoch  16 |   500/  558 batches | lr 0.00006 | ms/batch 16.12 | loss 0.04975308\n",
      "| Epoch  16 |   550/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.04776711\n",
      "\n",
      "Val set: Average loss: 0.05078714\n",
      "\n",
      "| Epoch  17 |    50/  558 batches | lr 0.00006 | ms/batch 16.57 | loss 0.04448040\n",
      "| Epoch  17 |   100/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.04381836\n",
      "| Epoch  17 |   150/  558 batches | lr 0.00006 | ms/batch 14.47 | loss 0.04463583\n",
      "| Epoch  17 |   200/  558 batches | lr 0.00006 | ms/batch 14.54 | loss 0.04915633\n",
      "| Epoch  17 |   250/  558 batches | lr 0.00006 | ms/batch 14.50 | loss 0.04619130\n",
      "| Epoch  17 |   300/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.04711919\n",
      "| Epoch  17 |   350/  558 batches | lr 0.00006 | ms/batch 16.38 | loss 0.04732305\n",
      "| Epoch  17 |   400/  558 batches | lr 0.00006 | ms/batch 16.14 | loss 0.04734353\n",
      "| Epoch  17 |   450/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04902440\n",
      "| Epoch  17 |   500/  558 batches | lr 0.00006 | ms/batch 14.49 | loss 0.04946474\n",
      "| Epoch  17 |   550/  558 batches | lr 0.00006 | ms/batch 14.53 | loss 0.04744534\n",
      "\n",
      "Val set: Average loss: 0.05008512\n",
      "\n",
      "| Epoch  18 |    50/  558 batches | lr 0.00006 | ms/batch 14.94 | loss 0.04416504\n",
      "| Epoch  18 |   100/  558 batches | lr 0.00006 | ms/batch 16.20 | loss 0.04353371\n",
      "| Epoch  18 |   150/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.04440973\n",
      "| Epoch  18 |   200/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.04895326\n",
      "| Epoch  18 |   250/  558 batches | lr 0.00006 | ms/batch 14.45 | loss 0.04593872\n",
      "| Epoch  18 |   300/  558 batches | lr 0.00006 | ms/batch 16.14 | loss 0.04680849\n",
      "| Epoch  18 |   350/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04717397\n",
      "| Epoch  18 |   400/  558 batches | lr 0.00006 | ms/batch 16.17 | loss 0.04714166\n",
      "| Epoch  18 |   450/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.04887472\n",
      "| Epoch  18 |   500/  558 batches | lr 0.00006 | ms/batch 14.47 | loss 0.04920285\n",
      "| Epoch  18 |   550/  558 batches | lr 0.00006 | ms/batch 14.52 | loss 0.04716912\n",
      "\n",
      "Val set: Average loss: 0.04964939\n",
      "\n",
      "| Epoch  19 |    50/  558 batches | lr 0.00006 | ms/batch 16.38 | loss 0.04397958\n",
      "| Epoch  19 |   100/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04322392\n",
      "| Epoch  19 |   150/  558 batches | lr 0.00006 | ms/batch 16.18 | loss 0.04422352\n",
      "| Epoch  19 |   200/  558 batches | lr 0.00006 | ms/batch 16.09 | loss 0.04871169\n",
      "| Epoch  19 |   250/  558 batches | lr 0.00006 | ms/batch 14.55 | loss 0.04574619\n",
      "| Epoch  19 |   300/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04653164\n",
      "| Epoch  19 |   350/  558 batches | lr 0.00006 | ms/batch 16.15 | loss 0.04707119\n",
      "| Epoch  19 |   400/  558 batches | lr 0.00006 | ms/batch 14.50 | loss 0.04697433\n",
      "| Epoch  19 |   450/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04875242\n",
      "| Epoch  19 |   500/  558 batches | lr 0.00006 | ms/batch 16.15 | loss 0.04878159\n",
      "| Epoch  19 |   550/  558 batches | lr 0.00006 | ms/batch 14.46 | loss 0.04694549\n",
      "\n",
      "Val set: Average loss: 0.04934647\n",
      "\n",
      "| Epoch  20 |    50/  558 batches | lr 0.00006 | ms/batch 14.90 | loss 0.04371135\n",
      "| Epoch  20 |   100/  558 batches | lr 0.00006 | ms/batch 16.12 | loss 0.04300964\n",
      "| Epoch  20 |   150/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04408732\n",
      "| Epoch  20 |   200/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.04860843\n",
      "| Epoch  20 |   250/  558 batches | lr 0.00006 | ms/batch 16.18 | loss 0.04557600\n",
      "| Epoch  20 |   300/  558 batches | lr 0.00006 | ms/batch 14.50 | loss 0.04632728\n",
      "| Epoch  20 |   350/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.04694540\n",
      "| Epoch  20 |   400/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04682729\n",
      "| Epoch  20 |   450/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.04861493\n",
      "| Epoch  20 |   500/  558 batches | lr 0.00006 | ms/batch 16.45 | loss 0.04846610\n",
      "| Epoch  20 |   550/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04658930\n",
      "\n",
      "Val set: Average loss: 0.04895813\n",
      "\n",
      "| Epoch  21 |    50/  558 batches | lr 0.00006 | ms/batch 14.86 | loss 0.04349238\n",
      "| Epoch  21 |   100/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.04277390\n",
      "| Epoch  21 |   150/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04382327\n",
      "| Epoch  21 |   200/  558 batches | lr 0.00006 | ms/batch 14.75 | loss 0.04843390\n",
      "| Epoch  21 |   250/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04533557\n",
      "| Epoch  21 |   300/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04597170\n",
      "| Epoch  21 |   350/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04684732\n",
      "| Epoch  21 |   400/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.04662363\n",
      "| Epoch  21 |   450/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.04846936\n",
      "| Epoch  21 |   500/  558 batches | lr 0.00006 | ms/batch 14.53 | loss 0.04821198\n",
      "| Epoch  21 |   550/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.04635722\n",
      "\n",
      "Val set: Average loss: 0.04878143\n",
      "\n",
      "| Epoch  22 |    50/  558 batches | lr 0.00006 | ms/batch 14.98 | loss 0.04330711\n",
      "| Epoch  22 |   100/  558 batches | lr 0.00006 | ms/batch 16.07 | loss 0.04252890\n",
      "| Epoch  22 |   150/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.04367232\n",
      "| Epoch  22 |   200/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.04833421\n",
      "| Epoch  22 |   250/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.04520206\n",
      "| Epoch  22 |   300/  558 batches | lr 0.00006 | ms/batch 16.14 | loss 0.04582847\n",
      "| Epoch  22 |   350/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.04675528\n",
      "| Epoch  22 |   400/  558 batches | lr 0.00006 | ms/batch 16.13 | loss 0.04643017\n",
      "| Epoch  22 |   450/  558 batches | lr 0.00006 | ms/batch 14.56 | loss 0.04827363\n",
      "| Epoch  22 |   500/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.04781381\n",
      "| Epoch  22 |   550/  558 batches | lr 0.00006 | ms/batch 16.17 | loss 0.04612392\n",
      "\n",
      "Val set: Average loss: 0.04850384\n",
      "\n",
      "| Epoch  23 |    50/  558 batches | lr 0.00006 | ms/batch 14.74 | loss 0.04313858\n",
      "| Epoch  23 |   100/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.04232040\n",
      "| Epoch  23 |   150/  558 batches | lr 0.00006 | ms/batch 16.09 | loss 0.04343217\n",
      "| Epoch  23 |   200/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.04821615\n",
      "| Epoch  23 |   250/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04502194\n",
      "| Epoch  23 |   300/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.04561390\n",
      "| Epoch  23 |   350/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.04666979\n",
      "| Epoch  23 |   400/  558 batches | lr 0.00006 | ms/batch 16.20 | loss 0.04628846\n",
      "| Epoch  23 |   450/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04815666\n",
      "| Epoch  23 |   500/  558 batches | lr 0.00006 | ms/batch 14.55 | loss 0.04754685\n",
      "| Epoch  23 |   550/  558 batches | lr 0.00006 | ms/batch 14.49 | loss 0.04591710\n",
      "\n",
      "Val set: Average loss: 0.04831344\n",
      "\n",
      "| Epoch  24 |    50/  558 batches | lr 0.00006 | ms/batch 16.58 | loss 0.04291557\n",
      "| Epoch  24 |   100/  558 batches | lr 0.00006 | ms/batch 16.17 | loss 0.04212463\n",
      "| Epoch  24 |   150/  558 batches | lr 0.00006 | ms/batch 16.20 | loss 0.04328141\n",
      "| Epoch  24 |   200/  558 batches | lr 0.00006 | ms/batch 14.49 | loss 0.04801318\n",
      "| Epoch  24 |   250/  558 batches | lr 0.00006 | ms/batch 14.55 | loss 0.04490430\n",
      "| Epoch  24 |   300/  558 batches | lr 0.00006 | ms/batch 14.53 | loss 0.04534872\n",
      "| Epoch  24 |   350/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.04649345\n",
      "| Epoch  24 |   400/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.04602319\n",
      "| Epoch  24 |   450/  558 batches | lr 0.00006 | ms/batch 16.49 | loss 0.04797219\n",
      "| Epoch  24 |   500/  558 batches | lr 0.00006 | ms/batch 14.76 | loss 0.04731080\n",
      "| Epoch  24 |   550/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04574557\n",
      "\n",
      "Val set: Average loss: 0.04804963\n",
      "\n",
      "| Epoch  25 |    50/  558 batches | lr 0.00006 | ms/batch 16.74 | loss 0.04271214\n",
      "| Epoch  25 |   100/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.04197672\n",
      "| Epoch  25 |   150/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.04308195\n",
      "| Epoch  25 |   200/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.04790686\n",
      "| Epoch  25 |   250/  558 batches | lr 0.00006 | ms/batch 14.56 | loss 0.04466735\n",
      "| Epoch  25 |   300/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.04512036\n",
      "| Epoch  25 |   350/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.04638784\n",
      "| Epoch  25 |   400/  558 batches | lr 0.00006 | ms/batch 16.45 | loss 0.04583114\n",
      "| Epoch  25 |   450/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04787680\n",
      "| Epoch  25 |   500/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.04692475\n",
      "| Epoch  25 |   550/  558 batches | lr 0.00006 | ms/batch 14.49 | loss 0.04560481\n",
      "\n",
      "Val set: Average loss: 0.04788669\n",
      "\n",
      "| Epoch  26 |    50/  558 batches | lr 0.00006 | ms/batch 14.83 | loss 0.04257034\n",
      "| Epoch  26 |   100/  558 batches | lr 0.00006 | ms/batch 14.82 | loss 0.04184085\n",
      "| Epoch  26 |   150/  558 batches | lr 0.00006 | ms/batch 16.18 | loss 0.04295996\n",
      "| Epoch  26 |   200/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.04786387\n",
      "| Epoch  26 |   250/  558 batches | lr 0.00006 | ms/batch 16.38 | loss 0.04460164\n",
      "| Epoch  26 |   300/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.04512076\n",
      "| Epoch  26 |   350/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04628575\n",
      "| Epoch  26 |   400/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.04566735\n",
      "| Epoch  26 |   450/  558 batches | lr 0.00006 | ms/batch 16.16 | loss 0.04768372\n",
      "| Epoch  26 |   500/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.04665335\n",
      "| Epoch  26 |   550/  558 batches | lr 0.00006 | ms/batch 16.38 | loss 0.04541352\n",
      "\n",
      "Val set: Average loss: 0.04775623\n",
      "\n",
      "| Epoch  27 |    50/  558 batches | lr 0.00006 | ms/batch 16.50 | loss 0.04236762\n",
      "| Epoch  27 |   100/  558 batches | lr 0.00006 | ms/batch 14.74 | loss 0.04160115\n",
      "| Epoch  27 |   150/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.04281053\n",
      "| Epoch  27 |   200/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.04763964\n",
      "| Epoch  27 |   250/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.04440672\n",
      "| Epoch  27 |   300/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04484397\n",
      "| Epoch  27 |   350/  558 batches | lr 0.00006 | ms/batch 16.11 | loss 0.04624828\n",
      "| Epoch  27 |   400/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04552242\n",
      "| Epoch  27 |   450/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.04763567\n",
      "| Epoch  27 |   500/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.04640631\n",
      "| Epoch  27 |   550/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.04523552\n",
      "\n",
      "Val set: Average loss: 0.04759001\n",
      "\n",
      "| Epoch  28 |    50/  558 batches | lr 0.00006 | ms/batch 16.51 | loss 0.04220131\n",
      "| Epoch  28 |   100/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.04144489\n",
      "| Epoch  28 |   150/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04253713\n",
      "| Epoch  28 |   200/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04751433\n",
      "| Epoch  28 |   250/  558 batches | lr 0.00006 | ms/batch 14.54 | loss 0.04425173\n",
      "| Epoch  28 |   300/  558 batches | lr 0.00006 | ms/batch 16.20 | loss 0.04472438\n",
      "| Epoch  28 |   350/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04612310\n",
      "| Epoch  28 |   400/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.04535283\n",
      "| Epoch  28 |   450/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.04744288\n",
      "| Epoch  28 |   500/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.04615238\n",
      "| Epoch  28 |   550/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04512876\n",
      "\n",
      "Val set: Average loss: 0.04752614\n",
      "\n",
      "| Epoch  29 |    50/  558 batches | lr 0.00006 | ms/batch 14.93 | loss 0.04205189\n",
      "| Epoch  29 |   100/  558 batches | lr 0.00006 | ms/batch 16.48 | loss 0.04138966\n",
      "| Epoch  29 |   150/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.04238358\n",
      "| Epoch  29 |   200/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.04747505\n",
      "| Epoch  29 |   250/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.04418587\n",
      "| Epoch  29 |   300/  558 batches | lr 0.00006 | ms/batch 16.15 | loss 0.04435727\n",
      "| Epoch  29 |   350/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04598999\n",
      "| Epoch  29 |   400/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.04521171\n",
      "| Epoch  29 |   450/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04727323\n",
      "| Epoch  29 |   500/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.04588599\n",
      "| Epoch  29 |   550/  558 batches | lr 0.00006 | ms/batch 14.53 | loss 0.04498822\n",
      "\n",
      "Val set: Average loss: 0.04742235\n",
      "\n",
      "| Epoch  30 |    50/  558 batches | lr 0.00006 | ms/batch 16.69 | loss 0.04191308\n",
      "| Epoch  30 |   100/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.04119899\n",
      "| Epoch  30 |   150/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.04231917\n",
      "| Epoch  30 |   200/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.04723611\n",
      "| Epoch  30 |   250/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.04401185\n",
      "| Epoch  30 |   300/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.04425049\n",
      "| Epoch  30 |   350/  558 batches | lr 0.00006 | ms/batch 16.10 | loss 0.04598099\n",
      "| Epoch  30 |   400/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.04507373\n",
      "| Epoch  30 |   450/  558 batches | lr 0.00006 | ms/batch 16.38 | loss 0.04705187\n",
      "| Epoch  30 |   500/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.04554435\n",
      "| Epoch  30 |   550/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04483057\n",
      "\n",
      "Val set: Average loss: 0.04727405\n",
      "\n",
      "| Epoch  31 |    50/  558 batches | lr 0.00006 | ms/batch 14.84 | loss 0.04175060\n",
      "| Epoch  31 |   100/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.04115284\n",
      "| Epoch  31 |   150/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.04213568\n",
      "| Epoch  31 |   200/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.04717109\n",
      "| Epoch  31 |   250/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04385164\n",
      "| Epoch  31 |   300/  558 batches | lr 0.00006 | ms/batch 14.52 | loss 0.04392917\n",
      "| Epoch  31 |   350/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.04591776\n",
      "| Epoch  31 |   400/  558 batches | lr 0.00006 | ms/batch 16.18 | loss 0.04492261\n",
      "| Epoch  31 |   450/  558 batches | lr 0.00006 | ms/batch 14.60 | loss 0.04693252\n",
      "| Epoch  31 |   500/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.04531647\n",
      "| Epoch  31 |   550/  558 batches | lr 0.00006 | ms/batch 16.14 | loss 0.04469893\n",
      "\n",
      "Val set: Average loss: 0.04712310\n",
      "\n",
      "| Epoch  32 |    50/  558 batches | lr 0.00006 | ms/batch 14.97 | loss 0.04166219\n",
      "| Epoch  32 |   100/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04100177\n",
      "| Epoch  32 |   150/  558 batches | lr 0.00006 | ms/batch 16.18 | loss 0.04201973\n",
      "| Epoch  32 |   200/  558 batches | lr 0.00006 | ms/batch 14.60 | loss 0.04699128\n",
      "| Epoch  32 |   250/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.04366965\n",
      "| Epoch  32 |   300/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.04376333\n",
      "| Epoch  32 |   350/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04575834\n",
      "| Epoch  32 |   400/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04485440\n",
      "| Epoch  32 |   450/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.04676162\n",
      "| Epoch  32 |   500/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04512669\n",
      "| Epoch  32 |   550/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04459798\n",
      "\n",
      "Val set: Average loss: 0.04706790\n",
      "\n",
      "| Epoch  33 |    50/  558 batches | lr 0.00006 | ms/batch 15.07 | loss 0.04151464\n",
      "| Epoch  33 |   100/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.04091427\n",
      "| Epoch  33 |   150/  558 batches | lr 0.00006 | ms/batch 14.52 | loss 0.04187802\n",
      "| Epoch  33 |   200/  558 batches | lr 0.00006 | ms/batch 14.60 | loss 0.04692106\n",
      "| Epoch  33 |   250/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.04343301\n",
      "| Epoch  33 |   300/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.04350997\n",
      "| Epoch  33 |   350/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.04558971\n",
      "| Epoch  33 |   400/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04461757\n",
      "| Epoch  33 |   450/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.04656502\n",
      "| Epoch  33 |   500/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.04486939\n",
      "| Epoch  33 |   550/  558 batches | lr 0.00006 | ms/batch 16.20 | loss 0.04447695\n",
      "\n",
      "Val set: Average loss: 0.04694546\n",
      "\n",
      "| Epoch  34 |    50/  558 batches | lr 0.00006 | ms/batch 15.07 | loss 0.04137041\n",
      "| Epoch  34 |   100/  558 batches | lr 0.00006 | ms/batch 16.44 | loss 0.04074031\n",
      "| Epoch  34 |   150/  558 batches | lr 0.00006 | ms/batch 16.44 | loss 0.04167472\n",
      "| Epoch  34 |   200/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.04680644\n",
      "| Epoch  34 |   250/  558 batches | lr 0.00006 | ms/batch 14.55 | loss 0.04336044\n",
      "| Epoch  34 |   300/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.04335896\n",
      "| Epoch  34 |   350/  558 batches | lr 0.00006 | ms/batch 14.81 | loss 0.04555130\n",
      "| Epoch  34 |   400/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.04454969\n",
      "| Epoch  34 |   450/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04638095\n",
      "| Epoch  34 |   500/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04458587\n",
      "| Epoch  34 |   550/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04439871\n",
      "\n",
      "Val set: Average loss: 0.04693359\n",
      "\n",
      "| Epoch  35 |    50/  558 batches | lr 0.00006 | ms/batch 16.56 | loss 0.04131640\n",
      "| Epoch  35 |   100/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.04066717\n",
      "| Epoch  35 |   150/  558 batches | lr 0.00006 | ms/batch 16.39 | loss 0.04152888\n",
      "| Epoch  35 |   200/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.04673650\n",
      "| Epoch  35 |   250/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.04323768\n",
      "| Epoch  35 |   300/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04321305\n",
      "| Epoch  35 |   350/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04539019\n",
      "| Epoch  35 |   400/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04429552\n",
      "| Epoch  35 |   450/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04615675\n",
      "| Epoch  35 |   500/  558 batches | lr 0.00006 | ms/batch 14.51 | loss 0.04439719\n",
      "| Epoch  35 |   550/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.04429234\n",
      "\n",
      "Val set: Average loss: 0.04681072\n",
      "\n",
      "| Epoch  36 |    50/  558 batches | lr 0.00006 | ms/batch 16.50 | loss 0.04115681\n",
      "| Epoch  36 |   100/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04052339\n",
      "| Epoch  36 |   150/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04139027\n",
      "| Epoch  36 |   200/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.04665235\n",
      "| Epoch  36 |   250/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.04305709\n",
      "| Epoch  36 |   300/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04303612\n",
      "| Epoch  36 |   350/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04530481\n",
      "| Epoch  36 |   400/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.04419465\n",
      "| Epoch  36 |   450/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04601093\n",
      "| Epoch  36 |   500/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.04414808\n",
      "| Epoch  36 |   550/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04416100\n",
      "\n",
      "Val set: Average loss: 0.04670318\n",
      "\n",
      "| Epoch  37 |    50/  558 batches | lr 0.00006 | ms/batch 14.81 | loss 0.04110025\n",
      "| Epoch  37 |   100/  558 batches | lr 0.00006 | ms/batch 15.44 | loss 0.04035578\n",
      "| Epoch  37 |   150/  558 batches | lr 0.00006 | ms/batch 16.61 | loss 0.04129619\n",
      "| Epoch  37 |   200/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04649516\n",
      "| Epoch  37 |   250/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.04295146\n",
      "| Epoch  37 |   300/  558 batches | lr 0.00006 | ms/batch 14.83 | loss 0.04288820\n",
      "| Epoch  37 |   350/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04518980\n",
      "| Epoch  37 |   400/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.04400918\n",
      "| Epoch  37 |   450/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.04581043\n",
      "| Epoch  37 |   500/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.04401797\n",
      "| Epoch  37 |   550/  558 batches | lr 0.00006 | ms/batch 16.40 | loss 0.04406549\n",
      "\n",
      "Val set: Average loss: 0.04669434\n",
      "\n",
      "| Epoch  38 |    50/  558 batches | lr 0.00006 | ms/batch 16.45 | loss 0.04099149\n",
      "| Epoch  38 |   100/  558 batches | lr 0.00006 | ms/batch 14.56 | loss 0.04025893\n",
      "| Epoch  38 |   150/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.04114805\n",
      "| Epoch  38 |   200/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.04635296\n",
      "| Epoch  38 |   250/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04276557\n",
      "| Epoch  38 |   300/  558 batches | lr 0.00006 | ms/batch 14.82 | loss 0.04276379\n",
      "| Epoch  38 |   350/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04506360\n",
      "| Epoch  38 |   400/  558 batches | lr 0.00006 | ms/batch 14.55 | loss 0.04381123\n",
      "| Epoch  38 |   450/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.04562767\n",
      "| Epoch  38 |   500/  558 batches | lr 0.00006 | ms/batch 16.38 | loss 0.04386807\n",
      "| Epoch  38 |   550/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04398653\n",
      "\n",
      "Val set: Average loss: 0.04655092\n",
      "\n",
      "| Epoch  39 |    50/  558 batches | lr 0.00006 | ms/batch 16.56 | loss 0.04089444\n",
      "| Epoch  39 |   100/  558 batches | lr 0.00006 | ms/batch 16.50 | loss 0.04011292\n",
      "| Epoch  39 |   150/  558 batches | lr 0.00006 | ms/batch 14.52 | loss 0.04109931\n",
      "| Epoch  39 |   200/  558 batches | lr 0.00006 | ms/batch 14.53 | loss 0.04625778\n",
      "| Epoch  39 |   250/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04281030\n",
      "| Epoch  39 |   300/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04267356\n",
      "| Epoch  39 |   350/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.04497518\n",
      "| Epoch  39 |   400/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04375668\n",
      "| Epoch  39 |   450/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04551378\n",
      "| Epoch  39 |   500/  558 batches | lr 0.00006 | ms/batch 14.56 | loss 0.04365427\n",
      "| Epoch  39 |   550/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04386867\n",
      "\n",
      "Val set: Average loss: 0.04652177\n",
      "\n",
      "| Epoch  40 |    50/  558 batches | lr 0.00006 | ms/batch 14.97 | loss 0.04078201\n",
      "| Epoch  40 |   100/  558 batches | lr 0.00006 | ms/batch 16.39 | loss 0.03998800\n",
      "| Epoch  40 |   150/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04092133\n",
      "| Epoch  40 |   200/  558 batches | lr 0.00006 | ms/batch 14.60 | loss 0.04616083\n",
      "| Epoch  40 |   250/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.04262621\n",
      "| Epoch  40 |   300/  558 batches | lr 0.00006 | ms/batch 16.14 | loss 0.04256102\n",
      "| Epoch  40 |   350/  558 batches | lr 0.00006 | ms/batch 14.77 | loss 0.04485426\n",
      "| Epoch  40 |   400/  558 batches | lr 0.00006 | ms/batch 16.38 | loss 0.04366302\n",
      "| Epoch  40 |   450/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.04541273\n",
      "| Epoch  40 |   500/  558 batches | lr 0.00006 | ms/batch 14.54 | loss 0.04346231\n",
      "| Epoch  40 |   550/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04375460\n",
      "\n",
      "Val set: Average loss: 0.04635655\n",
      "\n",
      "| Epoch  41 |    50/  558 batches | lr 0.00006 | ms/batch 16.68 | loss 0.04068335\n",
      "| Epoch  41 |   100/  558 batches | lr 0.00006 | ms/batch 16.17 | loss 0.03983453\n",
      "| Epoch  41 |   150/  558 batches | lr 0.00006 | ms/batch 14.83 | loss 0.04080308\n",
      "| Epoch  41 |   200/  558 batches | lr 0.00006 | ms/batch 16.43 | loss 0.04610209\n",
      "| Epoch  41 |   250/  558 batches | lr 0.00006 | ms/batch 14.56 | loss 0.04256741\n",
      "| Epoch  41 |   300/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.04238778\n",
      "| Epoch  41 |   350/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04476072\n",
      "| Epoch  41 |   400/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.04357313\n",
      "| Epoch  41 |   450/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.04524701\n",
      "| Epoch  41 |   500/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.04335394\n",
      "| Epoch  41 |   550/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04373276\n",
      "\n",
      "Val set: Average loss: 0.04614454\n",
      "\n",
      "| Epoch  42 |    50/  558 batches | lr 0.00006 | ms/batch 14.77 | loss 0.04055593\n",
      "| Epoch  42 |   100/  558 batches | lr 0.00006 | ms/batch 16.17 | loss 0.03974979\n",
      "| Epoch  42 |   150/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.04067597\n",
      "| Epoch  42 |   200/  558 batches | lr 0.00006 | ms/batch 14.80 | loss 0.04602558\n",
      "| Epoch  42 |   250/  558 batches | lr 0.00006 | ms/batch 16.15 | loss 0.04244642\n",
      "| Epoch  42 |   300/  558 batches | lr 0.00006 | ms/batch 14.60 | loss 0.04232436\n",
      "| Epoch  42 |   350/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.04456564\n",
      "| Epoch  42 |   400/  558 batches | lr 0.00006 | ms/batch 16.20 | loss 0.04346286\n",
      "| Epoch  42 |   450/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.04509698\n",
      "| Epoch  42 |   500/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04310746\n",
      "| Epoch  42 |   550/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.04355641\n",
      "\n",
      "Val set: Average loss: 0.04617541\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  43 |    50/  558 batches | lr 0.00006 | ms/batch 14.95 | loss 0.04047659\n",
      "| Epoch  43 |   100/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.03966622\n",
      "| Epoch  43 |   150/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.04054773\n",
      "| Epoch  43 |   200/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.04590211\n",
      "| Epoch  43 |   250/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.04231607\n",
      "| Epoch  43 |   300/  558 batches | lr 0.00006 | ms/batch 16.14 | loss 0.04226027\n",
      "| Epoch  43 |   350/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.04438814\n",
      "| Epoch  43 |   400/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04324774\n",
      "| Epoch  43 |   450/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.04493345\n",
      "| Epoch  43 |   500/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04296115\n",
      "| Epoch  43 |   550/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.04349136\n",
      "\n",
      "Val set: Average loss: 0.04604014\n",
      "\n",
      "| Epoch  44 |    50/  558 batches | lr 0.00006 | ms/batch 16.52 | loss 0.04035421\n",
      "| Epoch  44 |   100/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.03950106\n",
      "| Epoch  44 |   150/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04049566\n",
      "| Epoch  44 |   200/  558 batches | lr 0.00006 | ms/batch 16.17 | loss 0.04576463\n",
      "| Epoch  44 |   250/  558 batches | lr 0.00006 | ms/batch 14.77 | loss 0.04218271\n",
      "| Epoch  44 |   300/  558 batches | lr 0.00006 | ms/batch 16.44 | loss 0.04215192\n",
      "| Epoch  44 |   350/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.04421283\n",
      "| Epoch  44 |   400/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04314911\n",
      "| Epoch  44 |   450/  558 batches | lr 0.00006 | ms/batch 14.56 | loss 0.04475035\n",
      "| Epoch  44 |   500/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.04282856\n",
      "| Epoch  44 |   550/  558 batches | lr 0.00006 | ms/batch 14.86 | loss 0.04339276\n",
      "\n",
      "Val set: Average loss: 0.04593292\n",
      "\n",
      "| Epoch  45 |    50/  558 batches | lr 0.00006 | ms/batch 16.55 | loss 0.04025783\n",
      "| Epoch  45 |   100/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.03940631\n",
      "| Epoch  45 |   150/  558 batches | lr 0.00006 | ms/batch 16.41 | loss 0.04038116\n",
      "| Epoch  45 |   200/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.04566119\n",
      "| Epoch  45 |   250/  558 batches | lr 0.00006 | ms/batch 14.56 | loss 0.04210136\n",
      "| Epoch  45 |   300/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.04209443\n",
      "| Epoch  45 |   350/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.04409335\n",
      "| Epoch  45 |   400/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.04297394\n",
      "| Epoch  45 |   450/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.04456060\n",
      "| Epoch  45 |   500/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.04263757\n",
      "| Epoch  45 |   550/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04329120\n",
      "\n",
      "Val set: Average loss: 0.04570629\n",
      "\n",
      "| Epoch  46 |    50/  558 batches | lr 0.00006 | ms/batch 15.04 | loss 0.04013594\n",
      "| Epoch  46 |   100/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.03931146\n",
      "| Epoch  46 |   150/  558 batches | lr 0.00006 | ms/batch 16.20 | loss 0.04022098\n",
      "| Epoch  46 |   200/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.04560207\n",
      "| Epoch  46 |   250/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04211289\n",
      "| Epoch  46 |   300/  558 batches | lr 0.00006 | ms/batch 16.16 | loss 0.04206569\n",
      "| Epoch  46 |   350/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04386773\n",
      "| Epoch  46 |   400/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.04285880\n",
      "| Epoch  46 |   450/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04443780\n",
      "| Epoch  46 |   500/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.04247601\n",
      "| Epoch  46 |   550/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04317931\n",
      "\n",
      "Val set: Average loss: 0.04571674\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  47 |    50/  558 batches | lr 0.00006 | ms/batch 16.50 | loss 0.04008570\n",
      "| Epoch  47 |   100/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.03919733\n",
      "| Epoch  47 |   150/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04014450\n",
      "| Epoch  47 |   200/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04542414\n",
      "| Epoch  47 |   250/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.04196025\n",
      "| Epoch  47 |   300/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.04205882\n",
      "| Epoch  47 |   350/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04373232\n",
      "| Epoch  47 |   400/  558 batches | lr 0.00006 | ms/batch 14.53 | loss 0.04285008\n",
      "| Epoch  47 |   450/  558 batches | lr 0.00006 | ms/batch 16.41 | loss 0.04432949\n",
      "| Epoch  47 |   500/  558 batches | lr 0.00006 | ms/batch 16.14 | loss 0.04242531\n",
      "| Epoch  47 |   550/  558 batches | lr 0.00006 | ms/batch 14.56 | loss 0.04312980\n",
      "\n",
      "Val set: Average loss: 0.04550113\n",
      "\n",
      "| Epoch  48 |    50/  558 batches | lr 0.00006 | ms/batch 14.99 | loss 0.03998296\n",
      "| Epoch  48 |   100/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.03909019\n",
      "| Epoch  48 |   150/  558 batches | lr 0.00006 | ms/batch 14.76 | loss 0.04002751\n",
      "| Epoch  48 |   200/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04540762\n",
      "| Epoch  48 |   250/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.04198076\n",
      "| Epoch  48 |   300/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04195199\n",
      "| Epoch  48 |   350/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.04356242\n",
      "| Epoch  48 |   400/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04255737\n",
      "| Epoch  48 |   450/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.04417823\n",
      "| Epoch  48 |   500/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.04225536\n",
      "| Epoch  48 |   550/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.04302259\n",
      "\n",
      "Val set: Average loss: 0.04545627\n",
      "\n",
      "| Epoch  49 |    50/  558 batches | lr 0.00006 | ms/batch 14.85 | loss 0.03988170\n",
      "| Epoch  49 |   100/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.03902090\n",
      "| Epoch  49 |   150/  558 batches | lr 0.00006 | ms/batch 16.17 | loss 0.03991601\n",
      "| Epoch  49 |   200/  558 batches | lr 0.00006 | ms/batch 14.88 | loss 0.04529833\n",
      "| Epoch  49 |   250/  558 batches | lr 0.00006 | ms/batch 16.18 | loss 0.04194633\n",
      "| Epoch  49 |   300/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04201153\n",
      "| Epoch  49 |   350/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04345589\n",
      "| Epoch  49 |   400/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04245329\n",
      "| Epoch  49 |   450/  558 batches | lr 0.00006 | ms/batch 14.55 | loss 0.04403807\n",
      "| Epoch  49 |   500/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.04215784\n",
      "| Epoch  49 |   550/  558 batches | lr 0.00006 | ms/batch 16.38 | loss 0.04295908\n",
      "\n",
      "Val set: Average loss: 0.04533045\n",
      "\n",
      "| Epoch  50 |    50/  558 batches | lr 0.00006 | ms/batch 16.64 | loss 0.03978689\n",
      "| Epoch  50 |   100/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.03886494\n",
      "| Epoch  50 |   150/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.03990895\n",
      "| Epoch  50 |   200/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04517141\n",
      "| Epoch  50 |   250/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.04175811\n",
      "| Epoch  50 |   300/  558 batches | lr 0.00006 | ms/batch 14.77 | loss 0.04180563\n",
      "| Epoch  50 |   350/  558 batches | lr 0.00006 | ms/batch 16.46 | loss 0.04334468\n",
      "| Epoch  50 |   400/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04239066\n",
      "| Epoch  50 |   450/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.04394319\n",
      "| Epoch  50 |   500/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.04204935\n",
      "| Epoch  50 |   550/  558 batches | lr 0.00006 | ms/batch 16.17 | loss 0.04282995\n",
      "\n",
      "Val set: Average loss: 0.04530942\n",
      "\n",
      "| Epoch  51 |    50/  558 batches | lr 0.00006 | ms/batch 14.89 | loss 0.03971459\n",
      "| Epoch  51 |   100/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.03891712\n",
      "| Epoch  51 |   150/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.03973905\n",
      "| Epoch  51 |   200/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.04515252\n",
      "| Epoch  51 |   250/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.04175997\n",
      "| Epoch  51 |   300/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.04181503\n",
      "| Epoch  51 |   350/  558 batches | lr 0.00006 | ms/batch 14.81 | loss 0.04318354\n",
      "| Epoch  51 |   400/  558 batches | lr 0.00006 | ms/batch 16.40 | loss 0.04212746\n",
      "| Epoch  51 |   450/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.04379316\n",
      "| Epoch  51 |   500/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.04190622\n",
      "| Epoch  51 |   550/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04279750\n",
      "\n",
      "Val set: Average loss: 0.04511653\n",
      "\n",
      "| Epoch  52 |    50/  558 batches | lr 0.00006 | ms/batch 16.51 | loss 0.03961545\n",
      "| Epoch  52 |   100/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.03879304\n",
      "| Epoch  52 |   150/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.03964379\n",
      "| Epoch  52 |   200/  558 batches | lr 0.00006 | ms/batch 16.41 | loss 0.04500858\n",
      "| Epoch  52 |   250/  558 batches | lr 0.00006 | ms/batch 16.16 | loss 0.04168777\n",
      "| Epoch  52 |   300/  558 batches | lr 0.00006 | ms/batch 14.52 | loss 0.04169915\n",
      "| Epoch  52 |   350/  558 batches | lr 0.00006 | ms/batch 14.76 | loss 0.04313545\n",
      "| Epoch  52 |   400/  558 batches | lr 0.00006 | ms/batch 16.12 | loss 0.04219036\n",
      "| Epoch  52 |   450/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.04374642\n",
      "| Epoch  52 |   500/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04192643\n",
      "| Epoch  52 |   550/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.04272962\n",
      "\n",
      "Val set: Average loss: 0.04504710\n",
      "\n",
      "| Epoch  53 |    50/  558 batches | lr 0.00006 | ms/batch 15.02 | loss 0.03950863\n",
      "| Epoch  53 |   100/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.03873139\n",
      "| Epoch  53 |   150/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03956217\n",
      "| Epoch  53 |   200/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.04495612\n",
      "| Epoch  53 |   250/  558 batches | lr 0.00006 | ms/batch 16.41 | loss 0.04156579\n",
      "| Epoch  53 |   300/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04153670\n",
      "| Epoch  53 |   350/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.04313557\n",
      "| Epoch  53 |   400/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.04210016\n",
      "| Epoch  53 |   450/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.04362656\n",
      "| Epoch  53 |   500/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.04175178\n",
      "| Epoch  53 |   550/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.04265065\n",
      "\n",
      "Val set: Average loss: 0.04492527\n",
      "\n",
      "| Epoch  54 |    50/  558 batches | lr 0.00006 | ms/batch 16.56 | loss 0.03943561\n",
      "| Epoch  54 |   100/  558 batches | lr 0.00006 | ms/batch 16.39 | loss 0.03861264\n",
      "| Epoch  54 |   150/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.03944871\n",
      "| Epoch  54 |   200/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04485745\n",
      "| Epoch  54 |   250/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04157117\n",
      "| Epoch  54 |   300/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.04195255\n",
      "| Epoch  54 |   350/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.04298815\n",
      "| Epoch  54 |   400/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.04202913\n",
      "| Epoch  54 |   450/  558 batches | lr 0.00006 | ms/batch 14.74 | loss 0.04350710\n",
      "| Epoch  54 |   500/  558 batches | lr 0.00006 | ms/batch 14.55 | loss 0.04163178\n",
      "| Epoch  54 |   550/  558 batches | lr 0.00006 | ms/batch 16.43 | loss 0.04254108\n",
      "\n",
      "Val set: Average loss: 0.04481604\n",
      "\n",
      "| Epoch  55 |    50/  558 batches | lr 0.00006 | ms/batch 14.86 | loss 0.03929526\n",
      "| Epoch  55 |   100/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.03854699\n",
      "| Epoch  55 |   150/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.03937968\n",
      "| Epoch  55 |   200/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04477854\n",
      "| Epoch  55 |   250/  558 batches | lr 0.00006 | ms/batch 14.76 | loss 0.04150942\n",
      "| Epoch  55 |   300/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04172691\n",
      "| Epoch  55 |   350/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.04284012\n",
      "| Epoch  55 |   400/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04192593\n",
      "| Epoch  55 |   450/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.04333466\n",
      "| Epoch  55 |   500/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04152230\n",
      "| Epoch  55 |   550/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.04244601\n",
      "\n",
      "Val set: Average loss: 0.04479309\n",
      "\n",
      "| Epoch  56 |    50/  558 batches | lr 0.00006 | ms/batch 16.56 | loss 0.03925753\n",
      "| Epoch  56 |   100/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.03853482\n",
      "| Epoch  56 |   150/  558 batches | lr 0.00006 | ms/batch 14.76 | loss 0.03926885\n",
      "| Epoch  56 |   200/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.04478570\n",
      "| Epoch  56 |   250/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04145636\n",
      "| Epoch  56 |   300/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.04164199\n",
      "| Epoch  56 |   350/  558 batches | lr 0.00006 | ms/batch 16.44 | loss 0.04278626\n",
      "| Epoch  56 |   400/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.04190310\n",
      "| Epoch  56 |   450/  558 batches | lr 0.00006 | ms/batch 14.83 | loss 0.04326838\n",
      "| Epoch  56 |   500/  558 batches | lr 0.00006 | ms/batch 16.39 | loss 0.04142510\n",
      "| Epoch  56 |   550/  558 batches | lr 0.00006 | ms/batch 14.88 | loss 0.04241277\n",
      "\n",
      "Val set: Average loss: 0.04468644\n",
      "\n",
      "| Epoch  57 |    50/  558 batches | lr 0.00006 | ms/batch 14.98 | loss 0.03917380\n",
      "| Epoch  57 |   100/  558 batches | lr 0.00006 | ms/batch 16.43 | loss 0.03843752\n",
      "| Epoch  57 |   150/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03921102\n",
      "| Epoch  57 |   200/  558 batches | lr 0.00006 | ms/batch 14.91 | loss 0.04464059\n",
      "| Epoch  57 |   250/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.04134624\n",
      "| Epoch  57 |   300/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04156591\n",
      "| Epoch  57 |   350/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04266012\n",
      "| Epoch  57 |   400/  558 batches | lr 0.00006 | ms/batch 16.41 | loss 0.04177256\n",
      "| Epoch  57 |   450/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04312298\n",
      "| Epoch  57 |   500/  558 batches | lr 0.00006 | ms/batch 14.78 | loss 0.04136593\n",
      "| Epoch  57 |   550/  558 batches | lr 0.00006 | ms/batch 16.17 | loss 0.04231057\n",
      "\n",
      "Val set: Average loss: 0.04462760\n",
      "\n",
      "| Epoch  58 |    50/  558 batches | lr 0.00006 | ms/batch 14.86 | loss 0.03907488\n",
      "| Epoch  58 |   100/  558 batches | lr 0.00006 | ms/batch 14.55 | loss 0.03834857\n",
      "| Epoch  58 |   150/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.03916107\n",
      "| Epoch  58 |   200/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.04461893\n",
      "| Epoch  58 |   250/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.04132427\n",
      "| Epoch  58 |   300/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.04154564\n",
      "| Epoch  58 |   350/  558 batches | lr 0.00006 | ms/batch 14.47 | loss 0.04250064\n",
      "| Epoch  58 |   400/  558 batches | lr 0.00006 | ms/batch 14.60 | loss 0.04154913\n",
      "| Epoch  58 |   450/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.04298992\n",
      "| Epoch  58 |   500/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.04118828\n",
      "| Epoch  58 |   550/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.04224227\n",
      "\n",
      "Val set: Average loss: 0.04449696\n",
      "\n",
      "| Epoch  59 |    50/  558 batches | lr 0.00006 | ms/batch 16.59 | loss 0.03896488\n",
      "| Epoch  59 |   100/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.03828018\n",
      "| Epoch  59 |   150/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.03903647\n",
      "| Epoch  59 |   200/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.04454003\n",
      "| Epoch  59 |   250/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.04127268\n",
      "| Epoch  59 |   300/  558 batches | lr 0.00006 | ms/batch 14.85 | loss 0.04146476\n",
      "| Epoch  59 |   350/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.04247100\n",
      "| Epoch  59 |   400/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04158659\n",
      "| Epoch  59 |   450/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.04293425\n",
      "| Epoch  59 |   500/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.04120745\n",
      "| Epoch  59 |   550/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.04213953\n",
      "\n",
      "Val set: Average loss: 0.04447800\n",
      "\n",
      "| Epoch  60 |    50/  558 batches | lr 0.00006 | ms/batch 16.69 | loss 0.03891685\n",
      "| Epoch  60 |   100/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.03820085\n",
      "| Epoch  60 |   150/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.03900335\n",
      "| Epoch  60 |   200/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04441009\n",
      "| Epoch  60 |   250/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04114242\n",
      "| Epoch  60 |   300/  558 batches | lr 0.00006 | ms/batch 16.65 | loss 0.04128588\n",
      "| Epoch  60 |   350/  558 batches | lr 0.00006 | ms/batch 17.20 | loss 0.04230654\n",
      "| Epoch  60 |   400/  558 batches | lr 0.00006 | ms/batch 14.80 | loss 0.04142130\n",
      "| Epoch  60 |   450/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.04280930\n",
      "| Epoch  60 |   500/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04104783\n",
      "| Epoch  60 |   550/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04211644\n",
      "\n",
      "Val set: Average loss: 0.04433255\n",
      "\n",
      "| Epoch  61 |    50/  558 batches | lr 0.00006 | ms/batch 15.02 | loss 0.03886006\n",
      "| Epoch  61 |   100/  558 batches | lr 0.00006 | ms/batch 16.51 | loss 0.03815115\n",
      "| Epoch  61 |   150/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03885768\n",
      "| Epoch  61 |   200/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.04438655\n",
      "| Epoch  61 |   250/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04117309\n",
      "| Epoch  61 |   300/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.04132370\n",
      "| Epoch  61 |   350/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.04220581\n",
      "| Epoch  61 |   400/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.04132184\n",
      "| Epoch  61 |   450/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.04275536\n",
      "| Epoch  61 |   500/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04095797\n",
      "| Epoch  61 |   550/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04200337\n",
      "\n",
      "Val set: Average loss: 0.04420668\n",
      "\n",
      "| Epoch  62 |    50/  558 batches | lr 0.00006 | ms/batch 16.63 | loss 0.03876534\n",
      "| Epoch  62 |   100/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.03801963\n",
      "| Epoch  62 |   150/  558 batches | lr 0.00006 | ms/batch 16.48 | loss 0.03880047\n",
      "| Epoch  62 |   200/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.04430277\n",
      "| Epoch  62 |   250/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04099981\n",
      "| Epoch  62 |   300/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.04126921\n",
      "| Epoch  62 |   350/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04214500\n",
      "| Epoch  62 |   400/  558 batches | lr 0.00006 | ms/batch 14.79 | loss 0.04123532\n",
      "| Epoch  62 |   450/  558 batches | lr 0.00006 | ms/batch 16.46 | loss 0.04263656\n",
      "| Epoch  62 |   500/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04086963\n",
      "| Epoch  62 |   550/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.04190247\n",
      "\n",
      "Val set: Average loss: 0.04415652\n",
      "\n",
      "| Epoch  63 |    50/  558 batches | lr 0.00006 | ms/batch 15.01 | loss 0.03866392\n",
      "| Epoch  63 |   100/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03798844\n",
      "| Epoch  63 |   150/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.03869656\n",
      "| Epoch  63 |   200/  558 batches | lr 0.00006 | ms/batch 16.72 | loss 0.04418224\n",
      "| Epoch  63 |   250/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.04097335\n",
      "| Epoch  63 |   300/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.04119801\n",
      "| Epoch  63 |   350/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.04206866\n",
      "| Epoch  63 |   400/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.04107607\n",
      "| Epoch  63 |   450/  558 batches | lr 0.00006 | ms/batch 14.74 | loss 0.04252082\n",
      "| Epoch  63 |   500/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.04089823\n",
      "| Epoch  63 |   550/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.04190583\n",
      "\n",
      "Val set: Average loss: 0.04416920\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  64 |    50/  558 batches | lr 0.00006 | ms/batch 14.89 | loss 0.03859429\n",
      "| Epoch  64 |   100/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.03797420\n",
      "| Epoch  64 |   150/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03865663\n",
      "| Epoch  64 |   200/  558 batches | lr 0.00006 | ms/batch 14.77 | loss 0.04406580\n",
      "| Epoch  64 |   250/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04079888\n",
      "| Epoch  64 |   300/  558 batches | lr 0.00006 | ms/batch 16.49 | loss 0.04116754\n",
      "| Epoch  64 |   350/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04196310\n",
      "| Epoch  64 |   400/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.04112240\n",
      "| Epoch  64 |   450/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.04250577\n",
      "| Epoch  64 |   500/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.04081306\n",
      "| Epoch  64 |   550/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.04181576\n",
      "\n",
      "Val set: Average loss: 0.04409389\n",
      "\n",
      "| Epoch  65 |    50/  558 batches | lr 0.00006 | ms/batch 16.62 | loss 0.03853447\n",
      "| Epoch  65 |   100/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.03791821\n",
      "| Epoch  65 |   150/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.03854735\n",
      "| Epoch  65 |   200/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04402964\n",
      "| Epoch  65 |   250/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.04078812\n",
      "| Epoch  65 |   300/  558 batches | lr 0.00006 | ms/batch 14.75 | loss 0.04112662\n",
      "| Epoch  65 |   350/  558 batches | lr 0.00006 | ms/batch 16.41 | loss 0.04187441\n",
      "| Epoch  65 |   400/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.04100563\n",
      "| Epoch  65 |   450/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.04241632\n",
      "| Epoch  65 |   500/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.04061346\n",
      "| Epoch  65 |   550/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.04171100\n",
      "\n",
      "Val set: Average loss: 0.04387701\n",
      "\n",
      "| Epoch  66 |    50/  558 batches | lr 0.00006 | ms/batch 15.07 | loss 0.03846275\n",
      "| Epoch  66 |   100/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.03794476\n",
      "| Epoch  66 |   150/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.03851809\n",
      "| Epoch  66 |   200/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.04382145\n",
      "| Epoch  66 |   250/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04064305\n",
      "| Epoch  66 |   300/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.04111511\n",
      "| Epoch  66 |   350/  558 batches | lr 0.00006 | ms/batch 14.80 | loss 0.04185780\n",
      "| Epoch  66 |   400/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.04100303\n",
      "| Epoch  66 |   450/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.04235735\n",
      "| Epoch  66 |   500/  558 batches | lr 0.00006 | ms/batch 14.60 | loss 0.04051765\n",
      "| Epoch  66 |   550/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04170429\n",
      "\n",
      "Val set: Average loss: 0.04405664\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  67 |    50/  558 batches | lr 0.00006 | ms/batch 16.55 | loss 0.03839801\n",
      "| Epoch  67 |   100/  558 batches | lr 0.00006 | ms/batch 16.43 | loss 0.03783912\n",
      "| Epoch  67 |   150/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.03845957\n",
      "| Epoch  67 |   200/  558 batches | lr 0.00006 | ms/batch 16.38 | loss 0.04380913\n",
      "| Epoch  67 |   250/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.04060276\n",
      "| Epoch  67 |   300/  558 batches | lr 0.00006 | ms/batch 14.75 | loss 0.04122510\n",
      "| Epoch  67 |   350/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.04193861\n",
      "| Epoch  67 |   400/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04097975\n",
      "| Epoch  67 |   450/  558 batches | lr 0.00006 | ms/batch 14.81 | loss 0.04220072\n",
      "| Epoch  67 |   500/  558 batches | lr 0.00006 | ms/batch 16.18 | loss 0.04051520\n",
      "| Epoch  67 |   550/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.04164017\n",
      "\n",
      "Val set: Average loss: 0.04396890\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  68 |    50/  558 batches | lr 0.00006 | ms/batch 14.95 | loss 0.03833694\n",
      "| Epoch  68 |   100/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.03768533\n",
      "| Epoch  68 |   150/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.03840326\n",
      "| Epoch  68 |   200/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.04366036\n",
      "| Epoch  68 |   250/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.04054784\n",
      "| Epoch  68 |   300/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04109943\n",
      "| Epoch  68 |   350/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04185171\n",
      "| Epoch  68 |   400/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.04100516\n",
      "| Epoch  68 |   450/  558 batches | lr 0.00006 | ms/batch 16.44 | loss 0.04208318\n",
      "| Epoch  68 |   500/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.04043700\n",
      "| Epoch  68 |   550/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.04160004\n",
      "\n",
      "Val set: Average loss: 0.04389707\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  69 |    50/  558 batches | lr 0.00006 | ms/batch 15.09 | loss 0.03828610\n",
      "| Epoch  69 |   100/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.03788581\n",
      "| Epoch  69 |   150/  558 batches | lr 0.00006 | ms/batch 16.40 | loss 0.03832789\n",
      "| Epoch  69 |   200/  558 batches | lr 0.00006 | ms/batch 16.41 | loss 0.04362060\n",
      "| Epoch  69 |   250/  558 batches | lr 0.00006 | ms/batch 16.41 | loss 0.04050848\n",
      "| Epoch  69 |   300/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04103056\n",
      "| Epoch  69 |   350/  558 batches | lr 0.00006 | ms/batch 14.74 | loss 0.04180919\n",
      "| Epoch  69 |   400/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04096673\n",
      "| Epoch  69 |   450/  558 batches | lr 0.00006 | ms/batch 16.54 | loss 0.04206979\n",
      "| Epoch  69 |   500/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.04035931\n",
      "| Epoch  69 |   550/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.04155798\n",
      "\n",
      "Val set: Average loss: 0.04381325\n",
      "\n",
      "| Epoch  70 |    50/  558 batches | lr 0.00006 | ms/batch 16.58 | loss 0.03818530\n",
      "| Epoch  70 |   100/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.03782220\n",
      "| Epoch  70 |   150/  558 batches | lr 0.00006 | ms/batch 14.74 | loss 0.03825149\n",
      "| Epoch  70 |   200/  558 batches | lr 0.00006 | ms/batch 16.48 | loss 0.04347903\n",
      "| Epoch  70 |   250/  558 batches | lr 0.00006 | ms/batch 16.44 | loss 0.04045253\n",
      "| Epoch  70 |   300/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.04097228\n",
      "| Epoch  70 |   350/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04170588\n",
      "| Epoch  70 |   400/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.04071618\n",
      "| Epoch  70 |   450/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.04190840\n",
      "| Epoch  70 |   500/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.04025826\n",
      "| Epoch  70 |   550/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.04149687\n",
      "\n",
      "Val set: Average loss: 0.04408819\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  71 |    50/  558 batches | lr 0.00006 | ms/batch 16.57 | loss 0.03823296\n",
      "| Epoch  71 |   100/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.03780486\n",
      "| Epoch  71 |   150/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.03818513\n",
      "| Epoch  71 |   200/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04342274\n",
      "| Epoch  71 |   250/  558 batches | lr 0.00006 | ms/batch 14.87 | loss 0.04023452\n",
      "| Epoch  71 |   300/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.04079731\n",
      "| Epoch  71 |   350/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04153794\n",
      "| Epoch  71 |   400/  558 batches | lr 0.00006 | ms/batch 16.46 | loss 0.04069075\n",
      "| Epoch  71 |   450/  558 batches | lr 0.00006 | ms/batch 15.53 | loss 0.04185788\n",
      "| Epoch  71 |   500/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04010618\n",
      "| Epoch  71 |   550/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.04142113\n",
      "\n",
      "Val set: Average loss: 0.04396692\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  72 |    50/  558 batches | lr 0.00006 | ms/batch 14.96 | loss 0.03810402\n",
      "| Epoch  72 |   100/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.03761314\n",
      "| Epoch  72 |   150/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.03813859\n",
      "| Epoch  72 |   200/  558 batches | lr 0.00006 | ms/batch 14.46 | loss 0.04342890\n",
      "| Epoch  72 |   250/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04014258\n",
      "| Epoch  72 |   300/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.04035733\n",
      "| Epoch  72 |   350/  558 batches | lr 0.00006 | ms/batch 14.76 | loss 0.04134256\n",
      "| Epoch  72 |   400/  558 batches | lr 0.00006 | ms/batch 16.14 | loss 0.04035851\n",
      "| Epoch  72 |   450/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04169534\n",
      "| Epoch  72 |   500/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.04015106\n",
      "| Epoch  72 |   550/  558 batches | lr 0.00006 | ms/batch 14.52 | loss 0.04142767\n",
      "\n",
      "Val set: Average loss: 0.04405252\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  73 |    50/  558 batches | lr 0.00006 | ms/batch 16.38 | loss 0.03809451\n",
      "| Epoch  73 |   100/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.03757660\n",
      "| Epoch  73 |   150/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.03806003\n",
      "| Epoch  73 |   200/  558 batches | lr 0.00006 | ms/batch 16.18 | loss 0.04333854\n",
      "| Epoch  73 |   250/  558 batches | lr 0.00006 | ms/batch 14.55 | loss 0.04009966\n",
      "| Epoch  73 |   300/  558 batches | lr 0.00006 | ms/batch 14.54 | loss 0.04075616\n",
      "| Epoch  73 |   350/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.04156680\n",
      "| Epoch  73 |   400/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.04042998\n",
      "| Epoch  73 |   450/  558 batches | lr 0.00006 | ms/batch 16.40 | loss 0.04164641\n",
      "| Epoch  73 |   500/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.04004776\n",
      "| Epoch  73 |   550/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.04127261\n",
      "\n",
      "Val set: Average loss: 0.04399189\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  74 |    50/  558 batches | lr 0.00006 | ms/batch 15.01 | loss 0.03798941\n",
      "| Epoch  74 |   100/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.03733062\n",
      "| Epoch  74 |   150/  558 batches | lr 0.00006 | ms/batch 14.60 | loss 0.03803135\n",
      "| Epoch  74 |   200/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.04324227\n",
      "| Epoch  74 |   250/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.04001725\n",
      "| Epoch  74 |   300/  558 batches | lr 0.00006 | ms/batch 14.54 | loss 0.04025203\n",
      "| Epoch  74 |   350/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.04084886\n",
      "| Epoch  74 |   400/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04013240\n",
      "| Epoch  74 |   450/  558 batches | lr 0.00006 | ms/batch 14.77 | loss 0.04142295\n",
      "| Epoch  74 |   500/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.03991898\n",
      "| Epoch  74 |   550/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04126279\n",
      "\n",
      "Val set: Average loss: 0.04407448\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  75 |    50/  558 batches | lr 0.00006 | ms/batch 14.84 | loss 0.03792514\n",
      "| Epoch  75 |   100/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.03726887\n",
      "| Epoch  75 |   150/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.03799144\n",
      "| Epoch  75 |   200/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.04321180\n",
      "| Epoch  75 |   250/  558 batches | lr 0.00006 | ms/batch 16.40 | loss 0.03994622\n",
      "| Epoch  75 |   300/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.04028065\n",
      "| Epoch  75 |   350/  558 batches | lr 0.00006 | ms/batch 14.53 | loss 0.04130905\n",
      "| Epoch  75 |   400/  558 batches | lr 0.00006 | ms/batch 14.55 | loss 0.04026221\n",
      "| Epoch  75 |   450/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.04152601\n",
      "| Epoch  75 |   500/  558 batches | lr 0.00006 | ms/batch 14.81 | loss 0.03986500\n",
      "| Epoch  75 |   550/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.04109130\n",
      "\n",
      "Val set: Average loss: 0.04375824\n",
      "\n",
      "| Epoch  76 |    50/  558 batches | lr 0.00006 | ms/batch 14.97 | loss 0.03786223\n",
      "| Epoch  76 |   100/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.03713898\n",
      "| Epoch  76 |   150/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.03791320\n",
      "| Epoch  76 |   200/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04313894\n",
      "| Epoch  76 |   250/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.03987235\n",
      "| Epoch  76 |   300/  558 batches | lr 0.00006 | ms/batch 16.39 | loss 0.04037539\n",
      "| Epoch  76 |   350/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.04122221\n",
      "| Epoch  76 |   400/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.04020326\n",
      "| Epoch  76 |   450/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04146291\n",
      "| Epoch  76 |   500/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.03974730\n",
      "| Epoch  76 |   550/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.04108392\n",
      "\n",
      "Val set: Average loss: 0.04392640\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  77 |    50/  558 batches | lr 0.00006 | ms/batch 14.97 | loss 0.03780873\n",
      "| Epoch  77 |   100/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.03704785\n",
      "| Epoch  77 |   150/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.03782037\n",
      "| Epoch  77 |   200/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04301117\n",
      "| Epoch  77 |   250/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.03980230\n",
      "| Epoch  77 |   300/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.04063413\n",
      "| Epoch  77 |   350/  558 batches | lr 0.00006 | ms/batch 14.81 | loss 0.04154288\n",
      "| Epoch  77 |   400/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.04009780\n",
      "| Epoch  77 |   450/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04124865\n",
      "| Epoch  77 |   500/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.03974746\n",
      "| Epoch  77 |   550/  558 batches | lr 0.00006 | ms/batch 14.75 | loss 0.04102903\n",
      "\n",
      "Val set: Average loss: 0.04385635\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  78 |    50/  558 batches | lr 0.00006 | ms/batch 16.62 | loss 0.03782556\n",
      "| Epoch  78 |   100/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.03710361\n",
      "| Epoch  78 |   150/  558 batches | lr 0.00006 | ms/batch 16.40 | loss 0.03778254\n",
      "| Epoch  78 |   200/  558 batches | lr 0.00006 | ms/batch 14.54 | loss 0.04294637\n",
      "| Epoch  78 |   250/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.03966422\n",
      "| Epoch  78 |   300/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04014476\n",
      "| Epoch  78 |   350/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.04091108\n",
      "| Epoch  78 |   400/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.03990636\n",
      "| Epoch  78 |   450/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.04113898\n",
      "| Epoch  78 |   500/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.03968185\n",
      "| Epoch  78 |   550/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04106548\n",
      "\n",
      "Val set: Average loss: 0.04392727\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  79 |    50/  558 batches | lr 0.00006 | ms/batch 16.40 | loss 0.03773060\n",
      "| Epoch  79 |   100/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.03693493\n",
      "| Epoch  79 |   150/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.03773071\n",
      "| Epoch  79 |   200/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04302612\n",
      "| Epoch  79 |   250/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.03964067\n",
      "| Epoch  79 |   300/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.03956308\n",
      "| Epoch  79 |   350/  558 batches | lr 0.00006 | ms/batch 14.52 | loss 0.04073197\n",
      "| Epoch  79 |   400/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.03970745\n",
      "| Epoch  79 |   450/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.04109341\n",
      "| Epoch  79 |   500/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.03967512\n",
      "| Epoch  79 |   550/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04091561\n",
      "\n",
      "Val set: Average loss: 0.04383923\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  80 |    50/  558 batches | lr 0.00006 | ms/batch 14.93 | loss 0.03755699\n",
      "| Epoch  80 |   100/  558 batches | lr 0.00006 | ms/batch 14.54 | loss 0.03676741\n",
      "| Epoch  80 |   150/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.03807036\n",
      "| Epoch  80 |   200/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.04276198\n",
      "| Epoch  80 |   250/  558 batches | lr 0.00006 | ms/batch 16.45 | loss 0.03957597\n",
      "| Epoch  80 |   300/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.04072423\n",
      "| Epoch  80 |   350/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04116253\n",
      "| Epoch  80 |   400/  558 batches | lr 0.00006 | ms/batch 14.60 | loss 0.03979077\n",
      "| Epoch  80 |   450/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.04092436\n",
      "| Epoch  80 |   500/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.03952590\n",
      "| Epoch  80 |   550/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.04089858\n",
      "\n",
      "Val set: Average loss: 0.04371462\n",
      "\n",
      "| Epoch  81 |    50/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.03750181\n",
      "| Epoch  81 |   100/  558 batches | lr 0.00006 | ms/batch 14.78 | loss 0.03672906\n",
      "| Epoch  81 |   150/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.03767695\n",
      "| Epoch  81 |   200/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.04271890\n",
      "| Epoch  81 |   250/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.03948451\n",
      "| Epoch  81 |   300/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.03955465\n",
      "| Epoch  81 |   350/  558 batches | lr 0.00006 | ms/batch 16.13 | loss 0.04056435\n",
      "| Epoch  81 |   400/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.03954168\n",
      "| Epoch  81 |   450/  558 batches | lr 0.00006 | ms/batch 14.60 | loss 0.04085359\n",
      "| Epoch  81 |   500/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.03966057\n",
      "| Epoch  81 |   550/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.04073780\n",
      "\n",
      "Val set: Average loss: 0.04347037\n",
      "\n",
      "| Epoch  82 |    50/  558 batches | lr 0.00006 | ms/batch 16.63 | loss 0.03736582\n",
      "| Epoch  82 |   100/  558 batches | lr 0.00006 | ms/batch 16.39 | loss 0.03716530\n",
      "| Epoch  82 |   150/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.03930138\n",
      "| Epoch  82 |   200/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04312907\n",
      "| Epoch  82 |   250/  558 batches | lr 0.00006 | ms/batch 15.32 | loss 0.03955418\n",
      "| Epoch  82 |   300/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.03964852\n",
      "| Epoch  82 |   350/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04067296\n",
      "| Epoch  82 |   400/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.03969665\n",
      "| Epoch  82 |   450/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04082701\n",
      "| Epoch  82 |   500/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.03936521\n",
      "| Epoch  82 |   550/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04071617\n",
      "\n",
      "Val set: Average loss: 0.04341459\n",
      "\n",
      "| Epoch  83 |    50/  558 batches | lr 0.00006 | ms/batch 14.98 | loss 0.03734221\n",
      "| Epoch  83 |   100/  558 batches | lr 0.00006 | ms/batch 16.44 | loss 0.03680243\n",
      "| Epoch  83 |   150/  558 batches | lr 0.00006 | ms/batch 16.55 | loss 0.03783384\n",
      "| Epoch  83 |   200/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.04271481\n",
      "| Epoch  83 |   250/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.03943438\n",
      "| Epoch  83 |   300/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.03975750\n",
      "| Epoch  83 |   350/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04073718\n",
      "| Epoch  83 |   400/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03961226\n",
      "| Epoch  83 |   450/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04072962\n",
      "| Epoch  83 |   500/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.03930192\n",
      "| Epoch  83 |   550/  558 batches | lr 0.00006 | ms/batch 14.80 | loss 0.04070437\n",
      "\n",
      "Val set: Average loss: 0.04364824\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  84 |    50/  558 batches | lr 0.00006 | ms/batch 16.46 | loss 0.03742339\n",
      "| Epoch  84 |   100/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.03698609\n",
      "| Epoch  84 |   150/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.03749343\n",
      "| Epoch  84 |   200/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.04249395\n",
      "| Epoch  84 |   250/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.03934991\n",
      "| Epoch  84 |   300/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.03988117\n",
      "| Epoch  84 |   350/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.04121446\n",
      "| Epoch  84 |   400/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.03966079\n",
      "| Epoch  84 |   450/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.04069915\n",
      "| Epoch  84 |   500/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.03920727\n",
      "| Epoch  84 |   550/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04061900\n",
      "\n",
      "Val set: Average loss: 0.04354154\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  85 |    50/  558 batches | lr 0.00006 | ms/batch 15.02 | loss 0.03747217\n",
      "| Epoch  85 |   100/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.03707835\n",
      "| Epoch  85 |   150/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.03746136\n",
      "| Epoch  85 |   200/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.04242592\n",
      "| Epoch  85 |   250/  558 batches | lr 0.00006 | ms/batch 16.39 | loss 0.03920697\n",
      "| Epoch  85 |   300/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.03968518\n",
      "| Epoch  85 |   350/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04047289\n",
      "| Epoch  85 |   400/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.03945889\n",
      "| Epoch  85 |   450/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04062195\n",
      "| Epoch  85 |   500/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03919572\n",
      "| Epoch  85 |   550/  558 batches | lr 0.00006 | ms/batch 16.39 | loss 0.04048841\n",
      "\n",
      "Val set: Average loss: 0.04337994\n",
      "\n",
      "| Epoch  86 |    50/  558 batches | lr 0.00006 | ms/batch 15.00 | loss 0.03740422\n",
      "| Epoch  86 |   100/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.03696413\n",
      "| Epoch  86 |   150/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.03733896\n",
      "| Epoch  86 |   200/  558 batches | lr 0.00006 | ms/batch 14.74 | loss 0.04250429\n",
      "| Epoch  86 |   250/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03910853\n",
      "| Epoch  86 |   300/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.04013389\n",
      "| Epoch  86 |   350/  558 batches | lr 0.00006 | ms/batch 14.60 | loss 0.04108477\n",
      "| Epoch  86 |   400/  558 batches | lr 0.00006 | ms/batch 14.56 | loss 0.03949276\n",
      "| Epoch  86 |   450/  558 batches | lr 0.00006 | ms/batch 16.14 | loss 0.04050910\n",
      "| Epoch  86 |   500/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.03920054\n",
      "| Epoch  86 |   550/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.04039348\n",
      "\n",
      "Val set: Average loss: 0.04334279\n",
      "\n",
      "| Epoch  87 |    50/  558 batches | lr 0.00006 | ms/batch 14.93 | loss 0.03726890\n",
      "| Epoch  87 |   100/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.03659495\n",
      "| Epoch  87 |   150/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.03726715\n",
      "| Epoch  87 |   200/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.04284325\n",
      "| Epoch  87 |   250/  558 batches | lr 0.00006 | ms/batch 16.38 | loss 0.03917539\n",
      "| Epoch  87 |   300/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.03926498\n",
      "| Epoch  87 |   350/  558 batches | lr 0.00006 | ms/batch 14.85 | loss 0.04018737\n",
      "| Epoch  87 |   400/  558 batches | lr 0.00006 | ms/batch 16.16 | loss 0.03907955\n",
      "| Epoch  87 |   450/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.04052238\n",
      "| Epoch  87 |   500/  558 batches | lr 0.00006 | ms/batch 14.56 | loss 0.03957305\n",
      "| Epoch  87 |   550/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04052057\n",
      "\n",
      "Val set: Average loss: 0.04324479\n",
      "\n",
      "| Epoch  88 |    50/  558 batches | lr 0.00006 | ms/batch 15.08 | loss 0.03703089\n",
      "| Epoch  88 |   100/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.03691819\n",
      "| Epoch  88 |   150/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.03897042\n",
      "| Epoch  88 |   200/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04284053\n",
      "| Epoch  88 |   250/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.03916626\n",
      "| Epoch  88 |   300/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.04004323\n",
      "| Epoch  88 |   350/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04074428\n",
      "| Epoch  88 |   400/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.03934896\n",
      "| Epoch  88 |   450/  558 batches | lr 0.00006 | ms/batch 16.47 | loss 0.04035422\n",
      "| Epoch  88 |   500/  558 batches | lr 0.00006 | ms/batch 14.81 | loss 0.03907617\n",
      "| Epoch  88 |   550/  558 batches | lr 0.00006 | ms/batch 14.75 | loss 0.04039709\n",
      "\n",
      "Val set: Average loss: 0.04336375\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  89 |    50/  558 batches | lr 0.00006 | ms/batch 16.68 | loss 0.03723346\n",
      "| Epoch  89 |   100/  558 batches | lr 0.00006 | ms/batch 16.56 | loss 0.03652905\n",
      "| Epoch  89 |   150/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.03721608\n",
      "| Epoch  89 |   200/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04252589\n",
      "| Epoch  89 |   250/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.03898472\n",
      "| Epoch  89 |   300/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.03919633\n",
      "| Epoch  89 |   350/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.03997343\n",
      "| Epoch  89 |   400/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.03897362\n",
      "| Epoch  89 |   450/  558 batches | lr 0.00006 | ms/batch 16.43 | loss 0.04033471\n",
      "| Epoch  89 |   500/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.03972381\n",
      "| Epoch  89 |   550/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.04044432\n",
      "\n",
      "Val set: Average loss: 0.04320775\n",
      "\n",
      "| Epoch  90 |    50/  558 batches | lr 0.00006 | ms/batch 16.51 | loss 0.03701191\n",
      "| Epoch  90 |   100/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.03654402\n",
      "| Epoch  90 |   150/  558 batches | lr 0.00006 | ms/batch 16.51 | loss 0.03767322\n",
      "| Epoch  90 |   200/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.04261747\n",
      "| Epoch  90 |   250/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.03908469\n",
      "| Epoch  90 |   300/  558 batches | lr 0.00006 | ms/batch 14.60 | loss 0.04021595\n",
      "| Epoch  90 |   350/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.04091248\n",
      "| Epoch  90 |   400/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.03921095\n",
      "| Epoch  90 |   450/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04022425\n",
      "| Epoch  90 |   500/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.03923764\n",
      "| Epoch  90 |   550/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.04030633\n",
      "\n",
      "Val set: Average loss: 0.04328745\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  91 |    50/  558 batches | lr 0.00006 | ms/batch 14.97 | loss 0.03688487\n",
      "| Epoch  91 |   100/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.03625449\n",
      "| Epoch  91 |   150/  558 batches | lr 0.00006 | ms/batch 16.17 | loss 0.03782046\n",
      "| Epoch  91 |   200/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.04191134\n",
      "| Epoch  91 |   250/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.03855412\n",
      "| Epoch  91 |   300/  558 batches | lr 0.00006 | ms/batch 14.51 | loss 0.03881373\n",
      "| Epoch  91 |   350/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.04007599\n",
      "| Epoch  91 |   400/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.03905275\n",
      "| Epoch  91 |   450/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04022029\n",
      "| Epoch  91 |   500/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.04012884\n",
      "| Epoch  91 |   550/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04044923\n",
      "\n",
      "Val set: Average loss: 0.04318163\n",
      "\n",
      "| Epoch  92 |    50/  558 batches | lr 0.00006 | ms/batch 16.45 | loss 0.03690700\n",
      "| Epoch  92 |   100/  558 batches | lr 0.00006 | ms/batch 14.56 | loss 0.03659388\n",
      "| Epoch  92 |   150/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.03755181\n",
      "| Epoch  92 |   200/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04227761\n",
      "| Epoch  92 |   250/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.03906748\n",
      "| Epoch  92 |   300/  558 batches | lr 0.00006 | ms/batch 14.76 | loss 0.03960210\n",
      "| Epoch  92 |   350/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.03993661\n",
      "| Epoch  92 |   400/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.03910749\n",
      "| Epoch  92 |   450/  558 batches | lr 0.00006 | ms/batch 15.42 | loss 0.04014063\n",
      "| Epoch  92 |   500/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.03951979\n",
      "| Epoch  92 |   550/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.04027803\n",
      "\n",
      "Val set: Average loss: 0.04302838\n",
      "\n",
      "| Epoch  93 |    50/  558 batches | lr 0.00006 | ms/batch 16.62 | loss 0.03676527\n",
      "| Epoch  93 |   100/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.03653934\n",
      "| Epoch  93 |   150/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.03755196\n",
      "| Epoch  93 |   200/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04229180\n",
      "| Epoch  93 |   250/  558 batches | lr 0.00006 | ms/batch 16.19 | loss 0.03897742\n",
      "| Epoch  93 |   300/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.03906640\n",
      "| Epoch  93 |   350/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.03991730\n",
      "| Epoch  93 |   400/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.03892398\n",
      "| Epoch  93 |   450/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.04005033\n",
      "| Epoch  93 |   500/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.03935642\n",
      "| Epoch  93 |   550/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04021127\n",
      "\n",
      "Val set: Average loss: 0.04320958\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  94 |    50/  558 batches | lr 0.00006 | ms/batch 15.07 | loss 0.03663485\n",
      "| Epoch  94 |   100/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.03645749\n",
      "| Epoch  94 |   150/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.03747710\n",
      "| Epoch  94 |   200/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.04242796\n",
      "| Epoch  94 |   250/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.03897518\n",
      "| Epoch  94 |   300/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.03905735\n",
      "| Epoch  94 |   350/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.03988951\n",
      "| Epoch  94 |   400/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.03891722\n",
      "| Epoch  94 |   450/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03998190\n",
      "| Epoch  94 |   500/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.03909990\n",
      "| Epoch  94 |   550/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.04016347\n",
      "\n",
      "Val set: Average loss: 0.04328987\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  95 |    50/  558 batches | lr 0.00006 | ms/batch 16.58 | loss 0.03664356\n",
      "| Epoch  95 |   100/  558 batches | lr 0.00006 | ms/batch 16.21 | loss 0.03628763\n",
      "| Epoch  95 |   150/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.03728679\n",
      "| Epoch  95 |   200/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.04192727\n",
      "| Epoch  95 |   250/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.03868455\n",
      "| Epoch  95 |   300/  558 batches | lr 0.00006 | ms/batch 14.59 | loss 0.03880022\n",
      "| Epoch  95 |   350/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.03996140\n",
      "| Epoch  95 |   400/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03891696\n",
      "| Epoch  95 |   450/  558 batches | lr 0.00006 | ms/batch 14.58 | loss 0.03985364\n",
      "| Epoch  95 |   500/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.03904184\n",
      "| Epoch  95 |   550/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.04009480\n",
      "\n",
      "Val set: Average loss: 0.04332236\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  96 |    50/  558 batches | lr 0.00006 | ms/batch 15.02 | loss 0.03660633\n",
      "| Epoch  96 |   100/  558 batches | lr 0.00006 | ms/batch 16.38 | loss 0.03626531\n",
      "| Epoch  96 |   150/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.03721159\n",
      "| Epoch  96 |   200/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.04205011\n",
      "| Epoch  96 |   250/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.03859677\n",
      "| Epoch  96 |   300/  558 batches | lr 0.00006 | ms/batch 14.54 | loss 0.03873622\n",
      "| Epoch  96 |   350/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.03971795\n",
      "| Epoch  96 |   400/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.03878542\n",
      "| Epoch  96 |   450/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03982703\n",
      "| Epoch  96 |   500/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.03910961\n",
      "| Epoch  96 |   550/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.04006151\n",
      "\n",
      "Val set: Average loss: 0.04331056\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  97 |    50/  558 batches | lr 0.00006 | ms/batch 14.89 | loss 0.03645248\n",
      "| Epoch  97 |   100/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.03634129\n",
      "| Epoch  97 |   150/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.03714640\n",
      "| Epoch  97 |   200/  558 batches | lr 0.00006 | ms/batch 14.75 | loss 0.04202823\n",
      "| Epoch  97 |   250/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.03862458\n",
      "| Epoch  97 |   300/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.03841824\n",
      "| Epoch  97 |   350/  558 batches | lr 0.00006 | ms/batch 14.76 | loss 0.03988211\n",
      "| Epoch  97 |   400/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.03874617\n",
      "| Epoch  97 |   450/  558 batches | lr 0.00006 | ms/batch 16.40 | loss 0.03975348\n",
      "| Epoch  97 |   500/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.03901196\n",
      "| Epoch  97 |   550/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.03999418\n",
      "\n",
      "Val set: Average loss: 0.04328618\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  98 |    50/  558 batches | lr 0.00006 | ms/batch 16.58 | loss 0.03644771\n",
      "| Epoch  98 |   100/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.03622410\n",
      "| Epoch  98 |   150/  558 batches | lr 0.00006 | ms/batch 14.88 | loss 0.03705965\n",
      "| Epoch  98 |   200/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.04176063\n",
      "| Epoch  98 |   250/  558 batches | lr 0.00006 | ms/batch 14.80 | loss 0.03842200\n",
      "| Epoch  98 |   300/  558 batches | lr 0.00006 | ms/batch 16.40 | loss 0.03846689\n",
      "| Epoch  98 |   350/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.03973717\n",
      "| Epoch  98 |   400/  558 batches | lr 0.00006 | ms/batch 14.74 | loss 0.03873953\n",
      "| Epoch  98 |   450/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.03966114\n",
      "| Epoch  98 |   500/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.03885471\n",
      "| Epoch  98 |   550/  558 batches | lr 0.00006 | ms/batch 14.78 | loss 0.03987438\n",
      "\n",
      "Val set: Average loss: 0.04331303\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch  99 |    50/  558 batches | lr 0.00006 | ms/batch 16.54 | loss 0.03644579\n",
      "| Epoch  99 |   100/  558 batches | lr 0.00006 | ms/batch 14.80 | loss 0.03607985\n",
      "| Epoch  99 |   150/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.03691285\n",
      "| Epoch  99 |   200/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.04168431\n",
      "| Epoch  99 |   250/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.03832242\n",
      "| Epoch  99 |   300/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.03832053\n",
      "| Epoch  99 |   350/  558 batches | lr 0.00006 | ms/batch 14.80 | loss 0.03969895\n",
      "| Epoch  99 |   400/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.03868157\n",
      "| Epoch  99 |   450/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.03963034\n",
      "| Epoch  99 |   500/  558 batches | lr 0.00006 | ms/batch 14.57 | loss 0.03886608\n",
      "| Epoch  99 |   550/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.03984402\n",
      "\n",
      "Val set: Average loss: 0.04322549\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch 100 |    50/  558 batches | lr 0.00006 | ms/batch 15.03 | loss 0.03636574\n",
      "| Epoch 100 |   100/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.03604134\n",
      "| Epoch 100 |   150/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.03699845\n",
      "| Epoch 100 |   200/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04161787\n",
      "| Epoch 100 |   250/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.03828196\n",
      "| Epoch 100 |   300/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.03829336\n",
      "| Epoch 100 |   350/  558 batches | lr 0.00006 | ms/batch 14.77 | loss 0.03967483\n",
      "| Epoch 100 |   400/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.03859063\n",
      "| Epoch 100 |   450/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.03956562\n",
      "| Epoch 100 |   500/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.03870348\n",
      "| Epoch 100 |   550/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.03982400\n",
      "\n",
      "Val set: Average loss: 0.04328158\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch 101 |    50/  558 batches | lr 0.00006 | ms/batch 16.65 | loss 0.03632978\n",
      "| Epoch 101 |   100/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.03594384\n",
      "| Epoch 101 |   150/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03685315\n",
      "| Epoch 101 |   200/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.04158599\n",
      "| Epoch 101 |   250/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.03821960\n",
      "| Epoch 101 |   300/  558 batches | lr 0.00006 | ms/batch 14.63 | loss 0.03804568\n",
      "| Epoch 101 |   350/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.03977756\n",
      "| Epoch 101 |   400/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.03871795\n",
      "| Epoch 101 |   450/  558 batches | lr 0.00006 | ms/batch 16.40 | loss 0.03950346\n",
      "| Epoch 101 |   500/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03867450\n",
      "| Epoch 101 |   550/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.03976722\n",
      "\n",
      "Val set: Average loss: 0.04331312\n",
      "\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch 102 |    50/  558 batches | lr 0.00006 | ms/batch 15.00 | loss 0.03623177\n",
      "| Epoch 102 |   100/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.03604930\n",
      "| Epoch 102 |   150/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.03690994\n",
      "| Epoch 102 |   200/  558 batches | lr 0.00006 | ms/batch 16.43 | loss 0.04160200\n",
      "| Epoch 102 |   250/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03811529\n",
      "| Epoch 102 |   300/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.03823806\n",
      "| Epoch 102 |   350/  558 batches | lr 0.00006 | ms/batch 14.76 | loss 0.03956305\n",
      "| Epoch 102 |   400/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.03857185\n",
      "| Epoch 102 |   450/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.03944208\n",
      "| Epoch 102 |   500/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.03857275\n",
      "| Epoch 102 |   550/  558 batches | lr 0.00006 | ms/batch 16.41 | loss 0.03971050\n",
      "\n",
      "Val set: Average loss: 0.04328233\n",
      "\n",
      "EarlyStopping counter: 10 out of 20\n",
      "| Epoch 103 |    50/  558 batches | lr 0.00006 | ms/batch 14.91 | loss 0.03619437\n",
      "| Epoch 103 |   100/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.03579973\n",
      "| Epoch 103 |   150/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.03667972\n",
      "| Epoch 103 |   200/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.04148031\n",
      "| Epoch 103 |   250/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.03798393\n",
      "| Epoch 103 |   300/  558 batches | lr 0.00006 | ms/batch 16.33 | loss 0.03815610\n",
      "| Epoch 103 |   350/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.03956547\n",
      "| Epoch 103 |   400/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.03859997\n",
      "| Epoch 103 |   450/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.03934272\n",
      "| Epoch 103 |   500/  558 batches | lr 0.00006 | ms/batch 16.23 | loss 0.03847964\n",
      "| Epoch 103 |   550/  558 batches | lr 0.00006 | ms/batch 16.39 | loss 0.03963348\n",
      "\n",
      "Val set: Average loss: 0.04321528\n",
      "\n",
      "EarlyStopping counter: 11 out of 20\n",
      "| Epoch 104 |    50/  558 batches | lr 0.00006 | ms/batch 16.54 | loss 0.03614326\n",
      "| Epoch 104 |   100/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.03575561\n",
      "| Epoch 104 |   150/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.03669586\n",
      "| Epoch 104 |   200/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.04139799\n",
      "| Epoch 104 |   250/  558 batches | lr 0.00006 | ms/batch 16.39 | loss 0.03792627\n",
      "| Epoch 104 |   300/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.03812652\n",
      "| Epoch 104 |   350/  558 batches | lr 0.00006 | ms/batch 16.39 | loss 0.03951150\n",
      "| Epoch 104 |   400/  558 batches | lr 0.00006 | ms/batch 16.35 | loss 0.03844565\n",
      "| Epoch 104 |   450/  558 batches | lr 0.00006 | ms/batch 14.74 | loss 0.03929471\n",
      "| Epoch 104 |   500/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.03854194\n",
      "| Epoch 104 |   550/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.03959631\n",
      "\n",
      "Val set: Average loss: 0.04322654\n",
      "\n",
      "EarlyStopping counter: 12 out of 20\n",
      "| Epoch 105 |    50/  558 batches | lr 0.00006 | ms/batch 14.99 | loss 0.03611577\n",
      "| Epoch 105 |   100/  558 batches | lr 0.00006 | ms/batch 16.29 | loss 0.03571408\n",
      "| Epoch 105 |   150/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.03662788\n",
      "| Epoch 105 |   200/  558 batches | lr 0.00006 | ms/batch 14.74 | loss 0.04129961\n",
      "| Epoch 105 |   250/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.03790693\n",
      "| Epoch 105 |   300/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.03807459\n",
      "| Epoch 105 |   350/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.03937897\n",
      "| Epoch 105 |   400/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.03828963\n",
      "| Epoch 105 |   450/  558 batches | lr 0.00006 | ms/batch 16.38 | loss 0.03924131\n",
      "| Epoch 105 |   500/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.03842930\n",
      "| Epoch 105 |   550/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.03952074\n",
      "\n",
      "Val set: Average loss: 0.04315735\n",
      "\n",
      "EarlyStopping counter: 13 out of 20\n",
      "| Epoch 106 |    50/  558 batches | lr 0.00006 | ms/batch 16.63 | loss 0.03601379\n",
      "| Epoch 106 |   100/  558 batches | lr 0.00006 | ms/batch 16.43 | loss 0.03595049\n",
      "| Epoch 106 |   150/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.03669043\n",
      "| Epoch 106 |   200/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.04128329\n",
      "| Epoch 106 |   250/  558 batches | lr 0.00006 | ms/batch 14.62 | loss 0.03787307\n",
      "| Epoch 106 |   300/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.03792548\n",
      "| Epoch 106 |   350/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.03923626\n",
      "| Epoch 106 |   400/  558 batches | lr 0.00006 | ms/batch 16.57 | loss 0.03828789\n",
      "| Epoch 106 |   450/  558 batches | lr 0.00006 | ms/batch 14.76 | loss 0.03916888\n",
      "| Epoch 106 |   500/  558 batches | lr 0.00006 | ms/batch 16.27 | loss 0.03824243\n",
      "| Epoch 106 |   550/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.03948873\n",
      "\n",
      "Val set: Average loss: 0.04306517\n",
      "\n",
      "EarlyStopping counter: 14 out of 20\n",
      "| Epoch 107 |    50/  558 batches | lr 0.00006 | ms/batch 15.01 | loss 0.03604824\n",
      "| Epoch 107 |   100/  558 batches | lr 0.00006 | ms/batch 16.41 | loss 0.03558074\n",
      "| Epoch 107 |   150/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.03641383\n",
      "| Epoch 107 |   200/  558 batches | lr 0.00006 | ms/batch 15.00 | loss 0.04125688\n",
      "| Epoch 107 |   250/  558 batches | lr 0.00006 | ms/batch 16.39 | loss 0.03785319\n",
      "| Epoch 107 |   300/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.03788199\n",
      "| Epoch 107 |   350/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.03971208\n",
      "| Epoch 107 |   400/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.03854214\n",
      "| Epoch 107 |   450/  558 batches | lr 0.00006 | ms/batch 16.36 | loss 0.03916049\n",
      "| Epoch 107 |   500/  558 batches | lr 0.00006 | ms/batch 14.85 | loss 0.03826172\n",
      "| Epoch 107 |   550/  558 batches | lr 0.00006 | ms/batch 16.24 | loss 0.03946699\n",
      "\n",
      "Val set: Average loss: 0.04317446\n",
      "\n",
      "EarlyStopping counter: 15 out of 20\n",
      "| Epoch 108 |    50/  558 batches | lr 0.00006 | ms/batch 15.03 | loss 0.03607062\n",
      "| Epoch 108 |   100/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.03559221\n",
      "| Epoch 108 |   150/  558 batches | lr 0.00006 | ms/batch 16.40 | loss 0.03637713\n",
      "| Epoch 108 |   200/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.04117058\n",
      "| Epoch 108 |   250/  558 batches | lr 0.00006 | ms/batch 16.43 | loss 0.03776862\n",
      "| Epoch 108 |   300/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.03756595\n",
      "| Epoch 108 |   350/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.03935999\n",
      "| Epoch 108 |   400/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.03828743\n",
      "| Epoch 108 |   450/  558 batches | lr 0.00006 | ms/batch 16.26 | loss 0.03907427\n",
      "| Epoch 108 |   500/  558 batches | lr 0.00006 | ms/batch 14.69 | loss 0.03827970\n",
      "| Epoch 108 |   550/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.03939928\n",
      "\n",
      "Val set: Average loss: 0.04325211\n",
      "\n",
      "EarlyStopping counter: 16 out of 20\n",
      "| Epoch 109 |    50/  558 batches | lr 0.00006 | ms/batch 16.60 | loss 0.03591749\n",
      "| Epoch 109 |   100/  558 batches | lr 0.00006 | ms/batch 14.61 | loss 0.03545725\n",
      "| Epoch 109 |   150/  558 batches | lr 0.00006 | ms/batch 14.75 | loss 0.03644612\n",
      "| Epoch 109 |   200/  558 batches | lr 0.00006 | ms/batch 16.25 | loss 0.04107443\n",
      "| Epoch 109 |   250/  558 batches | lr 0.00006 | ms/batch 14.75 | loss 0.03749409\n",
      "| Epoch 109 |   300/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.03753615\n",
      "| Epoch 109 |   350/  558 batches | lr 0.00006 | ms/batch 16.28 | loss 0.03929441\n",
      "| Epoch 109 |   400/  558 batches | lr 0.00006 | ms/batch 14.64 | loss 0.03828884\n",
      "| Epoch 109 |   450/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.03900270\n",
      "| Epoch 109 |   500/  558 batches | lr 0.00006 | ms/batch 16.22 | loss 0.03823319\n",
      "| Epoch 109 |   550/  558 batches | lr 0.00006 | ms/batch 14.75 | loss 0.03937636\n",
      "\n",
      "Val set: Average loss: 0.04319519\n",
      "\n",
      "EarlyStopping counter: 17 out of 20\n",
      "| Epoch 110 |    50/  558 batches | lr 0.00006 | ms/batch 16.75 | loss 0.03574766\n",
      "| Epoch 110 |   100/  558 batches | lr 0.00006 | ms/batch 14.95 | loss 0.03566459\n",
      "| Epoch 110 |   150/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.03647036\n",
      "| Epoch 110 |   200/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.04101793\n",
      "| Epoch 110 |   250/  558 batches | lr 0.00006 | ms/batch 14.71 | loss 0.03761082\n",
      "| Epoch 110 |   300/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.03783959\n",
      "| Epoch 110 |   350/  558 batches | lr 0.00006 | ms/batch 16.42 | loss 0.03910504\n",
      "| Epoch 110 |   400/  558 batches | lr 0.00006 | ms/batch 14.90 | loss 0.03811956\n",
      "| Epoch 110 |   450/  558 batches | lr 0.00006 | ms/batch 16.31 | loss 0.03898596\n",
      "| Epoch 110 |   500/  558 batches | lr 0.00006 | ms/batch 14.73 | loss 0.03812583\n",
      "| Epoch 110 |   550/  558 batches | lr 0.00006 | ms/batch 14.65 | loss 0.03930493\n",
      "\n",
      "Val set: Average loss: 0.04314952\n",
      "\n",
      "EarlyStopping counter: 18 out of 20\n",
      "| Epoch 111 |    50/  558 batches | lr 0.00006 | ms/batch 14.99 | loss 0.03576044\n",
      "| Epoch 111 |   100/  558 batches | lr 0.00006 | ms/batch 16.30 | loss 0.03547150\n",
      "| Epoch 111 |   150/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.03635666\n",
      "| Epoch 111 |   200/  558 batches | lr 0.00006 | ms/batch 14.54 | loss 0.04096733\n",
      "| Epoch 111 |   250/  558 batches | lr 0.00006 | ms/batch 14.68 | loss 0.03749897\n",
      "| Epoch 111 |   300/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.03737091\n",
      "| Epoch 111 |   350/  558 batches | lr 0.00006 | ms/batch 14.70 | loss 0.03907229\n",
      "| Epoch 111 |   400/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.03816340\n",
      "| Epoch 111 |   450/  558 batches | lr 0.00006 | ms/batch 16.43 | loss 0.03886014\n",
      "| Epoch 111 |   500/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.03807855\n",
      "| Epoch 111 |   550/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.03919772\n",
      "\n",
      "Val set: Average loss: 0.04312325\n",
      "\n",
      "EarlyStopping counter: 19 out of 20\n",
      "| Epoch 112 |    50/  558 batches | lr 0.00006 | ms/batch 16.59 | loss 0.03576769\n",
      "| Epoch 112 |   100/  558 batches | lr 0.00006 | ms/batch 14.72 | loss 0.03536086\n",
      "| Epoch 112 |   150/  558 batches | lr 0.00006 | ms/batch 16.46 | loss 0.03611528\n",
      "| Epoch 112 |   200/  558 batches | lr 0.00006 | ms/batch 16.34 | loss 0.04088704\n",
      "| Epoch 112 |   250/  558 batches | lr 0.00006 | ms/batch 14.66 | loss 0.03740922\n",
      "| Epoch 112 |   300/  558 batches | lr 0.00006 | ms/batch 14.78 | loss 0.03750902\n",
      "| Epoch 112 |   350/  558 batches | lr 0.00006 | ms/batch 16.37 | loss 0.03932834\n",
      "| Epoch 112 |   400/  558 batches | lr 0.00006 | ms/batch 14.67 | loss 0.03841687\n",
      "| Epoch 112 |   450/  558 batches | lr 0.00006 | ms/batch 16.32 | loss 0.03882141\n",
      "| Epoch 112 |   500/  558 batches | lr 0.00006 | ms/batch 16.20 | loss 0.03798655\n",
      "| Epoch 112 |   550/  558 batches | lr 0.00006 | ms/batch 14.87 | loss 0.03914788\n",
      "\n",
      "Val set: Average loss: 0.04307866\n",
      "\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Stopping at Epoch: 112\n"
     ]
    }
   ],
   "source": [
    "load = False\n",
    "save_model_path = '../models/final_real_data_model_cleaned.chkpt'\n",
    "val_err_df_path = '../results/val_final_real_data_model_cleaned.csv'\n",
    "\n",
    "if not load:\n",
    "  train_losses, val_losses = train(\n",
    "      epochs,\n",
    "      batch_size,\n",
    "      model,\n",
    "      optimizer,\n",
    "      loss_fn,\n",
    "      X_train,\n",
    "      y_train,\n",
    "      X_val,\n",
    "      y_val)\n",
    "  val_err_df = pd.DataFrame({\n",
    "      'Training': train_losses,\n",
    "      'Validation': val_losses})\n",
    "  val_err_df.to_csv(val_err_df_path)\n",
    "  torch.save(model.state_dict(), save_model_path)\n",
    "else:\n",
    "  model = Net(input_size, output_size, hidden_size, num_layers, act_fn)\n",
    "  model.load_state_dict(torch.load(save_model_path, map_location=device))\n",
    "  model = model.to(device)\n",
    "  val_err_df = pd.read_csv(val_err_df_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650894219921,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "KkenWSRYvDpl",
    "outputId": "37746aa5-0b46-4a80-d14d-397868e89361",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_err_df[1:].plot(xlabel='epoch', ylabel='MSE')\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlG5Ky25vDpm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test the model\n",
    "\n",
    "Now that the model has been trained, let's verify the accuracy of the model's prediction on a small sample of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1650894219922,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "TBZ4eHAdvDpn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 30\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_out = model(X_test[0:test_size].to(device))\n",
    "\n",
    "test_out = output_sc.inverse_transform(test_out.cpu().detach().numpy())\n",
    "real_out = output_sc.inverse_transform(y_test[0:test_size].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1650894219926,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "wK1WeGIZvDpo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['strike', 'close', 'hv_21', 'moneyness', 'tau', 'r',\n",
    "       'call', 'put']\n",
    "test_options = pd.DataFrame(input_sc.inverse_transform(X_test[0:test_size].detach().cpu().numpy()), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1650894220542,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "_dvE5LGXvDpq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         strike        close     hv_21  moneyness       tau         r  \\\n0    175.201843   186.992081  0.464006   0.936050  0.895458  0.008430   \n1   1079.995117  2433.269287  0.594730   0.443467  0.120853  0.011406   \n2   1575.103271   977.128235  0.605451   0.620090  0.159154  0.008148   \n3   2223.874268   876.535767  0.712410   0.394227  1.739545  0.008331   \n4    135.148193   198.905624  0.600590   0.678965  0.021910  0.011925   \n5   3300.873779  2404.062012  0.592800   0.728562  0.469732  0.011437   \n6    104.984261   175.847183  0.742214   0.596897  1.104147  0.009094   \n7   2380.133057  2145.807373  0.751469   1.109373  0.002760  0.010322   \n8    260.254150   208.128983  0.823657   1.248085  0.041306  0.008529   \n9    239.980042   274.614197  0.383385   0.874500  2.060680  0.011925   \n10  3300.873779  2308.753662  0.741213   0.699303  0.046707  0.010437   \n11  2025.089233   876.535767  0.712410   0.432763  0.392824  0.008331   \n12  3110.989502  2082.012207  0.748074   1.494287  0.008407  0.010880   \n13   149.982849   271.924042  0.393963   1.811853  0.643067  0.011437   \n14   585.134155   177.384415  1.558969   0.303952  0.104404  0.009094   \n15    93.116524    89.378014  0.553633   0.963882  0.021910  0.008408   \n16  1220.059448  1028.048950  0.574217   1.186534  0.008407  0.008201   \n17   268.166016  1028.048950  0.574217   3.835707  0.412113  0.008201   \n18   149.982849   260.779144  0.472583   1.738349  0.145651  0.010673   \n19  1449.997314  1092.036133  0.631359   1.328190  0.145651  0.005705   \n20   244.924988   183.917618  0.498456   1.331223  0.063402  0.008232   \n21   159.872681   259.626221  0.462862   0.616879  0.065858  0.010880   \n22   689.966064   659.017761  0.797855   1.046864  0.659271  0.010719   \n23  2796.493652  2308.753662  0.741213   0.825794  0.008407  0.010437   \n24  1190.142700   724.506165  0.807647   0.609029  0.011107  0.010749   \n25  2100.251709   977.128235  0.605451   0.465590  0.024611  0.008148   \n26   740.033142  1022.476562  0.581078   1.381891  0.024611  0.007851   \n27   154.927795   156.631790  0.277605   1.010758  2.159869  0.008232   \n28   104.984261   165.855179  0.358512   0.632579  0.871152  0.008331   \n29  3089.231934  2108.145264  0.685554   0.682176  0.351578  0.009628   \n\n        call       put  Prediction        Real  \n0   0.000215  0.999785   25.539783   25.428410  \n1   0.000215  0.999785   -3.084072    0.834347  \n2   1.000080 -0.000080   63.846516    3.311031  \n3   1.000080 -0.000080   89.246368   74.011772  \n4   0.000215  0.999785   -1.391178    0.027981  \n5   1.000080 -0.000080   47.022457   43.801968  \n6   0.000215  0.999785    9.689625    3.887005  \n7   0.000215  0.999785  212.467499  223.390488  \n8   0.000215  0.999785   53.521835   51.865593  \n9   0.000215  0.999785   35.975079   27.559513  \n10  1.000080 -0.000080    8.121532    0.373569  \n11  1.000080 -0.000080    9.294514    3.541424  \n12  0.000215  0.999785  882.825684  883.340881  \n13  1.000080 -0.000080  132.454575  114.000076  \n14  1.000080 -0.000080    2.687397    0.027981  \n15  1.000080 -0.000080    7.924274    3.080646  \n16  0.000215  0.999785  214.060013  199.141998  \n17  1.000080 -0.000080  734.342834  714.235107  \n18  1.000080 -0.000080  127.735611  117.000992  \n19  0.000215  0.999785  447.693237  379.709656  \n20  0.000215  0.999785   64.861458   62.261913  \n21  0.000215  0.999785    0.675503    0.085583  \n22  0.000215  0.999785  157.646027  153.265717  \n23  1.000080 -0.000080    6.058201    0.143177  \n24  1.000080 -0.000080    2.680576    0.027981  \n25  1.000080 -0.000080    5.833233    0.085583  \n26  1.000080 -0.000080  284.378754  292.968079  \n27  1.000080 -0.000080   38.199238   33.664837  \n28  0.000215  0.999785    5.860485    3.023044  \n29  1.000080 -0.000080   33.670506   19.438290  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>strike</th>\n      <th>close</th>\n      <th>hv_21</th>\n      <th>moneyness</th>\n      <th>tau</th>\n      <th>r</th>\n      <th>call</th>\n      <th>put</th>\n      <th>Prediction</th>\n      <th>Real</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>175.201843</td>\n      <td>186.992081</td>\n      <td>0.464006</td>\n      <td>0.936050</td>\n      <td>0.895458</td>\n      <td>0.008430</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>25.539783</td>\n      <td>25.428410</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1079.995117</td>\n      <td>2433.269287</td>\n      <td>0.594730</td>\n      <td>0.443467</td>\n      <td>0.120853</td>\n      <td>0.011406</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>-3.084072</td>\n      <td>0.834347</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1575.103271</td>\n      <td>977.128235</td>\n      <td>0.605451</td>\n      <td>0.620090</td>\n      <td>0.159154</td>\n      <td>0.008148</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>63.846516</td>\n      <td>3.311031</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2223.874268</td>\n      <td>876.535767</td>\n      <td>0.712410</td>\n      <td>0.394227</td>\n      <td>1.739545</td>\n      <td>0.008331</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>89.246368</td>\n      <td>74.011772</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>135.148193</td>\n      <td>198.905624</td>\n      <td>0.600590</td>\n      <td>0.678965</td>\n      <td>0.021910</td>\n      <td>0.011925</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>-1.391178</td>\n      <td>0.027981</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3300.873779</td>\n      <td>2404.062012</td>\n      <td>0.592800</td>\n      <td>0.728562</td>\n      <td>0.469732</td>\n      <td>0.011437</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>47.022457</td>\n      <td>43.801968</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>104.984261</td>\n      <td>175.847183</td>\n      <td>0.742214</td>\n      <td>0.596897</td>\n      <td>1.104147</td>\n      <td>0.009094</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>9.689625</td>\n      <td>3.887005</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2380.133057</td>\n      <td>2145.807373</td>\n      <td>0.751469</td>\n      <td>1.109373</td>\n      <td>0.002760</td>\n      <td>0.010322</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>212.467499</td>\n      <td>223.390488</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>260.254150</td>\n      <td>208.128983</td>\n      <td>0.823657</td>\n      <td>1.248085</td>\n      <td>0.041306</td>\n      <td>0.008529</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>53.521835</td>\n      <td>51.865593</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>239.980042</td>\n      <td>274.614197</td>\n      <td>0.383385</td>\n      <td>0.874500</td>\n      <td>2.060680</td>\n      <td>0.011925</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>35.975079</td>\n      <td>27.559513</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3300.873779</td>\n      <td>2308.753662</td>\n      <td>0.741213</td>\n      <td>0.699303</td>\n      <td>0.046707</td>\n      <td>0.010437</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>8.121532</td>\n      <td>0.373569</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2025.089233</td>\n      <td>876.535767</td>\n      <td>0.712410</td>\n      <td>0.432763</td>\n      <td>0.392824</td>\n      <td>0.008331</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>9.294514</td>\n      <td>3.541424</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>3110.989502</td>\n      <td>2082.012207</td>\n      <td>0.748074</td>\n      <td>1.494287</td>\n      <td>0.008407</td>\n      <td>0.010880</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>882.825684</td>\n      <td>883.340881</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>149.982849</td>\n      <td>271.924042</td>\n      <td>0.393963</td>\n      <td>1.811853</td>\n      <td>0.643067</td>\n      <td>0.011437</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>132.454575</td>\n      <td>114.000076</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>585.134155</td>\n      <td>177.384415</td>\n      <td>1.558969</td>\n      <td>0.303952</td>\n      <td>0.104404</td>\n      <td>0.009094</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>2.687397</td>\n      <td>0.027981</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>93.116524</td>\n      <td>89.378014</td>\n      <td>0.553633</td>\n      <td>0.963882</td>\n      <td>0.021910</td>\n      <td>0.008408</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>7.924274</td>\n      <td>3.080646</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1220.059448</td>\n      <td>1028.048950</td>\n      <td>0.574217</td>\n      <td>1.186534</td>\n      <td>0.008407</td>\n      <td>0.008201</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>214.060013</td>\n      <td>199.141998</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>268.166016</td>\n      <td>1028.048950</td>\n      <td>0.574217</td>\n      <td>3.835707</td>\n      <td>0.412113</td>\n      <td>0.008201</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>734.342834</td>\n      <td>714.235107</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>149.982849</td>\n      <td>260.779144</td>\n      <td>0.472583</td>\n      <td>1.738349</td>\n      <td>0.145651</td>\n      <td>0.010673</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>127.735611</td>\n      <td>117.000992</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1449.997314</td>\n      <td>1092.036133</td>\n      <td>0.631359</td>\n      <td>1.328190</td>\n      <td>0.145651</td>\n      <td>0.005705</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>447.693237</td>\n      <td>379.709656</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>244.924988</td>\n      <td>183.917618</td>\n      <td>0.498456</td>\n      <td>1.331223</td>\n      <td>0.063402</td>\n      <td>0.008232</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>64.861458</td>\n      <td>62.261913</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>159.872681</td>\n      <td>259.626221</td>\n      <td>0.462862</td>\n      <td>0.616879</td>\n      <td>0.065858</td>\n      <td>0.010880</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>0.675503</td>\n      <td>0.085583</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>689.966064</td>\n      <td>659.017761</td>\n      <td>0.797855</td>\n      <td>1.046864</td>\n      <td>0.659271</td>\n      <td>0.010719</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>157.646027</td>\n      <td>153.265717</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2796.493652</td>\n      <td>2308.753662</td>\n      <td>0.741213</td>\n      <td>0.825794</td>\n      <td>0.008407</td>\n      <td>0.010437</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>6.058201</td>\n      <td>0.143177</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1190.142700</td>\n      <td>724.506165</td>\n      <td>0.807647</td>\n      <td>0.609029</td>\n      <td>0.011107</td>\n      <td>0.010749</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>2.680576</td>\n      <td>0.027981</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2100.251709</td>\n      <td>977.128235</td>\n      <td>0.605451</td>\n      <td>0.465590</td>\n      <td>0.024611</td>\n      <td>0.008148</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>5.833233</td>\n      <td>0.085583</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>740.033142</td>\n      <td>1022.476562</td>\n      <td>0.581078</td>\n      <td>1.381891</td>\n      <td>0.024611</td>\n      <td>0.007851</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>284.378754</td>\n      <td>292.968079</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>154.927795</td>\n      <td>156.631790</td>\n      <td>0.277605</td>\n      <td>1.010758</td>\n      <td>2.159869</td>\n      <td>0.008232</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>38.199238</td>\n      <td>33.664837</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>104.984261</td>\n      <td>165.855179</td>\n      <td>0.358512</td>\n      <td>0.632579</td>\n      <td>0.871152</td>\n      <td>0.008331</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>5.860485</td>\n      <td>3.023044</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>3089.231934</td>\n      <td>2108.145264</td>\n      <td>0.685554</td>\n      <td>0.682176</td>\n      <td>0.351578</td>\n      <td>0.009628</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>33.670506</td>\n      <td>19.438290</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_options['Prediction'] = test_out\n",
    "test_options['Real'] = real_out\n",
    "test_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1650894220545,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "TNw5-1jgvDpr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_options['Abs Error'] = np.abs(test_options.Prediction - test_options.Real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1650894220546,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "lAC6AdeVvDpr",
    "outputId": "78bd13cc-c5a0-4746-f402-9e84d8b78696",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         strike        close     hv_21  moneyness       tau         r  \\\n0    175.201843   186.992081  0.464006   0.936050  0.895458  0.008430   \n12  3110.989502  2082.012207  0.748074   1.494287  0.008407  0.010880   \n21   159.872681   259.626221  0.462862   0.616879  0.065858  0.010880   \n4    135.148193   198.905624  0.600590   0.678965  0.021910  0.011925   \n8    260.254150   208.128983  0.823657   1.248085  0.041306  0.008529   \n20   244.924988   183.917618  0.498456   1.331223  0.063402  0.008232   \n24  1190.142700   724.506165  0.807647   0.609029  0.011107  0.010749   \n14   585.134155   177.384415  1.558969   0.303952  0.104404  0.009094   \n28   104.984261   165.855179  0.358512   0.632579  0.871152  0.008331   \n5   3300.873779  2404.062012  0.592800   0.728562  0.469732  0.011437   \n1   1079.995117  2433.269287  0.594730   0.443467  0.120853  0.011406   \n22   689.966064   659.017761  0.797855   1.046864  0.659271  0.010719   \n27   154.927795   156.631790  0.277605   1.010758  2.159869  0.008232   \n15    93.116524    89.378014  0.553633   0.963882  0.021910  0.008408   \n25  2100.251709   977.128235  0.605451   0.465590  0.024611  0.008148   \n11  2025.089233   876.535767  0.712410   0.432763  0.392824  0.008331   \n6    104.984261   175.847183  0.742214   0.596897  1.104147  0.009094   \n23  2796.493652  2308.753662  0.741213   0.825794  0.008407  0.010437   \n10  3300.873779  2308.753662  0.741213   0.699303  0.046707  0.010437   \n9    239.980042   274.614197  0.383385   0.874500  2.060680  0.011925   \n26   740.033142  1022.476562  0.581078   1.381891  0.024611  0.007851   \n18   149.982849   260.779144  0.472583   1.738349  0.145651  0.010673   \n7   2380.133057  2145.807373  0.751469   1.109373  0.002760  0.010322   \n29  3089.231934  2108.145264  0.685554   0.682176  0.351578  0.009628   \n16  1220.059448  1028.048950  0.574217   1.186534  0.008407  0.008201   \n3   2223.874268   876.535767  0.712410   0.394227  1.739545  0.008331   \n13   149.982849   271.924042  0.393963   1.811853  0.643067  0.011437   \n17   268.166016  1028.048950  0.574217   3.835707  0.412113  0.008201   \n2   1575.103271   977.128235  0.605451   0.620090  0.159154  0.008148   \n19  1449.997314  1092.036133  0.631359   1.328190  0.145651  0.005705   \n\n        call       put  Prediction        Real  Abs Error  \n0   0.000215  0.999785   25.539783   25.428410   0.111374  \n12  0.000215  0.999785  882.825684  883.340881   0.515198  \n21  0.000215  0.999785    0.675503    0.085583   0.589920  \n4   0.000215  0.999785   -1.391178    0.027981   1.419159  \n8   0.000215  0.999785   53.521835   51.865593   1.656242  \n20  0.000215  0.999785   64.861458   62.261913   2.599545  \n24  1.000080 -0.000080    2.680576    0.027981   2.652596  \n14  1.000080 -0.000080    2.687397    0.027981   2.659416  \n28  0.000215  0.999785    5.860485    3.023044   2.837441  \n5   1.000080 -0.000080   47.022457   43.801968   3.220490  \n1   0.000215  0.999785   -3.084072    0.834347   3.918419  \n22  0.000215  0.999785  157.646027  153.265717   4.380310  \n27  1.000080 -0.000080   38.199238   33.664837   4.534401  \n15  1.000080 -0.000080    7.924274    3.080646   4.843628  \n25  1.000080 -0.000080    5.833233    0.085583   5.747650  \n11  1.000080 -0.000080    9.294514    3.541424   5.753090  \n6   0.000215  0.999785    9.689625    3.887005   5.802620  \n23  1.000080 -0.000080    6.058201    0.143177   5.915024  \n10  1.000080 -0.000080    8.121532    0.373569   7.747963  \n9   0.000215  0.999785   35.975079   27.559513   8.415565  \n26  1.000080 -0.000080  284.378754  292.968079   8.589325  \n18  1.000080 -0.000080  127.735611  117.000992  10.734619  \n7   0.000215  0.999785  212.467499  223.390488  10.922989  \n29  1.000080 -0.000080   33.670506   19.438290  14.232216  \n16  0.000215  0.999785  214.060013  199.141998  14.918015  \n3   1.000080 -0.000080   89.246368   74.011772  15.234596  \n13  1.000080 -0.000080  132.454575  114.000076  18.454498  \n17  1.000080 -0.000080  734.342834  714.235107  20.107727  \n2   1.000080 -0.000080   63.846516    3.311031  60.535484  \n19  0.000215  0.999785  447.693237  379.709656  67.983582  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>strike</th>\n      <th>close</th>\n      <th>hv_21</th>\n      <th>moneyness</th>\n      <th>tau</th>\n      <th>r</th>\n      <th>call</th>\n      <th>put</th>\n      <th>Prediction</th>\n      <th>Real</th>\n      <th>Abs Error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>175.201843</td>\n      <td>186.992081</td>\n      <td>0.464006</td>\n      <td>0.936050</td>\n      <td>0.895458</td>\n      <td>0.008430</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>25.539783</td>\n      <td>25.428410</td>\n      <td>0.111374</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>3110.989502</td>\n      <td>2082.012207</td>\n      <td>0.748074</td>\n      <td>1.494287</td>\n      <td>0.008407</td>\n      <td>0.010880</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>882.825684</td>\n      <td>883.340881</td>\n      <td>0.515198</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>159.872681</td>\n      <td>259.626221</td>\n      <td>0.462862</td>\n      <td>0.616879</td>\n      <td>0.065858</td>\n      <td>0.010880</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>0.675503</td>\n      <td>0.085583</td>\n      <td>0.589920</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>135.148193</td>\n      <td>198.905624</td>\n      <td>0.600590</td>\n      <td>0.678965</td>\n      <td>0.021910</td>\n      <td>0.011925</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>-1.391178</td>\n      <td>0.027981</td>\n      <td>1.419159</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>260.254150</td>\n      <td>208.128983</td>\n      <td>0.823657</td>\n      <td>1.248085</td>\n      <td>0.041306</td>\n      <td>0.008529</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>53.521835</td>\n      <td>51.865593</td>\n      <td>1.656242</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>244.924988</td>\n      <td>183.917618</td>\n      <td>0.498456</td>\n      <td>1.331223</td>\n      <td>0.063402</td>\n      <td>0.008232</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>64.861458</td>\n      <td>62.261913</td>\n      <td>2.599545</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1190.142700</td>\n      <td>724.506165</td>\n      <td>0.807647</td>\n      <td>0.609029</td>\n      <td>0.011107</td>\n      <td>0.010749</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>2.680576</td>\n      <td>0.027981</td>\n      <td>2.652596</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>585.134155</td>\n      <td>177.384415</td>\n      <td>1.558969</td>\n      <td>0.303952</td>\n      <td>0.104404</td>\n      <td>0.009094</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>2.687397</td>\n      <td>0.027981</td>\n      <td>2.659416</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>104.984261</td>\n      <td>165.855179</td>\n      <td>0.358512</td>\n      <td>0.632579</td>\n      <td>0.871152</td>\n      <td>0.008331</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>5.860485</td>\n      <td>3.023044</td>\n      <td>2.837441</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3300.873779</td>\n      <td>2404.062012</td>\n      <td>0.592800</td>\n      <td>0.728562</td>\n      <td>0.469732</td>\n      <td>0.011437</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>47.022457</td>\n      <td>43.801968</td>\n      <td>3.220490</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1079.995117</td>\n      <td>2433.269287</td>\n      <td>0.594730</td>\n      <td>0.443467</td>\n      <td>0.120853</td>\n      <td>0.011406</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>-3.084072</td>\n      <td>0.834347</td>\n      <td>3.918419</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>689.966064</td>\n      <td>659.017761</td>\n      <td>0.797855</td>\n      <td>1.046864</td>\n      <td>0.659271</td>\n      <td>0.010719</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>157.646027</td>\n      <td>153.265717</td>\n      <td>4.380310</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>154.927795</td>\n      <td>156.631790</td>\n      <td>0.277605</td>\n      <td>1.010758</td>\n      <td>2.159869</td>\n      <td>0.008232</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>38.199238</td>\n      <td>33.664837</td>\n      <td>4.534401</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>93.116524</td>\n      <td>89.378014</td>\n      <td>0.553633</td>\n      <td>0.963882</td>\n      <td>0.021910</td>\n      <td>0.008408</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>7.924274</td>\n      <td>3.080646</td>\n      <td>4.843628</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2100.251709</td>\n      <td>977.128235</td>\n      <td>0.605451</td>\n      <td>0.465590</td>\n      <td>0.024611</td>\n      <td>0.008148</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>5.833233</td>\n      <td>0.085583</td>\n      <td>5.747650</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2025.089233</td>\n      <td>876.535767</td>\n      <td>0.712410</td>\n      <td>0.432763</td>\n      <td>0.392824</td>\n      <td>0.008331</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>9.294514</td>\n      <td>3.541424</td>\n      <td>5.753090</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>104.984261</td>\n      <td>175.847183</td>\n      <td>0.742214</td>\n      <td>0.596897</td>\n      <td>1.104147</td>\n      <td>0.009094</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>9.689625</td>\n      <td>3.887005</td>\n      <td>5.802620</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2796.493652</td>\n      <td>2308.753662</td>\n      <td>0.741213</td>\n      <td>0.825794</td>\n      <td>0.008407</td>\n      <td>0.010437</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>6.058201</td>\n      <td>0.143177</td>\n      <td>5.915024</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3300.873779</td>\n      <td>2308.753662</td>\n      <td>0.741213</td>\n      <td>0.699303</td>\n      <td>0.046707</td>\n      <td>0.010437</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>8.121532</td>\n      <td>0.373569</td>\n      <td>7.747963</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>239.980042</td>\n      <td>274.614197</td>\n      <td>0.383385</td>\n      <td>0.874500</td>\n      <td>2.060680</td>\n      <td>0.011925</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>35.975079</td>\n      <td>27.559513</td>\n      <td>8.415565</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>740.033142</td>\n      <td>1022.476562</td>\n      <td>0.581078</td>\n      <td>1.381891</td>\n      <td>0.024611</td>\n      <td>0.007851</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>284.378754</td>\n      <td>292.968079</td>\n      <td>8.589325</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>149.982849</td>\n      <td>260.779144</td>\n      <td>0.472583</td>\n      <td>1.738349</td>\n      <td>0.145651</td>\n      <td>0.010673</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>127.735611</td>\n      <td>117.000992</td>\n      <td>10.734619</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2380.133057</td>\n      <td>2145.807373</td>\n      <td>0.751469</td>\n      <td>1.109373</td>\n      <td>0.002760</td>\n      <td>0.010322</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>212.467499</td>\n      <td>223.390488</td>\n      <td>10.922989</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>3089.231934</td>\n      <td>2108.145264</td>\n      <td>0.685554</td>\n      <td>0.682176</td>\n      <td>0.351578</td>\n      <td>0.009628</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>33.670506</td>\n      <td>19.438290</td>\n      <td>14.232216</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1220.059448</td>\n      <td>1028.048950</td>\n      <td>0.574217</td>\n      <td>1.186534</td>\n      <td>0.008407</td>\n      <td>0.008201</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>214.060013</td>\n      <td>199.141998</td>\n      <td>14.918015</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2223.874268</td>\n      <td>876.535767</td>\n      <td>0.712410</td>\n      <td>0.394227</td>\n      <td>1.739545</td>\n      <td>0.008331</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>89.246368</td>\n      <td>74.011772</td>\n      <td>15.234596</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>149.982849</td>\n      <td>271.924042</td>\n      <td>0.393963</td>\n      <td>1.811853</td>\n      <td>0.643067</td>\n      <td>0.011437</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>132.454575</td>\n      <td>114.000076</td>\n      <td>18.454498</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>268.166016</td>\n      <td>1028.048950</td>\n      <td>0.574217</td>\n      <td>3.835707</td>\n      <td>0.412113</td>\n      <td>0.008201</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>734.342834</td>\n      <td>714.235107</td>\n      <td>20.107727</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1575.103271</td>\n      <td>977.128235</td>\n      <td>0.605451</td>\n      <td>0.620090</td>\n      <td>0.159154</td>\n      <td>0.008148</td>\n      <td>1.000080</td>\n      <td>-0.000080</td>\n      <td>63.846516</td>\n      <td>3.311031</td>\n      <td>60.535484</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1449.997314</td>\n      <td>1092.036133</td>\n      <td>0.631359</td>\n      <td>1.328190</td>\n      <td>0.145651</td>\n      <td>0.005705</td>\n      <td>0.000215</td>\n      <td>0.999785</td>\n      <td>447.693237</td>\n      <td>379.709656</td>\n      <td>67.983582</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_options.sort_values('Abs Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Performance\n",
    "\n",
    "Let's now test the model's accuracy on different sets (training, validation and testing) using different loss metrics."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDJ-hkrSAqVh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MSE (Mean squared error)\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def get_mse(model, X, y, batch_size):\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X, y), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = loss_fn(out, batch_labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1650895213041,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "GG64Xp3vAib-",
    "outputId": "f846ee65-029d-4a76-b006-d65e4ac07a48",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE on the train set is:  0.0382613068176133\n",
      "The MSE on the val set is:  0.043078656042260785\n",
      "The MSE on the test set is:  0.03890983129718474\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The MSE on the train set is: ', get_mse(model, X_train, y_train, batch_size).mean())\n",
    "print('The MSE on the val set is: ', get_mse(model, X_val, y_val, batch_size).mean())\n",
    "print('The MSE on the test set is: ', get_mse(model, X_test, y_test, batch_size).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCLf2KQJAvoE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MAE (Mean absolute error)\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum |y_i - \\hat{y}_i |\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def get_mae(model, X, y, batch_size):\n",
    "    mae_loss = nn.L1Loss()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X, y), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = mae_loss(out, batch_labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1650895213494,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "mZohsEyLAu7S",
    "outputId": "4d6bfba3-5751-478d-d53e-73751702414e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE on the train set is:  0.06676574439264327\n",
      "The MAE on the val set is:  0.06912885052817208\n",
      "The MAE on the test set is:  0.0682397718408278\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The MAE on the train set is: ', get_mae(model, X_train, y_train, batch_size).mean())\n",
    "print('The MAE on the val set is: ', get_mae(model, X_val, y_val, batch_size).mean())\n",
    "print('The MAE on the test set is: ', get_mae(model, X_test, y_test, batch_size).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmsZ4aPaA1SJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### RSME (Root mean squared error)\n",
    "\n",
    "$$\n",
    "RSME = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the train set is:  0.19248191089536268\n",
      "The RMSE on the val set is:  0.2047474972882962\n",
      "The RMSE on the test set is:  0.1941669850142239\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The RMSE on the train set is: ', np.sqrt(get_mse(model, X_train, y_train, batch_size)).mean())\n",
    "print('The RMSE on the val set is: ', np.sqrt(get_mse(model, X_val, y_val, batch_size)).mean())\n",
    "print('The RMSE on the test set is: ', np.sqrt(get_mse(model, X_test, y_test, batch_size)).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8Jxp0L4A5zb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MAPE (Mean absolute percentage error)\n",
    "\n",
    "$$\n",
    "MAPE = \\frac{1}{n} \\sum \\frac{|y_i - \\hat{y}_i|}{y_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def get_mape(model, X, y, batch_size):\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X, y), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = MAPELoss(out, batch_labels.to(device)).detach().cpu().item()\n",
    "            losses.append(loss)\n",
    "\n",
    "    return np.array(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1650895213818,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "fHghkyflA79o",
    "outputId": "37838323-11f0-4eb9-d7d7-50303bf4e9ce",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAPE on the train set is:  0.5852175291667703\n",
      "The MAPE on the val set is:  0.7770054436155728\n",
      "The MAPE on the test set is:  0.5997079587408475\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The MAPE on the train set is: ', get_mape(model, X_train, y_train, batch_size).mean())\n",
    "print('The MAPE on the val set is: ', get_mape(model, X_val, y_val, batch_size).mean())\n",
    "print('The MAPE on the test set is: ', get_mape(model, X_test, y_test, batch_size).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLLLqkX6BEle",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650895213820,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "sf8FmLxxBGcu",
    "outputId": "764313a9-78fc-46ca-dd08-9fb8834e4374",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the R^2 score is:  0.9477473351615887\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(X_test[0:batch_size].to(device)).squeeze().cpu().detach().numpy()\n",
    "\n",
    "y_true = y_test[0:batch_size].cpu().squeeze().detach().numpy()\n",
    "\n",
    "r2 = r2_score(y_pred=out, y_true=y_true)\n",
    "\n",
    "print('the R^2 score is: ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1650895214553,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "awvWu7WxBIHB",
    "outputId": "1b3dccd3-90ef-47d5-9895-267fdf93020c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 648x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.scatter(\n",
    "    y=output_sc.inverse_transform(out.reshape(-1, 1)),\n",
    "    x=output_sc.inverse_transform(y_true.squeeze().reshape(-1, 1))\n",
    ")\n",
    "ax.set_xlabel('Actual Value')\n",
    "ax.set_ylabel('Predicted Value')\n",
    "\n",
    "ax.text(20, 80, f'$R^2$ = {np.round(r2, 6)}', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model-Testing-Heston.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}