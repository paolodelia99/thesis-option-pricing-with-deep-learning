{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ju-WUaXnY5cr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Models Testing on Heston data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1650891681415,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "hH3-v8y-AuXg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650891681417,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "QXEzFQ3iAyj_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891681418,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "gsH02HCWA0gC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_options_path = '../data/heston_synthetic_options.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1650891681421,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "jT4n95T6A4S6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3413,
     "status": "ok",
     "timestamp": 1650891684820,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "7H8lEQPCA5IS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paolo/PycharmProjects/th-bot/venv/lib/python3.8/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "synthetic_options = pd.read_csv(synthetic_options_path, index_col=0)\n",
    "synthetic_options = reduce_mem_usage(synthetic_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1650891684821,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "9TKRgvuOA8pC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_options = shuffle(synthetic_options, random_state=0)\n",
    "synthetic_options = synthetic_options.reset_index()\n",
    "synthetic_options = synthetic_options.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1650891684822,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "3HXliAsq132p",
    "outputId": "09bc2eb1-d20d-4471-ed50-5fa0eba727fb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         Price  Strike Type     Kappa       Rho     Theta        Xi       V_0  \\\n0           96    78.0    C  1.074219 -0.311279  0.375732  0.180298  0.367432   \n1          100    56.0    C  1.495117 -0.869629  0.011398  0.010925  0.257080   \n2          100    68.0    P  0.988770 -0.664551  0.344482  0.045258  0.328125   \n3          100    75.0    C  1.996094 -0.051880  0.417480  0.208374  0.328613   \n4          100    71.0    P  0.344971 -0.344727  0.227783  0.340820  0.054413   \n...        ...     ...  ...       ...       ...       ...       ...       ...   \n1059740    100    71.0    P  1.039062 -0.757324  0.227661  0.312500  0.445068   \n1059741    100    57.0    C  1.905273 -0.276855  0.322266  0.061432  0.120117   \n1059742     92   132.0    P  0.149536 -0.433350  0.262695  0.054565  0.404541   \n1059743    100   135.0    C  0.971680 -0.711426  0.270508  0.438477  0.483887   \n1059744    100   135.0    P  1.183594 -0.541504  0.479980  0.091614  0.104736   \n\n         Interest Rate  Time to Expiration  Option Price  \n0             0.045074            0.533203     17.828125  \n1             0.050201            0.977051     42.187500  \n2             0.051971            0.588867      0.026703  \n3             0.053741            0.215088     25.312500  \n4             0.016891            0.800781      0.070496  \n...                ...                 ...           ...  \n1059740       0.016006            0.487061      1.505859  \n1059741       0.025070            0.947754     41.687500  \n1059742       0.028809            0.551758     38.375000  \n1059743       0.097107            0.289307      0.180542  \n1059744       0.095215            0.864746     32.093750  \n\n[1059745 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>Strike</th>\n      <th>Type</th>\n      <th>Kappa</th>\n      <th>Rho</th>\n      <th>Theta</th>\n      <th>Xi</th>\n      <th>V_0</th>\n      <th>Interest Rate</th>\n      <th>Time to Expiration</th>\n      <th>Option Price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>96</td>\n      <td>78.0</td>\n      <td>C</td>\n      <td>1.074219</td>\n      <td>-0.311279</td>\n      <td>0.375732</td>\n      <td>0.180298</td>\n      <td>0.367432</td>\n      <td>0.045074</td>\n      <td>0.533203</td>\n      <td>17.828125</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100</td>\n      <td>56.0</td>\n      <td>C</td>\n      <td>1.495117</td>\n      <td>-0.869629</td>\n      <td>0.011398</td>\n      <td>0.010925</td>\n      <td>0.257080</td>\n      <td>0.050201</td>\n      <td>0.977051</td>\n      <td>42.187500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100</td>\n      <td>68.0</td>\n      <td>P</td>\n      <td>0.988770</td>\n      <td>-0.664551</td>\n      <td>0.344482</td>\n      <td>0.045258</td>\n      <td>0.328125</td>\n      <td>0.051971</td>\n      <td>0.588867</td>\n      <td>0.026703</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100</td>\n      <td>75.0</td>\n      <td>C</td>\n      <td>1.996094</td>\n      <td>-0.051880</td>\n      <td>0.417480</td>\n      <td>0.208374</td>\n      <td>0.328613</td>\n      <td>0.053741</td>\n      <td>0.215088</td>\n      <td>25.312500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100</td>\n      <td>71.0</td>\n      <td>P</td>\n      <td>0.344971</td>\n      <td>-0.344727</td>\n      <td>0.227783</td>\n      <td>0.340820</td>\n      <td>0.054413</td>\n      <td>0.016891</td>\n      <td>0.800781</td>\n      <td>0.070496</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1059740</th>\n      <td>100</td>\n      <td>71.0</td>\n      <td>P</td>\n      <td>1.039062</td>\n      <td>-0.757324</td>\n      <td>0.227661</td>\n      <td>0.312500</td>\n      <td>0.445068</td>\n      <td>0.016006</td>\n      <td>0.487061</td>\n      <td>1.505859</td>\n    </tr>\n    <tr>\n      <th>1059741</th>\n      <td>100</td>\n      <td>57.0</td>\n      <td>C</td>\n      <td>1.905273</td>\n      <td>-0.276855</td>\n      <td>0.322266</td>\n      <td>0.061432</td>\n      <td>0.120117</td>\n      <td>0.025070</td>\n      <td>0.947754</td>\n      <td>41.687500</td>\n    </tr>\n    <tr>\n      <th>1059742</th>\n      <td>92</td>\n      <td>132.0</td>\n      <td>P</td>\n      <td>0.149536</td>\n      <td>-0.433350</td>\n      <td>0.262695</td>\n      <td>0.054565</td>\n      <td>0.404541</td>\n      <td>0.028809</td>\n      <td>0.551758</td>\n      <td>38.375000</td>\n    </tr>\n    <tr>\n      <th>1059743</th>\n      <td>100</td>\n      <td>135.0</td>\n      <td>C</td>\n      <td>0.971680</td>\n      <td>-0.711426</td>\n      <td>0.270508</td>\n      <td>0.438477</td>\n      <td>0.483887</td>\n      <td>0.097107</td>\n      <td>0.289307</td>\n      <td>0.180542</td>\n    </tr>\n    <tr>\n      <th>1059744</th>\n      <td>100</td>\n      <td>135.0</td>\n      <td>P</td>\n      <td>1.183594</td>\n      <td>-0.541504</td>\n      <td>0.479980</td>\n      <td>0.091614</td>\n      <td>0.104736</td>\n      <td>0.095215</td>\n      <td>0.864746</td>\n      <td>32.093750</td>\n    </tr>\n  </tbody>\n</table>\n<p>1059745 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pn28_RUMBAFH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1650891685322,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "32oPe6XUBCTF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_options = pd.get_dummies(synthetic_options, prefix='', prefix_sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1650891686346,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "_Tlc7k7xBDPV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_sc = StandardScaler()\n",
    "output_sc = StandardScaler()\n",
    "input_data = input_sc.fit_transform(synthetic_options.drop('Option Price', axis=1))\n",
    "output_data = output_sc.fit_transform(synthetic_options['Option Price'].values.reshape(-1, 1))\n",
    "\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "\n",
    "last_train_idx = int(np.round(len(input_data) * train_size))\n",
    "last_val_idx = last_train_idx + int(np.round(len(input_data) * val_size))\n",
    "\n",
    "X_train = input_data[0:last_train_idx]\n",
    "X_val = input_data[last_train_idx:last_val_idx]\n",
    "X_test = input_data[last_val_idx:]\n",
    "\n",
    "y_train = output_data[0:last_train_idx]\n",
    "y_val = output_data[last_train_idx:last_val_idx]\n",
    "y_test = output_data[last_val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650891686347,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "7nEEoQGvvDpL",
    "outputId": "88b4d863-d037-439b-e625-5ebb52ad41ef",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Price', 'Strike', 'Kappa', 'Rho', 'Theta', 'Xi', 'V_0',\n       'Interest Rate', 'Time to Expiration', 'C', 'P'],\n      dtype='object')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_options.drop('Option Price', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1650891686349,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "IwMVtvfABIhR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = Variable(torch.Tensor(X_train))\n",
    "X_val = Variable(torch.Tensor(X_val))\n",
    "X_test = Variable(torch.Tensor(X_test))\n",
    "\n",
    "y_train = Variable(torch.Tensor(y_train))\n",
    "y_val = Variable(torch.Tensor(y_val))\n",
    "y_test = Variable(torch.Tensor(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ra2l5P1nBVCz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891686350,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "xTLMLFoSBWDy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "device = 'cuda:0' if CUDA else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1650891686352,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "CQ9Gl3bUBWsj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, module):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.module = module\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.module(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891686353,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "YL3SicQPBYq0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, layer_size, act_fn):\n",
    "      super(HiddenLayer, self).__init__()\n",
    "      \n",
    "      if act_fn == 'ReLU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.ReLU())\n",
    "      elif act_fn == 'LeakyReLU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.LeakyReLU())\n",
    "      elif act_fn == 'ELU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.ELU())\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1650891686792,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "lHUFGf9xBawS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, output_size, hidden_size, num_layers, act_fn):\n",
    "    super(Net, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.output_size = output_size\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if act_fn == 'ReLU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ReLU())\n",
    "    elif act_fn == 'LeakyReLU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.LeakyReLU())\n",
    "    elif act_fn == 'ELU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ELU())\n",
    "\n",
    "    self.hidden_layers_list = []\n",
    "\n",
    "    for i in range(num_layers // 2):\n",
    "      self.hidden_layers_list.append(\n",
    "          ResBlock(\n",
    "            nn.Sequential(\n",
    "                HiddenLayer(self.hidden_size, act_fn),\n",
    "                HiddenLayer(self.hidden_size, act_fn)\n",
    "            )\n",
    "        )\n",
    "      )\n",
    "\n",
    "    self.hidden_layers = nn.Sequential(*self.hidden_layers_list)\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        self.initial_layer,\n",
    "        self.hidden_layers,\n",
    "        nn.Linear(self.hidden_size, self.output_size)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650891686793,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "zcq_lQrHBdH8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(m, init_m: str):\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_uniform(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.uniform_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_normal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.normal_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_xuniform(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.xavier_uniform_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_xnormal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.xavier_normal_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  if init_m == 'uniform':\n",
    "    m.apply(init_uniform)\n",
    "  elif init_m == 'normal':\n",
    "    m.apply(init_normal)\n",
    "  elif init_m == 'xaiver uniform':\n",
    "    m.apply(init_xuniform)\n",
    "  elif init_m == 'xavier normal':\n",
    "    m.apply(init_xnormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuCpyycNCKEZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650891686794,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "YbCOnCSNCL25",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 11\n",
    "output_size = 1\n",
    "num_layers = 4\n",
    "hidden_size = 600\n",
    "batch_size = 1141\n",
    "epochs = 2000\n",
    "lr = 0.00012243587926335812\n",
    "init_method = 'xaiver uniform'\n",
    "act_fn = 'LeakyReLU'\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers, act_fn)\n",
    "init_weights(model, init_method)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 7351,
     "status": "ok",
     "timestamp": 1650891694138,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "QqZbxrppvDpZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1650891694142,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "Q-9T0GArCMgp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class OptDataset(Dataset):\n",
    "\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X[idx], self.y[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6-hCH2ivDpb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Losses Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1650891694144,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "mVaO8TruHW4M",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def MAPELoss(output, target):\n",
    "  return torch.mean(torch.abs((target - output) / target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1650891694149,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "tVLW5dHhvDpe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, X_val, y_val):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X_val, y_val), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = loss_fn(out, batch_labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    print('\\nVal set: Average loss: {:.8f}\\n'.format(\n",
    "                losses.mean()))\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68eF5_EovDpf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Early Stopping class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650891694150,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "n2So2ffSvDpg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code took form: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, \n",
    "                 patience=10, \n",
    "                 verbose=False, \n",
    "                 delta=0, \n",
    "                 path='../models/final_heston_model.chkpt',\n",
    "                 trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL_xmnKXvDph",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1650891694152,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "DrBBTGKJvDph",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val\n",
    "):\n",
    "\n",
    "  training_losses = []\n",
    "  validation_losses = []\n",
    "\n",
    "  early_stopping = EarlyStopping(patience=20)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "\n",
    "    for batch, batch_labels in DataLoader(OptDataset(X_train, y_train), batch_size=batch_size):\n",
    "      out = model(batch.to(device))\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss = loss_fn(out, batch_labels.to(device))\n",
    "      epoch_losses.append(loss.item())\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if i > 0 and i % 50 == 0:\n",
    "        avg_loss = total_loss / 50\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| Epoch {:3d} | {:5d}/{:5d} batches | lr {:2.5f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.8f}'.format(\n",
    "              epoch, i, len(X_train) // batch_size+1, lr, elapsed * 1000 / 50,\n",
    "              avg_loss))\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "      i += 1\n",
    "\n",
    "    training_losses.append(np.array(epoch_losses).mean())\n",
    "    val_loss = evaluate(model, loss_fn, X_val, y_val)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Stopping at Epoch: {epoch}\")\n",
    "        break\n",
    "\n",
    "  return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2525796,
     "status": "ok",
     "timestamp": 1650894219916,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "hBETWoCfvDpj",
    "outputId": "30acf99f-3a1e-4d87-d1ff-13068ebe4cd4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 |    50/  737 batches | lr 0.00015 | ms/batch 26.76 | loss 0.21072952\n",
      "| Epoch   0 |   100/  737 batches | lr 0.00015 | ms/batch 17.19 | loss 0.01568603\n",
      "| Epoch   0 |   150/  737 batches | lr 0.00015 | ms/batch 18.63 | loss 0.00772913\n",
      "| Epoch   0 |   200/  737 batches | lr 0.00015 | ms/batch 18.49 | loss 0.00447331\n",
      "| Epoch   0 |   250/  737 batches | lr 0.00015 | ms/batch 20.27 | loss 0.00302323\n",
      "| Epoch   0 |   300/  737 batches | lr 0.00015 | ms/batch 20.08 | loss 0.00216396\n",
      "| Epoch   0 |   350/  737 batches | lr 0.00015 | ms/batch 19.87 | loss 0.00180316\n",
      "| Epoch   0 |   400/  737 batches | lr 0.00015 | ms/batch 18.29 | loss 0.00150843\n",
      "| Epoch   0 |   450/  737 batches | lr 0.00015 | ms/batch 20.36 | loss 0.00154597\n",
      "| Epoch   0 |   500/  737 batches | lr 0.00015 | ms/batch 19.80 | loss 0.00124896\n",
      "| Epoch   0 |   550/  737 batches | lr 0.00015 | ms/batch 19.83 | loss 0.00115438\n",
      "| Epoch   0 |   600/  737 batches | lr 0.00015 | ms/batch 19.20 | loss 0.00139101\n",
      "| Epoch   0 |   650/  737 batches | lr 0.00015 | ms/batch 19.87 | loss 0.00097346\n",
      "| Epoch   0 |   700/  737 batches | lr 0.00015 | ms/batch 18.92 | loss 0.00100417\n",
      "\n",
      "Val set: Average loss: 0.00079036\n",
      "\n",
      "| Epoch   1 |    50/  737 batches | lr 0.00015 | ms/batch 19.29 | loss 0.00091356\n",
      "| Epoch   1 |   100/  737 batches | lr 0.00015 | ms/batch 19.44 | loss 0.00119448\n",
      "| Epoch   1 |   150/  737 batches | lr 0.00015 | ms/batch 17.47 | loss 0.00076024\n",
      "| Epoch   1 |   200/  737 batches | lr 0.00015 | ms/batch 17.28 | loss 0.00090174\n",
      "| Epoch   1 |   250/  737 batches | lr 0.00015 | ms/batch 20.95 | loss 0.00102446\n",
      "| Epoch   1 |   300/  737 batches | lr 0.00015 | ms/batch 20.01 | loss 0.00066414\n",
      "| Epoch   1 |   350/  737 batches | lr 0.00015 | ms/batch 18.07 | loss 0.00066423\n",
      "| Epoch   1 |   400/  737 batches | lr 0.00015 | ms/batch 19.99 | loss 0.00073589\n",
      "| Epoch   1 |   450/  737 batches | lr 0.00015 | ms/batch 20.24 | loss 0.00083669\n",
      "| Epoch   1 |   500/  737 batches | lr 0.00015 | ms/batch 21.22 | loss 0.00082941\n",
      "| Epoch   1 |   550/  737 batches | lr 0.00015 | ms/batch 19.65 | loss 0.00065536\n",
      "| Epoch   1 |   600/  737 batches | lr 0.00015 | ms/batch 19.43 | loss 0.00063742\n",
      "| Epoch   1 |   650/  737 batches | lr 0.00015 | ms/batch 20.02 | loss 0.00079788\n",
      "| Epoch   1 |   700/  737 batches | lr 0.00015 | ms/batch 20.34 | loss 0.00067223\n",
      "\n",
      "Val set: Average loss: 0.00052351\n",
      "\n",
      "| Epoch   2 |    50/  737 batches | lr 0.00015 | ms/batch 20.51 | loss 0.00059750\n",
      "| Epoch   2 |   100/  737 batches | lr 0.00015 | ms/batch 20.02 | loss 0.00076185\n",
      "| Epoch   2 |   150/  737 batches | lr 0.00015 | ms/batch 20.84 | loss 0.00053625\n",
      "| Epoch   2 |   200/  737 batches | lr 0.00015 | ms/batch 17.56 | loss 0.00054686\n",
      "| Epoch   2 |   250/  737 batches | lr 0.00015 | ms/batch 17.58 | loss 0.00072905\n",
      "| Epoch   2 |   300/  737 batches | lr 0.00015 | ms/batch 18.58 | loss 0.00053697\n",
      "| Epoch   2 |   350/  737 batches | lr 0.00015 | ms/batch 16.83 | loss 0.00085216\n",
      "| Epoch   2 |   400/  737 batches | lr 0.00015 | ms/batch 18.26 | loss 0.00047408\n",
      "| Epoch   2 |   450/  737 batches | lr 0.00015 | ms/batch 18.54 | loss 0.00049600\n",
      "| Epoch   2 |   500/  737 batches | lr 0.00015 | ms/batch 20.86 | loss 0.00069259\n",
      "| Epoch   2 |   550/  737 batches | lr 0.00015 | ms/batch 19.23 | loss 0.00057257\n",
      "| Epoch   2 |   600/  737 batches | lr 0.00015 | ms/batch 18.69 | loss 0.00048091\n",
      "| Epoch   2 |   650/  737 batches | lr 0.00015 | ms/batch 18.54 | loss 0.00055787\n",
      "| Epoch   2 |   700/  737 batches | lr 0.00015 | ms/batch 18.45 | loss 0.00065009\n",
      "\n",
      "Val set: Average loss: 0.00042765\n",
      "\n",
      "| Epoch   3 |    50/  737 batches | lr 0.00015 | ms/batch 16.92 | loss 0.00043978\n",
      "| Epoch   3 |   100/  737 batches | lr 0.00015 | ms/batch 20.19 | loss 0.00062763\n",
      "| Epoch   3 |   150/  737 batches | lr 0.00015 | ms/batch 16.80 | loss 0.00043066\n",
      "| Epoch   3 |   200/  737 batches | lr 0.00015 | ms/batch 18.41 | loss 0.00046735\n",
      "| Epoch   3 |   250/  737 batches | lr 0.00015 | ms/batch 16.97 | loss 0.00060822\n",
      "| Epoch   3 |   300/  737 batches | lr 0.00015 | ms/batch 19.02 | loss 0.00044099\n",
      "| Epoch   3 |   350/  737 batches | lr 0.00015 | ms/batch 18.73 | loss 0.00051005\n",
      "| Epoch   3 |   400/  737 batches | lr 0.00015 | ms/batch 20.13 | loss 0.00061325\n",
      "| Epoch   3 |   450/  737 batches | lr 0.00015 | ms/batch 18.47 | loss 0.00042520\n",
      "| Epoch   3 |   500/  737 batches | lr 0.00015 | ms/batch 18.44 | loss 0.00054790\n",
      "| Epoch   3 |   550/  737 batches | lr 0.00015 | ms/batch 18.60 | loss 0.00046762\n",
      "| Epoch   3 |   600/  737 batches | lr 0.00015 | ms/batch 18.47 | loss 0.00041900\n",
      "| Epoch   3 |   650/  737 batches | lr 0.00015 | ms/batch 20.16 | loss 0.00057289\n",
      "| Epoch   3 |   700/  737 batches | lr 0.00015 | ms/batch 18.41 | loss 0.00071761\n",
      "\n",
      "Val set: Average loss: 0.00034127\n",
      "\n",
      "| Epoch   4 |    50/  737 batches | lr 0.00015 | ms/batch 18.52 | loss 0.00038011\n",
      "| Epoch   4 |   100/  737 batches | lr 0.00015 | ms/batch 18.46 | loss 0.00050750\n",
      "| Epoch   4 |   150/  737 batches | lr 0.00015 | ms/batch 18.50 | loss 0.00036822\n",
      "| Epoch   4 |   200/  737 batches | lr 0.00015 | ms/batch 16.86 | loss 0.00044349\n",
      "| Epoch   4 |   250/  737 batches | lr 0.00015 | ms/batch 18.45 | loss 0.00044966\n",
      "| Epoch   4 |   300/  737 batches | lr 0.00015 | ms/batch 18.60 | loss 0.00037923\n",
      "| Epoch   4 |   350/  737 batches | lr 0.00015 | ms/batch 18.66 | loss 0.00048564\n",
      "| Epoch   4 |   400/  737 batches | lr 0.00015 | ms/batch 16.83 | loss 0.00040979\n",
      "| Epoch   4 |   450/  737 batches | lr 0.00015 | ms/batch 16.86 | loss 0.00044136\n",
      "| Epoch   4 |   500/  737 batches | lr 0.00015 | ms/batch 19.99 | loss 0.00039244\n",
      "| Epoch   4 |   550/  737 batches | lr 0.00015 | ms/batch 18.61 | loss 0.00043300\n",
      "| Epoch   4 |   600/  737 batches | lr 0.00015 | ms/batch 16.69 | loss 0.00042094\n",
      "| Epoch   4 |   650/  737 batches | lr 0.00015 | ms/batch 16.75 | loss 0.00042478\n",
      "| Epoch   4 |   700/  737 batches | lr 0.00015 | ms/batch 18.37 | loss 0.00052130\n",
      "\n",
      "Val set: Average loss: 0.00036453\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch   5 |    50/  737 batches | lr 0.00015 | ms/batch 16.95 | loss 0.00035387\n",
      "| Epoch   5 |   100/  737 batches | lr 0.00015 | ms/batch 18.25 | loss 0.00051364\n",
      "| Epoch   5 |   150/  737 batches | lr 0.00015 | ms/batch 18.63 | loss 0.00032901\n",
      "| Epoch   5 |   200/  737 batches | lr 0.00015 | ms/batch 18.58 | loss 0.00042266\n",
      "| Epoch   5 |   250/  737 batches | lr 0.00015 | ms/batch 16.87 | loss 0.00040987\n",
      "| Epoch   5 |   300/  737 batches | lr 0.00015 | ms/batch 16.93 | loss 0.00035021\n",
      "| Epoch   5 |   350/  737 batches | lr 0.00015 | ms/batch 18.48 | loss 0.00057366\n",
      "| Epoch   5 |   400/  737 batches | lr 0.00015 | ms/batch 16.86 | loss 0.00033739\n",
      "| Epoch   5 |   450/  737 batches | lr 0.00015 | ms/batch 19.19 | loss 0.00040823\n",
      "| Epoch   5 |   500/  737 batches | lr 0.00015 | ms/batch 18.39 | loss 0.00039698\n",
      "| Epoch   5 |   550/  737 batches | lr 0.00015 | ms/batch 18.38 | loss 0.00034753\n",
      "| Epoch   5 |   600/  737 batches | lr 0.00015 | ms/batch 16.74 | loss 0.00038282\n",
      "| Epoch   5 |   650/  737 batches | lr 0.00015 | ms/batch 18.30 | loss 0.00061489\n",
      "| Epoch   5 |   700/  737 batches | lr 0.00015 | ms/batch 18.40 | loss 0.00036916\n",
      "\n",
      "Val set: Average loss: 0.00027892\n",
      "\n",
      "| Epoch   6 |    50/  737 batches | lr 0.00015 | ms/batch 18.74 | loss 0.00028902\n",
      "| Epoch   6 |   100/  737 batches | lr 0.00015 | ms/batch 18.55 | loss 0.00045303\n",
      "| Epoch   6 |   150/  737 batches | lr 0.00015 | ms/batch 17.00 | loss 0.00029048\n",
      "| Epoch   6 |   200/  737 batches | lr 0.00015 | ms/batch 16.78 | loss 0.00029203\n",
      "| Epoch   6 |   250/  737 batches | lr 0.00015 | ms/batch 20.26 | loss 0.00043677\n",
      "| Epoch   6 |   300/  737 batches | lr 0.00015 | ms/batch 18.79 | loss 0.00027568\n",
      "| Epoch   6 |   350/  737 batches | lr 0.00015 | ms/batch 16.92 | loss 0.00036127\n",
      "| Epoch   6 |   400/  737 batches | lr 0.00015 | ms/batch 16.72 | loss 0.00039918\n",
      "| Epoch   6 |   450/  737 batches | lr 0.00015 | ms/batch 20.14 | loss 0.00039233\n",
      "| Epoch   6 |   500/  737 batches | lr 0.00015 | ms/batch 16.79 | loss 0.00030176\n",
      "| Epoch   6 |   550/  737 batches | lr 0.00015 | ms/batch 18.25 | loss 0.00040811\n",
      "| Epoch   6 |   600/  737 batches | lr 0.00015 | ms/batch 16.75 | loss 0.00051926\n",
      "| Epoch   6 |   650/  737 batches | lr 0.00015 | ms/batch 18.51 | loss 0.00030728\n",
      "| Epoch   6 |   700/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00029739\n",
      "\n",
      "Val set: Average loss: 0.00027735\n",
      "\n",
      "| Epoch   7 |    50/  737 batches | lr 0.00015 | ms/batch 19.10 | loss 0.00026494\n",
      "| Epoch   7 |   100/  737 batches | lr 0.00015 | ms/batch 18.45 | loss 0.00048851\n",
      "| Epoch   7 |   150/  737 batches | lr 0.00015 | ms/batch 18.44 | loss 0.00026283\n",
      "| Epoch   7 |   200/  737 batches | lr 0.00015 | ms/batch 16.62 | loss 0.00030188\n",
      "| Epoch   7 |   250/  737 batches | lr 0.00015 | ms/batch 17.26 | loss 0.00038544\n",
      "| Epoch   7 |   300/  737 batches | lr 0.00015 | ms/batch 19.88 | loss 0.00025757\n",
      "| Epoch   7 |   350/  737 batches | lr 0.00015 | ms/batch 18.32 | loss 0.00049093\n",
      "| Epoch   7 |   400/  737 batches | lr 0.00015 | ms/batch 17.04 | loss 0.00031591\n",
      "| Epoch   7 |   450/  737 batches | lr 0.00015 | ms/batch 17.15 | loss 0.00029499\n",
      "| Epoch   7 |   500/  737 batches | lr 0.00015 | ms/batch 20.99 | loss 0.00023998\n",
      "| Epoch   7 |   550/  737 batches | lr 0.00015 | ms/batch 20.40 | loss 0.00036423\n",
      "| Epoch   7 |   600/  737 batches | lr 0.00015 | ms/batch 19.73 | loss 0.00040072\n",
      "| Epoch   7 |   650/  737 batches | lr 0.00015 | ms/batch 19.80 | loss 0.00026138\n",
      "| Epoch   7 |   700/  737 batches | lr 0.00015 | ms/batch 17.90 | loss 0.00031959\n",
      "\n",
      "Val set: Average loss: 0.00025926\n",
      "\n",
      "| Epoch   8 |    50/  737 batches | lr 0.00015 | ms/batch 19.16 | loss 0.00034505\n",
      "| Epoch   8 |   100/  737 batches | lr 0.00015 | ms/batch 20.04 | loss 0.00031299\n",
      "| Epoch   8 |   150/  737 batches | lr 0.00015 | ms/batch 17.98 | loss 0.00036007\n",
      "| Epoch   8 |   200/  737 batches | lr 0.00015 | ms/batch 20.05 | loss 0.00031962\n",
      "| Epoch   8 |   250/  737 batches | lr 0.00015 | ms/batch 20.16 | loss 0.00059436\n",
      "| Epoch   8 |   300/  737 batches | lr 0.00015 | ms/batch 19.63 | loss 0.00029401\n",
      "| Epoch   8 |   350/  737 batches | lr 0.00015 | ms/batch 17.26 | loss 0.00025973\n",
      "| Epoch   8 |   400/  737 batches | lr 0.00015 | ms/batch 19.65 | loss 0.00025090\n",
      "| Epoch   8 |   450/  737 batches | lr 0.00015 | ms/batch 19.23 | loss 0.00021201\n",
      "| Epoch   8 |   500/  737 batches | lr 0.00015 | ms/batch 19.29 | loss 0.00021959\n",
      "| Epoch   8 |   550/  737 batches | lr 0.00015 | ms/batch 17.25 | loss 0.00030498\n",
      "| Epoch   8 |   600/  737 batches | lr 0.00015 | ms/batch 19.15 | loss 0.00032281\n",
      "| Epoch   8 |   650/  737 batches | lr 0.00015 | ms/batch 20.34 | loss 0.00021299\n",
      "| Epoch   8 |   700/  737 batches | lr 0.00015 | ms/batch 19.92 | loss 0.00042991\n",
      "\n",
      "Val set: Average loss: 0.00020396\n",
      "\n",
      "| Epoch   9 |    50/  737 batches | lr 0.00015 | ms/batch 19.90 | loss 0.00023158\n",
      "| Epoch   9 |   100/  737 batches | lr 0.00015 | ms/batch 18.09 | loss 0.00038978\n",
      "| Epoch   9 |   150/  737 batches | lr 0.00015 | ms/batch 19.21 | loss 0.00021952\n",
      "| Epoch   9 |   200/  737 batches | lr 0.00015 | ms/batch 19.23 | loss 0.00029994\n",
      "| Epoch   9 |   250/  737 batches | lr 0.00015 | ms/batch 19.18 | loss 0.00030433\n",
      "| Epoch   9 |   300/  737 batches | lr 0.00015 | ms/batch 17.32 | loss 0.00020623\n",
      "| Epoch   9 |   350/  737 batches | lr 0.00015 | ms/batch 18.14 | loss 0.00035112\n",
      "| Epoch   9 |   400/  737 batches | lr 0.00015 | ms/batch 22.34 | loss 0.00027152\n",
      "| Epoch   9 |   450/  737 batches | lr 0.00015 | ms/batch 18.03 | loss 0.00022864\n",
      "| Epoch   9 |   500/  737 batches | lr 0.00015 | ms/batch 19.16 | loss 0.00032422\n",
      "| Epoch   9 |   550/  737 batches | lr 0.00015 | ms/batch 19.20 | loss 0.00024765\n",
      "| Epoch   9 |   600/  737 batches | lr 0.00015 | ms/batch 22.26 | loss 0.00029464\n",
      "| Epoch   9 |   650/  737 batches | lr 0.00015 | ms/batch 18.73 | loss 0.00019373\n",
      "| Epoch   9 |   700/  737 batches | lr 0.00015 | ms/batch 19.21 | loss 0.00036500\n",
      "\n",
      "Val set: Average loss: 0.00018211\n",
      "\n",
      "| Epoch  10 |    50/  737 batches | lr 0.00015 | ms/batch 18.83 | loss 0.00018731\n",
      "| Epoch  10 |   100/  737 batches | lr 0.00015 | ms/batch 18.72 | loss 0.00045066\n",
      "| Epoch  10 |   150/  737 batches | lr 0.00015 | ms/batch 20.02 | loss 0.00022280\n",
      "| Epoch  10 |   200/  737 batches | lr 0.00015 | ms/batch 18.94 | loss 0.00029207\n",
      "| Epoch  10 |   250/  737 batches | lr 0.00015 | ms/batch 18.58 | loss 0.00055066\n",
      "| Epoch  10 |   300/  737 batches | lr 0.00015 | ms/batch 18.64 | loss 0.00020116\n",
      "| Epoch  10 |   350/  737 batches | lr 0.00015 | ms/batch 18.80 | loss 0.00020088\n",
      "| Epoch  10 |   400/  737 batches | lr 0.00015 | ms/batch 20.31 | loss 0.00021765\n",
      "| Epoch  10 |   450/  737 batches | lr 0.00015 | ms/batch 18.72 | loss 0.00018577\n",
      "| Epoch  10 |   500/  737 batches | lr 0.00015 | ms/batch 18.44 | loss 0.00028514\n",
      "| Epoch  10 |   550/  737 batches | lr 0.00015 | ms/batch 18.92 | loss 0.00036993\n",
      "| Epoch  10 |   600/  737 batches | lr 0.00015 | ms/batch 18.94 | loss 0.00030116\n",
      "| Epoch  10 |   650/  737 batches | lr 0.00015 | ms/batch 20.32 | loss 0.00019087\n",
      "| Epoch  10 |   700/  737 batches | lr 0.00015 | ms/batch 18.36 | loss 0.00019568\n",
      "\n",
      "Val set: Average loss: 0.00016563\n",
      "\n",
      "| Epoch  11 |    50/  737 batches | lr 0.00015 | ms/batch 20.52 | loss 0.00022393\n",
      "| Epoch  11 |   100/  737 batches | lr 0.00015 | ms/batch 18.42 | loss 0.00030828\n",
      "| Epoch  11 |   150/  737 batches | lr 0.00015 | ms/batch 18.80 | loss 0.00019968\n",
      "| Epoch  11 |   200/  737 batches | lr 0.00015 | ms/batch 18.47 | loss 0.00023057\n",
      "| Epoch  11 |   250/  737 batches | lr 0.00015 | ms/batch 18.58 | loss 0.00023137\n",
      "| Epoch  11 |   300/  737 batches | lr 0.00015 | ms/batch 18.76 | loss 0.00023176\n",
      "| Epoch  11 |   350/  737 batches | lr 0.00015 | ms/batch 20.65 | loss 0.00034606\n",
      "| Epoch  11 |   400/  737 batches | lr 0.00015 | ms/batch 18.85 | loss 0.00023538\n",
      "| Epoch  11 |   450/  737 batches | lr 0.00015 | ms/batch 18.55 | loss 0.00016693\n",
      "| Epoch  11 |   500/  737 batches | lr 0.00015 | ms/batch 18.72 | loss 0.00021780\n",
      "| Epoch  11 |   550/  737 batches | lr 0.00015 | ms/batch 18.56 | loss 0.00026996\n",
      "| Epoch  11 |   600/  737 batches | lr 0.00015 | ms/batch 20.05 | loss 0.00018417\n",
      "| Epoch  11 |   650/  737 batches | lr 0.00015 | ms/batch 18.45 | loss 0.00023545\n",
      "| Epoch  11 |   700/  737 batches | lr 0.00015 | ms/batch 18.44 | loss 0.00028352\n",
      "\n",
      "Val set: Average loss: 0.00018372\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  12 |    50/  737 batches | lr 0.00015 | ms/batch 18.77 | loss 0.00020313\n",
      "| Epoch  12 |   100/  737 batches | lr 0.00015 | ms/batch 18.36 | loss 0.00043668\n",
      "| Epoch  12 |   150/  737 batches | lr 0.00015 | ms/batch 16.66 | loss 0.00019232\n",
      "| Epoch  12 |   200/  737 batches | lr 0.00015 | ms/batch 16.74 | loss 0.00034743\n",
      "| Epoch  12 |   250/  737 batches | lr 0.00015 | ms/batch 20.08 | loss 0.00024397\n",
      "| Epoch  12 |   300/  737 batches | lr 0.00015 | ms/batch 18.25 | loss 0.00026811\n",
      "| Epoch  12 |   350/  737 batches | lr 0.00015 | ms/batch 16.61 | loss 0.00020189\n",
      "| Epoch  12 |   400/  737 batches | lr 0.00015 | ms/batch 17.63 | loss 0.00021796\n",
      "| Epoch  12 |   450/  737 batches | lr 0.00015 | ms/batch 22.50 | loss 0.00015561\n",
      "| Epoch  12 |   500/  737 batches | lr 0.00015 | ms/batch 19.22 | loss 0.00017276\n",
      "| Epoch  12 |   550/  737 batches | lr 0.00015 | ms/batch 20.33 | loss 0.00029225\n",
      "| Epoch  12 |   600/  737 batches | lr 0.00015 | ms/batch 21.48 | loss 0.00025270\n",
      "| Epoch  12 |   650/  737 batches | lr 0.00015 | ms/batch 18.51 | loss 0.00017011\n",
      "| Epoch  12 |   700/  737 batches | lr 0.00015 | ms/batch 20.05 | loss 0.00022553\n",
      "\n",
      "Val set: Average loss: 0.00018674\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  13 |    50/  737 batches | lr 0.00015 | ms/batch 18.67 | loss 0.00018722\n",
      "| Epoch  13 |   100/  737 batches | lr 0.00015 | ms/batch 17.85 | loss 0.00035366\n",
      "| Epoch  13 |   150/  737 batches | lr 0.00015 | ms/batch 21.21 | loss 0.00015046\n",
      "| Epoch  13 |   200/  737 batches | lr 0.00015 | ms/batch 17.72 | loss 0.00022428\n",
      "| Epoch  13 |   250/  737 batches | lr 0.00015 | ms/batch 18.72 | loss 0.00034513\n",
      "| Epoch  13 |   300/  737 batches | lr 0.00015 | ms/batch 17.99 | loss 0.00022509\n",
      "| Epoch  13 |   350/  737 batches | lr 0.00015 | ms/batch 19.20 | loss 0.00018575\n",
      "| Epoch  13 |   400/  737 batches | lr 0.00015 | ms/batch 20.62 | loss 0.00016455\n",
      "| Epoch  13 |   450/  737 batches | lr 0.00015 | ms/batch 19.37 | loss 0.00015619\n",
      "| Epoch  13 |   500/  737 batches | lr 0.00015 | ms/batch 19.27 | loss 0.00015568\n",
      "| Epoch  13 |   550/  737 batches | lr 0.00015 | ms/batch 19.23 | loss 0.00024769\n",
      "| Epoch  13 |   600/  737 batches | lr 0.00015 | ms/batch 19.15 | loss 0.00016546\n",
      "| Epoch  13 |   650/  737 batches | lr 0.00015 | ms/batch 19.35 | loss 0.00019781\n",
      "| Epoch  13 |   700/  737 batches | lr 0.00015 | ms/batch 21.02 | loss 0.00029116\n",
      "\n",
      "Val set: Average loss: 0.00016332\n",
      "\n",
      "| Epoch  14 |    50/  737 batches | lr 0.00015 | ms/batch 20.24 | loss 0.00017091\n",
      "| Epoch  14 |   100/  737 batches | lr 0.00015 | ms/batch 20.14 | loss 0.00023882\n",
      "| Epoch  14 |   150/  737 batches | lr 0.00015 | ms/batch 18.19 | loss 0.00025209\n",
      "| Epoch  14 |   200/  737 batches | lr 0.00015 | ms/batch 17.75 | loss 0.00034419\n",
      "| Epoch  14 |   250/  737 batches | lr 0.00015 | ms/batch 21.65 | loss 0.00021304\n",
      "| Epoch  14 |   300/  737 batches | lr 0.00015 | ms/batch 19.62 | loss 0.00026418\n",
      "| Epoch  14 |   350/  737 batches | lr 0.00015 | ms/batch 17.78 | loss 0.00021720\n",
      "| Epoch  14 |   400/  737 batches | lr 0.00015 | ms/batch 18.01 | loss 0.00014398\n",
      "| Epoch  14 |   450/  737 batches | lr 0.00015 | ms/batch 20.58 | loss 0.00013989\n",
      "| Epoch  14 |   500/  737 batches | lr 0.00015 | ms/batch 18.52 | loss 0.00014938\n",
      "| Epoch  14 |   550/  737 batches | lr 0.00015 | ms/batch 18.60 | loss 0.00020434\n",
      "| Epoch  14 |   600/  737 batches | lr 0.00015 | ms/batch 20.20 | loss 0.00025700\n",
      "| Epoch  14 |   650/  737 batches | lr 0.00015 | ms/batch 18.19 | loss 0.00014221\n",
      "| Epoch  14 |   700/  737 batches | lr 0.00015 | ms/batch 19.28 | loss 0.00021205\n",
      "\n",
      "Val set: Average loss: 0.00024334\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  15 |    50/  737 batches | lr 0.00015 | ms/batch 18.22 | loss 0.00030192\n",
      "| Epoch  15 |   100/  737 batches | lr 0.00015 | ms/batch 17.04 | loss 0.00019218\n",
      "| Epoch  15 |   150/  737 batches | lr 0.00015 | ms/batch 18.98 | loss 0.00014341\n",
      "| Epoch  15 |   200/  737 batches | lr 0.00015 | ms/batch 18.00 | loss 0.00021088\n",
      "| Epoch  15 |   250/  737 batches | lr 0.00015 | ms/batch 20.02 | loss 0.00019143\n",
      "| Epoch  15 |   300/  737 batches | lr 0.00015 | ms/batch 19.21 | loss 0.00017392\n",
      "| Epoch  15 |   350/  737 batches | lr 0.00015 | ms/batch 19.32 | loss 0.00015582\n",
      "| Epoch  15 |   400/  737 batches | lr 0.00015 | ms/batch 18.27 | loss 0.00012539\n",
      "| Epoch  15 |   450/  737 batches | lr 0.00015 | ms/batch 20.97 | loss 0.00028110\n",
      "| Epoch  15 |   500/  737 batches | lr 0.00015 | ms/batch 19.48 | loss 0.00021071\n",
      "| Epoch  15 |   550/  737 batches | lr 0.00015 | ms/batch 20.96 | loss 0.00024866\n",
      "| Epoch  15 |   600/  737 batches | lr 0.00015 | ms/batch 19.51 | loss 0.00023863\n",
      "| Epoch  15 |   650/  737 batches | lr 0.00015 | ms/batch 18.81 | loss 0.00013513\n",
      "| Epoch  15 |   700/  737 batches | lr 0.00015 | ms/batch 22.75 | loss 0.00019522\n",
      "\n",
      "Val set: Average loss: 0.00025232\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  16 |    50/  737 batches | lr 0.00015 | ms/batch 19.30 | loss 0.00018391\n",
      "| Epoch  16 |   100/  737 batches | lr 0.00015 | ms/batch 20.87 | loss 0.00020258\n",
      "| Epoch  16 |   150/  737 batches | lr 0.00015 | ms/batch 20.70 | loss 0.00013672\n",
      "| Epoch  16 |   200/  737 batches | lr 0.00015 | ms/batch 23.57 | loss 0.00013489\n",
      "| Epoch  16 |   250/  737 batches | lr 0.00015 | ms/batch 18.99 | loss 0.00019901\n",
      "| Epoch  16 |   300/  737 batches | lr 0.00015 | ms/batch 17.89 | loss 0.00015242\n",
      "| Epoch  16 |   350/  737 batches | lr 0.00015 | ms/batch 21.15 | loss 0.00022378\n",
      "| Epoch  16 |   400/  737 batches | lr 0.00015 | ms/batch 18.54 | loss 0.00012073\n",
      "| Epoch  16 |   450/  737 batches | lr 0.00015 | ms/batch 19.71 | loss 0.00051161\n",
      "| Epoch  16 |   500/  737 batches | lr 0.00015 | ms/batch 20.91 | loss 0.00024034\n",
      "| Epoch  16 |   550/  737 batches | lr 0.00015 | ms/batch 23.67 | loss 0.00018971\n",
      "| Epoch  16 |   600/  737 batches | lr 0.00015 | ms/batch 20.71 | loss 0.00015463\n",
      "| Epoch  16 |   650/  737 batches | lr 0.00015 | ms/batch 18.70 | loss 0.00014358\n",
      "| Epoch  16 |   700/  737 batches | lr 0.00015 | ms/batch 18.95 | loss 0.00018442\n",
      "\n",
      "Val set: Average loss: 0.00016735\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  17 |    50/  737 batches | lr 0.00015 | ms/batch 19.77 | loss 0.00015752\n",
      "| Epoch  17 |   100/  737 batches | lr 0.00015 | ms/batch 20.13 | loss 0.00019908\n",
      "| Epoch  17 |   150/  737 batches | lr 0.00015 | ms/batch 22.22 | loss 0.00015684\n",
      "| Epoch  17 |   200/  737 batches | lr 0.00015 | ms/batch 19.82 | loss 0.00020524\n",
      "| Epoch  17 |   250/  737 batches | lr 0.00015 | ms/batch 21.65 | loss 0.00018592\n",
      "| Epoch  17 |   300/  737 batches | lr 0.00015 | ms/batch 21.54 | loss 0.00024211\n",
      "| Epoch  17 |   350/  737 batches | lr 0.00015 | ms/batch 20.41 | loss 0.00015971\n",
      "| Epoch  17 |   400/  737 batches | lr 0.00015 | ms/batch 18.05 | loss 0.00011902\n",
      "| Epoch  17 |   450/  737 batches | lr 0.00015 | ms/batch 18.93 | loss 0.00018360\n",
      "| Epoch  17 |   500/  737 batches | lr 0.00015 | ms/batch 21.80 | loss 0.00018061\n",
      "| Epoch  17 |   550/  737 batches | lr 0.00015 | ms/batch 20.57 | loss 0.00018111\n",
      "| Epoch  17 |   600/  737 batches | lr 0.00015 | ms/batch 18.62 | loss 0.00014625\n",
      "| Epoch  17 |   650/  737 batches | lr 0.00015 | ms/batch 17.01 | loss 0.00013127\n",
      "| Epoch  17 |   700/  737 batches | lr 0.00015 | ms/batch 21.24 | loss 0.00019828\n",
      "\n",
      "Val set: Average loss: 0.00013394\n",
      "\n",
      "| Epoch  18 |    50/  737 batches | lr 0.00015 | ms/batch 19.64 | loss 0.00029625\n",
      "| Epoch  18 |   100/  737 batches | lr 0.00015 | ms/batch 19.62 | loss 0.00016099\n",
      "| Epoch  18 |   150/  737 batches | lr 0.00015 | ms/batch 17.86 | loss 0.00012515\n",
      "| Epoch  18 |   200/  737 batches | lr 0.00015 | ms/batch 17.95 | loss 0.00019286\n",
      "| Epoch  18 |   250/  737 batches | lr 0.00015 | ms/batch 22.55 | loss 0.00015039\n",
      "| Epoch  18 |   300/  737 batches | lr 0.00015 | ms/batch 21.27 | loss 0.00020314\n",
      "| Epoch  18 |   350/  737 batches | lr 0.00015 | ms/batch 18.34 | loss 0.00012749\n",
      "| Epoch  18 |   400/  737 batches | lr 0.00015 | ms/batch 18.39 | loss 0.00009885\n",
      "| Epoch  18 |   450/  737 batches | lr 0.00015 | ms/batch 22.23 | loss 0.00019147\n",
      "| Epoch  18 |   500/  737 batches | lr 0.00015 | ms/batch 18.03 | loss 0.00014314\n",
      "| Epoch  18 |   550/  737 batches | lr 0.00015 | ms/batch 18.92 | loss 0.00015145\n",
      "| Epoch  18 |   600/  737 batches | lr 0.00015 | ms/batch 19.99 | loss 0.00013110\n",
      "| Epoch  18 |   650/  737 batches | lr 0.00015 | ms/batch 18.79 | loss 0.00018338\n",
      "| Epoch  18 |   700/  737 batches | lr 0.00015 | ms/batch 20.02 | loss 0.00013484\n",
      "\n",
      "Val set: Average loss: 0.00036080\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  19 |    50/  737 batches | lr 0.00015 | ms/batch 18.77 | loss 0.00014864\n",
      "| Epoch  19 |   100/  737 batches | lr 0.00015 | ms/batch 18.44 | loss 0.00023577\n",
      "| Epoch  19 |   150/  737 batches | lr 0.00015 | ms/batch 21.49 | loss 0.00017158\n",
      "| Epoch  19 |   200/  737 batches | lr 0.00015 | ms/batch 18.74 | loss 0.00019346\n",
      "| Epoch  19 |   250/  737 batches | lr 0.00015 | ms/batch 20.16 | loss 0.00011108\n",
      "| Epoch  19 |   300/  737 batches | lr 0.00015 | ms/batch 18.47 | loss 0.00017342\n",
      "| Epoch  19 |   350/  737 batches | lr 0.00015 | ms/batch 19.33 | loss 0.00019658\n",
      "| Epoch  19 |   400/  737 batches | lr 0.00015 | ms/batch 22.04 | loss 0.00010566\n",
      "| Epoch  19 |   450/  737 batches | lr 0.00015 | ms/batch 19.98 | loss 0.00017325\n",
      "| Epoch  19 |   500/  737 batches | lr 0.00015 | ms/batch 20.73 | loss 0.00015766\n",
      "| Epoch  19 |   550/  737 batches | lr 0.00015 | ms/batch 19.26 | loss 0.00016864\n",
      "| Epoch  19 |   600/  737 batches | lr 0.00015 | ms/batch 20.02 | loss 0.00016796\n",
      "| Epoch  19 |   650/  737 batches | lr 0.00015 | ms/batch 20.14 | loss 0.00014201\n",
      "| Epoch  19 |   700/  737 batches | lr 0.00015 | ms/batch 21.04 | loss 0.00016173\n",
      "\n",
      "Val set: Average loss: 0.00012833\n",
      "\n",
      "| Epoch  20 |    50/  737 batches | lr 0.00015 | ms/batch 19.84 | loss 0.00011584\n",
      "| Epoch  20 |   100/  737 batches | lr 0.00015 | ms/batch 17.21 | loss 0.00032202\n",
      "| Epoch  20 |   150/  737 batches | lr 0.00015 | ms/batch 19.43 | loss 0.00012392\n",
      "| Epoch  20 |   200/  737 batches | lr 0.00015 | ms/batch 20.72 | loss 0.00020002\n",
      "| Epoch  20 |   250/  737 batches | lr 0.00015 | ms/batch 19.35 | loss 0.00014371\n",
      "| Epoch  20 |   300/  737 batches | lr 0.00015 | ms/batch 19.31 | loss 0.00025128\n",
      "| Epoch  20 |   350/  737 batches | lr 0.00015 | ms/batch 19.22 | loss 0.00020776\n",
      "| Epoch  20 |   400/  737 batches | lr 0.00015 | ms/batch 19.30 | loss 0.00010093\n",
      "| Epoch  20 |   450/  737 batches | lr 0.00015 | ms/batch 19.24 | loss 0.00012461\n",
      "| Epoch  20 |   500/  737 batches | lr 0.00015 | ms/batch 20.90 | loss 0.00024972\n",
      "| Epoch  20 |   550/  737 batches | lr 0.00015 | ms/batch 19.18 | loss 0.00019263\n",
      "| Epoch  20 |   600/  737 batches | lr 0.00015 | ms/batch 19.07 | loss 0.00014154\n",
      "| Epoch  20 |   650/  737 batches | lr 0.00015 | ms/batch 19.13 | loss 0.00013005\n",
      "| Epoch  20 |   700/  737 batches | lr 0.00015 | ms/batch 19.10 | loss 0.00015513\n",
      "\n",
      "Val set: Average loss: 0.00010321\n",
      "\n",
      "| Epoch  21 |    50/  737 batches | lr 0.00015 | ms/batch 17.85 | loss 0.00016341\n",
      "| Epoch  21 |   100/  737 batches | lr 0.00015 | ms/batch 20.40 | loss 0.00016582\n",
      "| Epoch  21 |   150/  737 batches | lr 0.00015 | ms/batch 17.34 | loss 0.00010452\n",
      "| Epoch  21 |   200/  737 batches | lr 0.00015 | ms/batch 19.00 | loss 0.00014575\n",
      "| Epoch  21 |   250/  737 batches | lr 0.00015 | ms/batch 17.68 | loss 0.00008883\n",
      "| Epoch  21 |   300/  737 batches | lr 0.00015 | ms/batch 19.12 | loss 0.00015017\n",
      "| Epoch  21 |   350/  737 batches | lr 0.00015 | ms/batch 20.87 | loss 0.00015678\n",
      "| Epoch  21 |   400/  737 batches | lr 0.00015 | ms/batch 19.38 | loss 0.00008276\n",
      "| Epoch  21 |   450/  737 batches | lr 0.00015 | ms/batch 19.19 | loss 0.00015002\n",
      "| Epoch  21 |   500/  737 batches | lr 0.00015 | ms/batch 19.00 | loss 0.00012351\n",
      "| Epoch  21 |   550/  737 batches | lr 0.00015 | ms/batch 19.03 | loss 0.00026327\n",
      "| Epoch  21 |   600/  737 batches | lr 0.00015 | ms/batch 19.13 | loss 0.00010636\n",
      "| Epoch  21 |   650/  737 batches | lr 0.00015 | ms/batch 20.81 | loss 0.00009387\n",
      "| Epoch  21 |   700/  737 batches | lr 0.00015 | ms/batch 18.88 | loss 0.00019843\n",
      "\n",
      "Val set: Average loss: 0.00015574\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  22 |    50/  737 batches | lr 0.00015 | ms/batch 19.11 | loss 0.00010278\n",
      "| Epoch  22 |   100/  737 batches | lr 0.00015 | ms/batch 17.30 | loss 0.00028191\n",
      "| Epoch  22 |   150/  737 batches | lr 0.00015 | ms/batch 18.96 | loss 0.00016510\n",
      "| Epoch  22 |   200/  737 batches | lr 0.00015 | ms/batch 18.98 | loss 0.00022014\n",
      "| Epoch  22 |   250/  737 batches | lr 0.00015 | ms/batch 18.85 | loss 0.00032713\n",
      "| Epoch  22 |   300/  737 batches | lr 0.00015 | ms/batch 17.30 | loss 0.00015797\n",
      "| Epoch  22 |   350/  737 batches | lr 0.00015 | ms/batch 18.91 | loss 0.00012047\n",
      "| Epoch  22 |   400/  737 batches | lr 0.00015 | ms/batch 18.97 | loss 0.00009714\n",
      "| Epoch  22 |   450/  737 batches | lr 0.00015 | ms/batch 18.96 | loss 0.00011157\n",
      "| Epoch  22 |   500/  737 batches | lr 0.00015 | ms/batch 17.18 | loss 0.00019673\n",
      "| Epoch  22 |   550/  737 batches | lr 0.00015 | ms/batch 17.15 | loss 0.00014260\n",
      "| Epoch  22 |   600/  737 batches | lr 0.00015 | ms/batch 21.49 | loss 0.00011163\n",
      "| Epoch  22 |   650/  737 batches | lr 0.00015 | ms/batch 19.05 | loss 0.00010061\n",
      "| Epoch  22 |   700/  737 batches | lr 0.00015 | ms/batch 17.24 | loss 0.00017676\n",
      "\n",
      "Val set: Average loss: 0.00008959\n",
      "\n",
      "| Epoch  23 |    50/  737 batches | lr 0.00015 | ms/batch 19.86 | loss 0.00013101\n",
      "| Epoch  23 |   100/  737 batches | lr 0.00015 | ms/batch 17.20 | loss 0.00011464\n",
      "| Epoch  23 |   150/  737 batches | lr 0.00015 | ms/batch 19.08 | loss 0.00011094\n",
      "| Epoch  23 |   200/  737 batches | lr 0.00015 | ms/batch 18.74 | loss 0.00010097\n",
      "| Epoch  23 |   250/  737 batches | lr 0.00015 | ms/batch 18.83 | loss 0.00011659\n",
      "| Epoch  23 |   300/  737 batches | lr 0.00015 | ms/batch 17.12 | loss 0.00011640\n",
      "| Epoch  23 |   350/  737 batches | lr 0.00015 | ms/batch 19.11 | loss 0.00023626\n",
      "| Epoch  23 |   400/  737 batches | lr 0.00015 | ms/batch 19.03 | loss 0.00007981\n",
      "| Epoch  23 |   450/  737 batches | lr 0.00015 | ms/batch 18.78 | loss 0.00013703\n",
      "| Epoch  23 |   500/  737 batches | lr 0.00015 | ms/batch 17.26 | loss 0.00013271\n",
      "| Epoch  23 |   550/  737 batches | lr 0.00015 | ms/batch 18.82 | loss 0.00014835\n",
      "| Epoch  23 |   600/  737 batches | lr 0.00015 | ms/batch 19.37 | loss 0.00009909\n",
      "| Epoch  23 |   650/  737 batches | lr 0.00015 | ms/batch 20.26 | loss 0.00015951\n",
      "| Epoch  23 |   700/  737 batches | lr 0.00015 | ms/batch 17.50 | loss 0.00021894\n",
      "\n",
      "Val set: Average loss: 0.00009672\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  24 |    50/  737 batches | lr 0.00015 | ms/batch 22.34 | loss 0.00012361\n",
      "| Epoch  24 |   100/  737 batches | lr 0.00015 | ms/batch 20.61 | loss 0.00012659\n",
      "| Epoch  24 |   150/  737 batches | lr 0.00015 | ms/batch 19.47 | loss 0.00015366\n",
      "| Epoch  24 |   200/  737 batches | lr 0.00015 | ms/batch 19.34 | loss 0.00016415\n",
      "| Epoch  24 |   250/  737 batches | lr 0.00015 | ms/batch 19.07 | loss 0.00009745\n",
      "| Epoch  24 |   300/  737 batches | lr 0.00015 | ms/batch 21.04 | loss 0.00019040\n",
      "| Epoch  24 |   350/  737 batches | lr 0.00015 | ms/batch 19.01 | loss 0.00016478\n",
      "| Epoch  24 |   400/  737 batches | lr 0.00015 | ms/batch 20.21 | loss 0.00008904\n",
      "| Epoch  24 |   450/  737 batches | lr 0.00015 | ms/batch 19.45 | loss 0.00014477\n",
      "| Epoch  24 |   500/  737 batches | lr 0.00015 | ms/batch 20.08 | loss 0.00012093\n",
      "| Epoch  24 |   550/  737 batches | lr 0.00015 | ms/batch 22.15 | loss 0.00014151\n",
      "| Epoch  24 |   600/  737 batches | lr 0.00015 | ms/batch 20.85 | loss 0.00009188\n",
      "| Epoch  24 |   650/  737 batches | lr 0.00015 | ms/batch 19.53 | loss 0.00011000\n",
      "| Epoch  24 |   700/  737 batches | lr 0.00015 | ms/batch 19.92 | loss 0.00020380\n",
      "\n",
      "Val set: Average loss: 0.00009185\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  25 |    50/  737 batches | lr 0.00015 | ms/batch 20.87 | loss 0.00011414\n",
      "| Epoch  25 |   100/  737 batches | lr 0.00015 | ms/batch 19.55 | loss 0.00011266\n",
      "| Epoch  25 |   150/  737 batches | lr 0.00015 | ms/batch 19.54 | loss 0.00012701\n",
      "| Epoch  25 |   200/  737 batches | lr 0.00015 | ms/batch 17.19 | loss 0.00010661\n",
      "| Epoch  25 |   250/  737 batches | lr 0.00015 | ms/batch 19.15 | loss 0.00011376\n",
      "| Epoch  25 |   300/  737 batches | lr 0.00015 | ms/batch 19.13 | loss 0.00017979\n",
      "| Epoch  25 |   350/  737 batches | lr 0.00015 | ms/batch 18.99 | loss 0.00014477\n",
      "| Epoch  25 |   400/  737 batches | lr 0.00015 | ms/batch 17.36 | loss 0.00009204\n",
      "| Epoch  25 |   450/  737 batches | lr 0.00015 | ms/batch 17.33 | loss 0.00017096\n",
      "| Epoch  25 |   500/  737 batches | lr 0.00015 | ms/batch 20.92 | loss 0.00009530\n",
      "| Epoch  25 |   550/  737 batches | lr 0.00015 | ms/batch 19.35 | loss 0.00009946\n",
      "| Epoch  25 |   600/  737 batches | lr 0.00015 | ms/batch 17.37 | loss 0.00013056\n",
      "| Epoch  25 |   650/  737 batches | lr 0.00015 | ms/batch 17.34 | loss 0.00013054\n",
      "| Epoch  25 |   700/  737 batches | lr 0.00015 | ms/batch 21.14 | loss 0.00023435\n",
      "\n",
      "Val set: Average loss: 0.00010787\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  26 |    50/  737 batches | lr 0.00015 | ms/batch 17.43 | loss 0.00015420\n",
      "| Epoch  26 |   100/  737 batches | lr 0.00015 | ms/batch 19.49 | loss 0.00015031\n",
      "| Epoch  26 |   150/  737 batches | lr 0.00015 | ms/batch 20.96 | loss 0.00012844\n",
      "| Epoch  26 |   200/  737 batches | lr 0.00015 | ms/batch 20.49 | loss 0.00017902\n",
      "| Epoch  26 |   250/  737 batches | lr 0.00015 | ms/batch 18.48 | loss 0.00025617\n",
      "| Epoch  26 |   300/  737 batches | lr 0.00015 | ms/batch 18.55 | loss 0.00013009\n",
      "| Epoch  26 |   350/  737 batches | lr 0.00015 | ms/batch 19.57 | loss 0.00009293\n",
      "| Epoch  26 |   400/  737 batches | lr 0.00015 | ms/batch 17.93 | loss 0.00007629\n",
      "| Epoch  26 |   450/  737 batches | lr 0.00015 | ms/batch 20.21 | loss 0.00014258\n",
      "| Epoch  26 |   500/  737 batches | lr 0.00015 | ms/batch 21.22 | loss 0.00015322\n",
      "| Epoch  26 |   550/  737 batches | lr 0.00015 | ms/batch 23.14 | loss 0.00016970\n",
      "| Epoch  26 |   600/  737 batches | lr 0.00015 | ms/batch 20.65 | loss 0.00013165\n",
      "| Epoch  26 |   650/  737 batches | lr 0.00015 | ms/batch 20.65 | loss 0.00013482\n",
      "| Epoch  26 |   700/  737 batches | lr 0.00015 | ms/batch 21.45 | loss 0.00010669\n",
      "\n",
      "Val set: Average loss: 0.00011348\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  27 |    50/  737 batches | lr 0.00015 | ms/batch 20.48 | loss 0.00021354\n",
      "| Epoch  27 |   100/  737 batches | lr 0.00015 | ms/batch 21.43 | loss 0.00009149\n",
      "| Epoch  27 |   150/  737 batches | lr 0.00015 | ms/batch 23.09 | loss 0.00010510\n",
      "| Epoch  27 |   200/  737 batches | lr 0.00015 | ms/batch 20.69 | loss 0.00014490\n",
      "| Epoch  27 |   250/  737 batches | lr 0.00015 | ms/batch 19.55 | loss 0.00010642\n",
      "| Epoch  27 |   300/  737 batches | lr 0.00015 | ms/batch 19.83 | loss 0.00015965\n",
      "| Epoch  27 |   350/  737 batches | lr 0.00015 | ms/batch 19.69 | loss 0.00013450\n",
      "| Epoch  27 |   400/  737 batches | lr 0.00015 | ms/batch 23.08 | loss 0.00007217\n",
      "| Epoch  27 |   450/  737 batches | lr 0.00015 | ms/batch 17.82 | loss 0.00010215\n",
      "| Epoch  27 |   500/  737 batches | lr 0.00015 | ms/batch 18.74 | loss 0.00015399\n",
      "| Epoch  27 |   550/  737 batches | lr 0.00015 | ms/batch 19.51 | loss 0.00015508\n",
      "| Epoch  27 |   600/  737 batches | lr 0.00015 | ms/batch 17.15 | loss 0.00010226\n",
      "| Epoch  27 |   650/  737 batches | lr 0.00015 | ms/batch 19.16 | loss 0.00008163\n",
      "| Epoch  27 |   700/  737 batches | lr 0.00015 | ms/batch 19.71 | loss 0.00009009\n",
      "\n",
      "Val set: Average loss: 0.00010076\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  28 |    50/  737 batches | lr 0.00015 | ms/batch 20.79 | loss 0.00008426\n",
      "| Epoch  28 |   100/  737 batches | lr 0.00015 | ms/batch 19.58 | loss 0.00022834\n",
      "| Epoch  28 |   150/  737 batches | lr 0.00015 | ms/batch 19.13 | loss 0.00011024\n",
      "| Epoch  28 |   200/  737 batches | lr 0.00015 | ms/batch 20.55 | loss 0.00008710\n",
      "| Epoch  28 |   250/  737 batches | lr 0.00015 | ms/batch 18.83 | loss 0.00010467\n",
      "| Epoch  28 |   300/  737 batches | lr 0.00015 | ms/batch 18.68 | loss 0.00014673\n",
      "| Epoch  28 |   350/  737 batches | lr 0.00015 | ms/batch 18.80 | loss 0.00016762\n",
      "| Epoch  28 |   400/  737 batches | lr 0.00015 | ms/batch 18.69 | loss 0.00007384\n",
      "| Epoch  28 |   450/  737 batches | lr 0.00015 | ms/batch 20.27 | loss 0.00008421\n",
      "| Epoch  28 |   500/  737 batches | lr 0.00015 | ms/batch 18.75 | loss 0.00010011\n",
      "| Epoch  28 |   550/  737 batches | lr 0.00015 | ms/batch 19.34 | loss 0.00011701\n",
      "| Epoch  28 |   600/  737 batches | lr 0.00015 | ms/batch 19.74 | loss 0.00010799\n",
      "| Epoch  28 |   650/  737 batches | lr 0.00015 | ms/batch 19.31 | loss 0.00014983\n",
      "| Epoch  28 |   700/  737 batches | lr 0.00015 | ms/batch 18.74 | loss 0.00009352\n",
      "\n",
      "Val set: Average loss: 0.00009752\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch  29 |    50/  737 batches | lr 0.00015 | ms/batch 18.84 | loss 0.00013709\n",
      "| Epoch  29 |   100/  737 batches | lr 0.00015 | ms/batch 16.77 | loss 0.00023146\n",
      "| Epoch  29 |   150/  737 batches | lr 0.00015 | ms/batch 18.26 | loss 0.00010304\n",
      "| Epoch  29 |   200/  737 batches | lr 0.00015 | ms/batch 18.66 | loss 0.00009230\n",
      "| Epoch  29 |   250/  737 batches | lr 0.00015 | ms/batch 18.36 | loss 0.00009410\n",
      "| Epoch  29 |   300/  737 batches | lr 0.00015 | ms/batch 16.66 | loss 0.00015662\n",
      "| Epoch  29 |   350/  737 batches | lr 0.00015 | ms/batch 16.61 | loss 0.00029869\n",
      "| Epoch  29 |   400/  737 batches | lr 0.00015 | ms/batch 20.09 | loss 0.00009524\n",
      "| Epoch  29 |   450/  737 batches | lr 0.00015 | ms/batch 18.24 | loss 0.00008489\n",
      "| Epoch  29 |   500/  737 batches | lr 0.00015 | ms/batch 16.69 | loss 0.00009363\n",
      "| Epoch  29 |   550/  737 batches | lr 0.00015 | ms/batch 16.71 | loss 0.00017142\n",
      "| Epoch  29 |   600/  737 batches | lr 0.00015 | ms/batch 19.94 | loss 0.00013648\n",
      "| Epoch  29 |   650/  737 batches | lr 0.00015 | ms/batch 16.96 | loss 0.00011636\n",
      "| Epoch  29 |   700/  737 batches | lr 0.00015 | ms/batch 18.74 | loss 0.00009472\n",
      "\n",
      "Val set: Average loss: 0.00008029\n",
      "\n",
      "| Epoch  30 |    50/  737 batches | lr 0.00015 | ms/batch 19.11 | loss 0.00013093\n",
      "| Epoch  30 |   100/  737 batches | lr 0.00015 | ms/batch 16.71 | loss 0.00013481\n",
      "| Epoch  30 |   150/  737 batches | lr 0.00015 | ms/batch 16.95 | loss 0.00008751\n",
      "| Epoch  30 |   200/  737 batches | lr 0.00015 | ms/batch 18.45 | loss 0.00013338\n",
      "| Epoch  30 |   250/  737 batches | lr 0.00015 | ms/batch 16.85 | loss 0.00013803\n",
      "| Epoch  30 |   300/  737 batches | lr 0.00015 | ms/batch 18.43 | loss 0.00013349\n",
      "| Epoch  30 |   350/  737 batches | lr 0.00015 | ms/batch 18.53 | loss 0.00014036\n",
      "| Epoch  30 |   400/  737 batches | lr 0.00015 | ms/batch 20.22 | loss 0.00006998\n",
      "| Epoch  30 |   450/  737 batches | lr 0.00015 | ms/batch 18.63 | loss 0.00007181\n",
      "| Epoch  30 |   500/  737 batches | lr 0.00015 | ms/batch 18.58 | loss 0.00007887\n",
      "| Epoch  30 |   550/  737 batches | lr 0.00015 | ms/batch 18.60 | loss 0.00009724\n",
      "| Epoch  30 |   600/  737 batches | lr 0.00015 | ms/batch 18.49 | loss 0.00010253\n",
      "| Epoch  30 |   650/  737 batches | lr 0.00015 | ms/batch 20.29 | loss 0.00008479\n",
      "| Epoch  30 |   700/  737 batches | lr 0.00015 | ms/batch 18.82 | loss 0.00008328\n",
      "\n",
      "Val set: Average loss: 0.00011672\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  31 |    50/  737 batches | lr 0.00015 | ms/batch 18.99 | loss 0.00022552\n",
      "| Epoch  31 |   100/  737 batches | lr 0.00015 | ms/batch 18.48 | loss 0.00016803\n",
      "| Epoch  31 |   150/  737 batches | lr 0.00015 | ms/batch 18.39 | loss 0.00009404\n",
      "| Epoch  31 |   200/  737 batches | lr 0.00015 | ms/batch 18.46 | loss 0.00009391\n",
      "| Epoch  31 |   250/  737 batches | lr 0.00015 | ms/batch 16.66 | loss 0.00006500\n",
      "| Epoch  31 |   300/  737 batches | lr 0.00015 | ms/batch 18.34 | loss 0.00010369\n",
      "| Epoch  31 |   350/  737 batches | lr 0.00015 | ms/batch 18.38 | loss 0.00009085\n",
      "| Epoch  31 |   400/  737 batches | lr 0.00015 | ms/batch 18.49 | loss 0.00017176\n",
      "| Epoch  31 |   450/  737 batches | lr 0.00015 | ms/batch 16.79 | loss 0.00014254\n",
      "| Epoch  31 |   500/  737 batches | lr 0.00015 | ms/batch 16.67 | loss 0.00010173\n",
      "| Epoch  31 |   550/  737 batches | lr 0.00015 | ms/batch 20.06 | loss 0.00013717\n",
      "| Epoch  31 |   600/  737 batches | lr 0.00015 | ms/batch 18.37 | loss 0.00007796\n",
      "| Epoch  31 |   650/  737 batches | lr 0.00015 | ms/batch 16.65 | loss 0.00007726\n",
      "| Epoch  31 |   700/  737 batches | lr 0.00015 | ms/batch 16.65 | loss 0.00006299\n",
      "\n",
      "Val set: Average loss: 0.00012894\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  32 |    50/  737 batches | lr 0.00015 | ms/batch 17.00 | loss 0.00011248\n",
      "| Epoch  32 |   100/  737 batches | lr 0.00015 | ms/batch 20.05 | loss 0.00008403\n",
      "| Epoch  32 |   150/  737 batches | lr 0.00015 | ms/batch 18.34 | loss 0.00007055\n",
      "| Epoch  32 |   200/  737 batches | lr 0.00015 | ms/batch 16.66 | loss 0.00006884\n",
      "| Epoch  32 |   250/  737 batches | lr 0.00015 | ms/batch 16.89 | loss 0.00015720\n",
      "| Epoch  32 |   300/  737 batches | lr 0.00015 | ms/batch 18.51 | loss 0.00016628\n",
      "| Epoch  32 |   350/  737 batches | lr 0.00015 | ms/batch 18.50 | loss 0.00023525\n",
      "| Epoch  32 |   400/  737 batches | lr 0.00015 | ms/batch 18.42 | loss 0.00008019\n",
      "| Epoch  32 |   450/  737 batches | lr 0.00015 | ms/batch 18.55 | loss 0.00008175\n",
      "| Epoch  32 |   500/  737 batches | lr 0.00015 | ms/batch 16.79 | loss 0.00006735\n",
      "| Epoch  32 |   550/  737 batches | lr 0.00015 | ms/batch 18.53 | loss 0.00007818\n",
      "| Epoch  32 |   600/  737 batches | lr 0.00015 | ms/batch 18.40 | loss 0.00008985\n",
      "| Epoch  32 |   650/  737 batches | lr 0.00015 | ms/batch 18.33 | loss 0.00006783\n",
      "| Epoch  32 |   700/  737 batches | lr 0.00015 | ms/batch 16.67 | loss 0.00027857\n",
      "\n",
      "Val set: Average loss: 0.00007902\n",
      "\n",
      "| Epoch  33 |    50/  737 batches | lr 0.00015 | ms/batch 18.88 | loss 0.00014484\n",
      "| Epoch  33 |   100/  737 batches | lr 0.00015 | ms/batch 18.36 | loss 0.00011441\n",
      "| Epoch  33 |   150/  737 batches | lr 0.00015 | ms/batch 18.62 | loss 0.00009382\n",
      "| Epoch  33 |   200/  737 batches | lr 0.00015 | ms/batch 17.00 | loss 0.00013185\n",
      "| Epoch  33 |   250/  737 batches | lr 0.00015 | ms/batch 18.81 | loss 0.00010504\n",
      "| Epoch  33 |   300/  737 batches | lr 0.00015 | ms/batch 18.86 | loss 0.00011788\n",
      "| Epoch  33 |   350/  737 batches | lr 0.00015 | ms/batch 18.60 | loss 0.00007792\n",
      "| Epoch  33 |   400/  737 batches | lr 0.00015 | ms/batch 16.80 | loss 0.00005876\n",
      "| Epoch  33 |   450/  737 batches | lr 0.00015 | ms/batch 16.82 | loss 0.00006512\n",
      "| Epoch  33 |   500/  737 batches | lr 0.00015 | ms/batch 20.13 | loss 0.00006669\n",
      "| Epoch  33 |   550/  737 batches | lr 0.00015 | ms/batch 18.48 | loss 0.00008020\n",
      "| Epoch  33 |   600/  737 batches | lr 0.00015 | ms/batch 16.76 | loss 0.00015559\n",
      "| Epoch  33 |   650/  737 batches | lr 0.00015 | ms/batch 16.65 | loss 0.00009653\n",
      "| Epoch  33 |   700/  737 batches | lr 0.00015 | ms/batch 18.56 | loss 0.00021124\n",
      "\n",
      "Val set: Average loss: 0.00007454\n",
      "\n",
      "| Epoch  34 |    50/  737 batches | lr 0.00015 | ms/batch 19.11 | loss 0.00009163\n",
      "| Epoch  34 |   100/  737 batches | lr 0.00015 | ms/batch 18.38 | loss 0.00007815\n",
      "| Epoch  34 |   150/  737 batches | lr 0.00015 | ms/batch 16.67 | loss 0.00007460\n",
      "| Epoch  34 |   200/  737 batches | lr 0.00015 | ms/batch 16.73 | loss 0.00008594\n",
      "| Epoch  34 |   250/  737 batches | lr 0.00015 | ms/batch 20.20 | loss 0.00008524\n",
      "| Epoch  34 |   300/  737 batches | lr 0.00015 | ms/batch 18.65 | loss 0.00012731\n",
      "| Epoch  34 |   350/  737 batches | lr 0.00015 | ms/batch 16.95 | loss 0.00011196\n",
      "| Epoch  34 |   400/  737 batches | lr 0.00015 | ms/batch 16.84 | loss 0.00006191\n",
      "| Epoch  34 |   450/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00009152\n",
      "| Epoch  34 |   500/  737 batches | lr 0.00015 | ms/batch 18.65 | loss 0.00008107\n",
      "| Epoch  34 |   550/  737 batches | lr 0.00015 | ms/batch 18.64 | loss 0.00034625\n",
      "| Epoch  34 |   600/  737 batches | lr 0.00015 | ms/batch 18.47 | loss 0.00019631\n",
      "| Epoch  34 |   650/  737 batches | lr 0.00015 | ms/batch 16.84 | loss 0.00007936\n",
      "| Epoch  34 |   700/  737 batches | lr 0.00015 | ms/batch 18.35 | loss 0.00006597\n",
      "\n",
      "Val set: Average loss: 0.00008253\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  35 |    50/  737 batches | lr 0.00015 | ms/batch 17.16 | loss 0.00012235\n",
      "| Epoch  35 |   100/  737 batches | lr 0.00015 | ms/batch 17.06 | loss 0.00009984\n",
      "| Epoch  35 |   150/  737 batches | lr 0.00015 | ms/batch 19.93 | loss 0.00008632\n",
      "| Epoch  35 |   200/  737 batches | lr 0.00015 | ms/batch 16.94 | loss 0.00010028\n",
      "| Epoch  35 |   250/  737 batches | lr 0.00015 | ms/batch 18.32 | loss 0.00008431\n",
      "| Epoch  35 |   300/  737 batches | lr 0.00015 | ms/batch 16.66 | loss 0.00013301\n",
      "| Epoch  35 |   350/  737 batches | lr 0.00015 | ms/batch 18.50 | loss 0.00010074\n",
      "| Epoch  35 |   400/  737 batches | lr 0.00015 | ms/batch 20.08 | loss 0.00005576\n",
      "| Epoch  35 |   450/  737 batches | lr 0.00015 | ms/batch 18.56 | loss 0.00008005\n",
      "| Epoch  35 |   500/  737 batches | lr 0.00015 | ms/batch 18.86 | loss 0.00009650\n",
      "| Epoch  35 |   550/  737 batches | lr 0.00015 | ms/batch 18.50 | loss 0.00010835\n",
      "| Epoch  35 |   600/  737 batches | lr 0.00015 | ms/batch 18.66 | loss 0.00009165\n",
      "| Epoch  35 |   650/  737 batches | lr 0.00015 | ms/batch 18.62 | loss 0.00007323\n",
      "| Epoch  35 |   700/  737 batches | lr 0.00015 | ms/batch 20.20 | loss 0.00006940\n",
      "\n",
      "Val set: Average loss: 0.00006960\n",
      "\n",
      "| Epoch  36 |    50/  737 batches | lr 0.00015 | ms/batch 18.98 | loss 0.00010934\n",
      "| Epoch  36 |   100/  737 batches | lr 0.00015 | ms/batch 16.68 | loss 0.00014538\n",
      "| Epoch  36 |   150/  737 batches | lr 0.00015 | ms/batch 18.62 | loss 0.00007168\n",
      "| Epoch  36 |   200/  737 batches | lr 0.00015 | ms/batch 20.38 | loss 0.00008088\n",
      "| Epoch  36 |   250/  737 batches | lr 0.00015 | ms/batch 18.63 | loss 0.00008887\n",
      "| Epoch  36 |   300/  737 batches | lr 0.00015 | ms/batch 18.66 | loss 0.00012531\n",
      "| Epoch  36 |   350/  737 batches | lr 0.00015 | ms/batch 18.99 | loss 0.00017833\n",
      "| Epoch  36 |   400/  737 batches | lr 0.00015 | ms/batch 18.85 | loss 0.00005805\n",
      "| Epoch  36 |   450/  737 batches | lr 0.00015 | ms/batch 18.73 | loss 0.00007046\n",
      "| Epoch  36 |   500/  737 batches | lr 0.00015 | ms/batch 20.37 | loss 0.00007227\n",
      "| Epoch  36 |   550/  737 batches | lr 0.00015 | ms/batch 18.67 | loss 0.00010653\n",
      "| Epoch  36 |   600/  737 batches | lr 0.00015 | ms/batch 18.72 | loss 0.00010284\n",
      "| Epoch  36 |   650/  737 batches | lr 0.00015 | ms/batch 18.70 | loss 0.00007538\n",
      "| Epoch  36 |   700/  737 batches | lr 0.00015 | ms/batch 18.84 | loss 0.00009244\n",
      "\n",
      "Val set: Average loss: 0.00021092\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  37 |    50/  737 batches | lr 0.00015 | ms/batch 18.83 | loss 0.00010189\n",
      "| Epoch  37 |   100/  737 batches | lr 0.00015 | ms/batch 16.73 | loss 0.00013808\n",
      "| Epoch  37 |   150/  737 batches | lr 0.00015 | ms/batch 16.73 | loss 0.00010330\n",
      "| Epoch  37 |   200/  737 batches | lr 0.00015 | ms/batch 18.38 | loss 0.00008265\n",
      "| Epoch  37 |   250/  737 batches | lr 0.00015 | ms/batch 16.84 | loss 0.00011728\n",
      "| Epoch  37 |   300/  737 batches | lr 0.00015 | ms/batch 18.44 | loss 0.00012788\n",
      "| Epoch  37 |   350/  737 batches | lr 0.00015 | ms/batch 18.82 | loss 0.00011470\n",
      "| Epoch  37 |   400/  737 batches | lr 0.00015 | ms/batch 18.74 | loss 0.00005845\n",
      "| Epoch  37 |   450/  737 batches | lr 0.00015 | ms/batch 18.62 | loss 0.00006596\n",
      "| Epoch  37 |   500/  737 batches | lr 0.00015 | ms/batch 16.94 | loss 0.00008872\n",
      "| Epoch  37 |   550/  737 batches | lr 0.00015 | ms/batch 18.65 | loss 0.00013697\n",
      "| Epoch  37 |   600/  737 batches | lr 0.00015 | ms/batch 18.40 | loss 0.00007208\n",
      "| Epoch  37 |   650/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00007377\n",
      "| Epoch  37 |   700/  737 batches | lr 0.00015 | ms/batch 16.69 | loss 0.00015207\n",
      "\n",
      "Val set: Average loss: 0.00006742\n",
      "\n",
      "| Epoch  38 |    50/  737 batches | lr 0.00015 | ms/batch 17.34 | loss 0.00008412\n",
      "| Epoch  38 |   100/  737 batches | lr 0.00015 | ms/batch 18.71 | loss 0.00010092\n",
      "| Epoch  38 |   150/  737 batches | lr 0.00015 | ms/batch 18.53 | loss 0.00006751\n",
      "| Epoch  38 |   200/  737 batches | lr 0.00015 | ms/batch 16.73 | loss 0.00008198\n",
      "| Epoch  38 |   250/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00008119\n",
      "| Epoch  38 |   300/  737 batches | lr 0.00015 | ms/batch 18.49 | loss 0.00020019\n",
      "| Epoch  38 |   350/  737 batches | lr 0.00015 | ms/batch 18.48 | loss 0.00012631\n",
      "| Epoch  38 |   400/  737 batches | lr 0.00015 | ms/batch 16.73 | loss 0.00006436\n",
      "| Epoch  38 |   450/  737 batches | lr 0.00015 | ms/batch 16.71 | loss 0.00008841\n",
      "| Epoch  38 |   500/  737 batches | lr 0.00015 | ms/batch 20.07 | loss 0.00009555\n",
      "| Epoch  38 |   550/  737 batches | lr 0.00015 | ms/batch 18.49 | loss 0.00012302\n",
      "| Epoch  38 |   600/  737 batches | lr 0.00015 | ms/batch 16.71 | loss 0.00011140\n",
      "| Epoch  38 |   650/  737 batches | lr 0.00015 | ms/batch 16.68 | loss 0.00007896\n",
      "| Epoch  38 |   700/  737 batches | lr 0.00015 | ms/batch 18.78 | loss 0.00008965\n",
      "\n",
      "Val set: Average loss: 0.00007561\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  39 |    50/  737 batches | lr 0.00015 | ms/batch 18.61 | loss 0.00006435\n",
      "| Epoch  39 |   100/  737 batches | lr 0.00015 | ms/batch 16.67 | loss 0.00008394\n",
      "| Epoch  39 |   150/  737 batches | lr 0.00015 | ms/batch 18.45 | loss 0.00010504\n",
      "| Epoch  39 |   200/  737 batches | lr 0.00015 | ms/batch 18.54 | loss 0.00010653\n",
      "| Epoch  39 |   250/  737 batches | lr 0.00015 | ms/batch 18.60 | loss 0.00016990\n",
      "| Epoch  39 |   300/  737 batches | lr 0.00015 | ms/batch 16.67 | loss 0.00013017\n",
      "| Epoch  39 |   350/  737 batches | lr 0.00015 | ms/batch 18.28 | loss 0.00012086\n",
      "| Epoch  39 |   400/  737 batches | lr 0.00015 | ms/batch 18.48 | loss 0.00008249\n",
      "| Epoch  39 |   450/  737 batches | lr 0.00015 | ms/batch 18.56 | loss 0.00008003\n",
      "| Epoch  39 |   500/  737 batches | lr 0.00015 | ms/batch 16.70 | loss 0.00006633\n",
      "| Epoch  39 |   550/  737 batches | lr 0.00015 | ms/batch 16.71 | loss 0.00008856\n",
      "| Epoch  39 |   600/  737 batches | lr 0.00015 | ms/batch 18.67 | loss 0.00010106\n",
      "| Epoch  39 |   650/  737 batches | lr 0.00015 | ms/batch 16.93 | loss 0.00007361\n",
      "| Epoch  39 |   700/  737 batches | lr 0.00015 | ms/batch 18.57 | loss 0.00007269\n",
      "\n",
      "Val set: Average loss: 0.00007350\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  40 |    50/  737 batches | lr 0.00015 | ms/batch 17.14 | loss 0.00006894\n",
      "| Epoch  40 |   100/  737 batches | lr 0.00015 | ms/batch 16.91 | loss 0.00018593\n",
      "| Epoch  40 |   150/  737 batches | lr 0.00015 | ms/batch 20.29 | loss 0.00012715\n",
      "| Epoch  40 |   200/  737 batches | lr 0.00015 | ms/batch 18.65 | loss 0.00008280\n",
      "| Epoch  40 |   250/  737 batches | lr 0.00015 | ms/batch 16.94 | loss 0.00009783\n",
      "| Epoch  40 |   300/  737 batches | lr 0.00015 | ms/batch 17.03 | loss 0.00009854\n",
      "| Epoch  40 |   350/  737 batches | lr 0.00015 | ms/batch 18.70 | loss 0.00009144\n",
      "| Epoch  40 |   400/  737 batches | lr 0.00015 | ms/batch 18.98 | loss 0.00005452\n",
      "| Epoch  40 |   450/  737 batches | lr 0.00015 | ms/batch 18.94 | loss 0.00006492\n",
      "| Epoch  40 |   500/  737 batches | lr 0.00015 | ms/batch 18.76 | loss 0.00013848\n",
      "| Epoch  40 |   550/  737 batches | lr 0.00015 | ms/batch 17.03 | loss 0.00012368\n",
      "| Epoch  40 |   600/  737 batches | lr 0.00015 | ms/batch 18.62 | loss 0.00013340\n",
      "| Epoch  40 |   650/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00009249\n",
      "| Epoch  40 |   700/  737 batches | lr 0.00015 | ms/batch 18.37 | loss 0.00009857\n",
      "\n",
      "Val set: Average loss: 0.00006524\n",
      "\n",
      "| Epoch  41 |    50/  737 batches | lr 0.00015 | ms/batch 18.89 | loss 0.00008288\n",
      "| Epoch  41 |   100/  737 batches | lr 0.00015 | ms/batch 16.73 | loss 0.00012538\n",
      "| Epoch  41 |   150/  737 batches | lr 0.00015 | ms/batch 18.56 | loss 0.00008838\n",
      "| Epoch  41 |   200/  737 batches | lr 0.00015 | ms/batch 18.57 | loss 0.00007681\n",
      "| Epoch  41 |   250/  737 batches | lr 0.00015 | ms/batch 18.50 | loss 0.00008659\n",
      "| Epoch  41 |   300/  737 batches | lr 0.00015 | ms/batch 16.64 | loss 0.00008247\n",
      "| Epoch  41 |   350/  737 batches | lr 0.00015 | ms/batch 16.83 | loss 0.00006958\n",
      "| Epoch  41 |   400/  737 batches | lr 0.00015 | ms/batch 20.46 | loss 0.00005170\n",
      "| Epoch  41 |   450/  737 batches | lr 0.00015 | ms/batch 18.50 | loss 0.00009051\n",
      "| Epoch  41 |   500/  737 batches | lr 0.00015 | ms/batch 17.20 | loss 0.00010620\n",
      "| Epoch  41 |   550/  737 batches | lr 0.00015 | ms/batch 17.04 | loss 0.00007855\n",
      "| Epoch  41 |   600/  737 batches | lr 0.00015 | ms/batch 18.95 | loss 0.00006281\n",
      "| Epoch  41 |   650/  737 batches | lr 0.00015 | ms/batch 18.45 | loss 0.00007534\n",
      "| Epoch  41 |   700/  737 batches | lr 0.00015 | ms/batch 18.52 | loss 0.00009835\n",
      "\n",
      "Val set: Average loss: 0.00009473\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  42 |    50/  737 batches | lr 0.00015 | ms/batch 18.80 | loss 0.00008420\n",
      "| Epoch  42 |   100/  737 batches | lr 0.00015 | ms/batch 18.64 | loss 0.00010688\n",
      "| Epoch  42 |   150/  737 batches | lr 0.00015 | ms/batch 18.63 | loss 0.00005427\n",
      "| Epoch  42 |   200/  737 batches | lr 0.00015 | ms/batch 20.02 | loss 0.00010291\n",
      "| Epoch  42 |   250/  737 batches | lr 0.00015 | ms/batch 18.61 | loss 0.00008943\n",
      "| Epoch  42 |   300/  737 batches | lr 0.00015 | ms/batch 18.44 | loss 0.00007133\n",
      "| Epoch  42 |   350/  737 batches | lr 0.00015 | ms/batch 18.58 | loss 0.00009130\n",
      "| Epoch  42 |   400/  737 batches | lr 0.00015 | ms/batch 18.60 | loss 0.00006840\n",
      "| Epoch  42 |   450/  737 batches | lr 0.00015 | ms/batch 20.26 | loss 0.00011453\n",
      "| Epoch  42 |   500/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00012569\n",
      "| Epoch  42 |   550/  737 batches | lr 0.00015 | ms/batch 18.53 | loss 0.00012875\n",
      "| Epoch  42 |   600/  737 batches | lr 0.00015 | ms/batch 18.47 | loss 0.00008946\n",
      "| Epoch  42 |   650/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00006736\n",
      "| Epoch  42 |   700/  737 batches | lr 0.00015 | ms/batch 18.68 | loss 0.00007427\n",
      "\n",
      "Val set: Average loss: 0.00008349\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  43 |    50/  737 batches | lr 0.00015 | ms/batch 18.80 | loss 0.00008046\n",
      "| Epoch  43 |   100/  737 batches | lr 0.00015 | ms/batch 16.90 | loss 0.00007862\n",
      "| Epoch  43 |   150/  737 batches | lr 0.00015 | ms/batch 18.36 | loss 0.00016811\n",
      "| Epoch  43 |   200/  737 batches | lr 0.00015 | ms/batch 18.69 | loss 0.00011403\n",
      "| Epoch  43 |   250/  737 batches | lr 0.00015 | ms/batch 18.65 | loss 0.00012330\n",
      "| Epoch  43 |   300/  737 batches | lr 0.00015 | ms/batch 16.73 | loss 0.00014919\n",
      "| Epoch  43 |   350/  737 batches | lr 0.00015 | ms/batch 16.62 | loss 0.00008883\n",
      "| Epoch  43 |   400/  737 batches | lr 0.00015 | ms/batch 20.00 | loss 0.00006357\n",
      "| Epoch  43 |   450/  737 batches | lr 0.00015 | ms/batch 18.34 | loss 0.00006365\n",
      "| Epoch  43 |   500/  737 batches | lr 0.00015 | ms/batch 16.66 | loss 0.00007893\n",
      "| Epoch  43 |   550/  737 batches | lr 0.00015 | ms/batch 16.72 | loss 0.00013911\n",
      "| Epoch  43 |   600/  737 batches | lr 0.00015 | ms/batch 19.95 | loss 0.00013122\n",
      "| Epoch  43 |   650/  737 batches | lr 0.00015 | ms/batch 16.73 | loss 0.00008342\n",
      "| Epoch  43 |   700/  737 batches | lr 0.00015 | ms/batch 18.32 | loss 0.00007144\n",
      "\n",
      "Val set: Average loss: 0.00005914\n",
      "\n",
      "| Epoch  44 |    50/  737 batches | lr 0.00015 | ms/batch 18.86 | loss 0.00010246\n",
      "| Epoch  44 |   100/  737 batches | lr 0.00015 | ms/batch 16.94 | loss 0.00007212\n",
      "| Epoch  44 |   150/  737 batches | lr 0.00015 | ms/batch 16.93 | loss 0.00007705\n",
      "| Epoch  44 |   200/  737 batches | lr 0.00015 | ms/batch 18.44 | loss 0.00008453\n",
      "| Epoch  44 |   250/  737 batches | lr 0.00015 | ms/batch 17.00 | loss 0.00010672\n",
      "| Epoch  44 |   300/  737 batches | lr 0.00015 | ms/batch 18.99 | loss 0.00012876\n",
      "| Epoch  44 |   350/  737 batches | lr 0.00015 | ms/batch 18.39 | loss 0.00007032\n",
      "| Epoch  44 |   400/  737 batches | lr 0.00015 | ms/batch 18.52 | loss 0.00005651\n",
      "| Epoch  44 |   450/  737 batches | lr 0.00015 | ms/batch 16.67 | loss 0.00004894\n",
      "| Epoch  44 |   500/  737 batches | lr 0.00015 | ms/batch 18.30 | loss 0.00006127\n",
      "| Epoch  44 |   550/  737 batches | lr 0.00015 | ms/batch 18.46 | loss 0.00006196\n",
      "| Epoch  44 |   600/  737 batches | lr 0.00015 | ms/batch 18.54 | loss 0.00005364\n",
      "| Epoch  44 |   650/  737 batches | lr 0.00015 | ms/batch 16.66 | loss 0.00006629\n",
      "| Epoch  44 |   700/  737 batches | lr 0.00015 | ms/batch 16.66 | loss 0.00015046\n",
      "\n",
      "Val set: Average loss: 0.00005645\n",
      "\n",
      "| Epoch  45 |    50/  737 batches | lr 0.00015 | ms/batch 19.29 | loss 0.00006794\n",
      "| Epoch  45 |   100/  737 batches | lr 0.00015 | ms/batch 20.41 | loss 0.00006738\n",
      "| Epoch  45 |   150/  737 batches | lr 0.00015 | ms/batch 18.75 | loss 0.00006258\n",
      "| Epoch  45 |   200/  737 batches | lr 0.00015 | ms/batch 18.83 | loss 0.00010244\n",
      "| Epoch  45 |   250/  737 batches | lr 0.00015 | ms/batch 19.03 | loss 0.00006063\n",
      "| Epoch  45 |   300/  737 batches | lr 0.00015 | ms/batch 18.68 | loss 0.00006179\n",
      "| Epoch  45 |   350/  737 batches | lr 0.00015 | ms/batch 20.36 | loss 0.00014081\n",
      "| Epoch  45 |   400/  737 batches | lr 0.00015 | ms/batch 18.60 | loss 0.00005538\n",
      "| Epoch  45 |   450/  737 batches | lr 0.00015 | ms/batch 18.50 | loss 0.00007744\n",
      "| Epoch  45 |   500/  737 batches | lr 0.00015 | ms/batch 18.58 | loss 0.00011885\n",
      "| Epoch  45 |   550/  737 batches | lr 0.00015 | ms/batch 18.67 | loss 0.00008767\n",
      "| Epoch  45 |   600/  737 batches | lr 0.00015 | ms/batch 20.09 | loss 0.00005355\n",
      "| Epoch  45 |   650/  737 batches | lr 0.00015 | ms/batch 18.79 | loss 0.00006781\n",
      "| Epoch  45 |   700/  737 batches | lr 0.00015 | ms/batch 18.43 | loss 0.00015536\n",
      "\n",
      "Val set: Average loss: 0.00007693\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  46 |    50/  737 batches | lr 0.00015 | ms/batch 19.17 | loss 0.00009212\n",
      "| Epoch  46 |   100/  737 batches | lr 0.00015 | ms/batch 18.72 | loss 0.00006791\n",
      "| Epoch  46 |   150/  737 batches | lr 0.00015 | ms/batch 16.91 | loss 0.00010359\n",
      "| Epoch  46 |   200/  737 batches | lr 0.00015 | ms/batch 16.81 | loss 0.00007799\n",
      "| Epoch  46 |   250/  737 batches | lr 0.00015 | ms/batch 20.20 | loss 0.00007460\n",
      "| Epoch  46 |   300/  737 batches | lr 0.00015 | ms/batch 18.38 | loss 0.00009304\n",
      "| Epoch  46 |   350/  737 batches | lr 0.00015 | ms/batch 16.72 | loss 0.00010547\n",
      "| Epoch  46 |   400/  737 batches | lr 0.00015 | ms/batch 16.86 | loss 0.00006352\n",
      "| Epoch  46 |   450/  737 batches | lr 0.00015 | ms/batch 18.52 | loss 0.00007007\n",
      "| Epoch  46 |   500/  737 batches | lr 0.00015 | ms/batch 20.19 | loss 0.00011573\n",
      "| Epoch  46 |   550/  737 batches | lr 0.00015 | ms/batch 18.66 | loss 0.00011884\n",
      "| Epoch  46 |   600/  737 batches | lr 0.00015 | ms/batch 18.46 | loss 0.00011886\n",
      "| Epoch  46 |   650/  737 batches | lr 0.00015 | ms/batch 18.61 | loss 0.00007306\n",
      "| Epoch  46 |   700/  737 batches | lr 0.00015 | ms/batch 18.68 | loss 0.00008348\n",
      "\n",
      "Val set: Average loss: 0.00008460\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  47 |    50/  737 batches | lr 0.00015 | ms/batch 18.90 | loss 0.00009008\n",
      "| Epoch  47 |   100/  737 batches | lr 0.00015 | ms/batch 16.74 | loss 0.00009418\n",
      "| Epoch  47 |   150/  737 batches | lr 0.00015 | ms/batch 18.33 | loss 0.00006532\n",
      "| Epoch  47 |   200/  737 batches | lr 0.00015 | ms/batch 18.60 | loss 0.00006797\n",
      "| Epoch  47 |   250/  737 batches | lr 0.00015 | ms/batch 18.31 | loss 0.00008740\n",
      "| Epoch  47 |   300/  737 batches | lr 0.00015 | ms/batch 16.74 | loss 0.00009918\n",
      "| Epoch  47 |   350/  737 batches | lr 0.00015 | ms/batch 16.64 | loss 0.00007494\n",
      "| Epoch  47 |   400/  737 batches | lr 0.00015 | ms/batch 20.01 | loss 0.00007021\n",
      "| Epoch  47 |   450/  737 batches | lr 0.00015 | ms/batch 18.58 | loss 0.00005660\n",
      "| Epoch  47 |   500/  737 batches | lr 0.00015 | ms/batch 16.80 | loss 0.00006040\n",
      "| Epoch  47 |   550/  737 batches | lr 0.00015 | ms/batch 16.94 | loss 0.00006547\n",
      "| Epoch  47 |   600/  737 batches | lr 0.00015 | ms/batch 20.26 | loss 0.00005857\n",
      "| Epoch  47 |   650/  737 batches | lr 0.00015 | ms/batch 16.87 | loss 0.00007877\n",
      "| Epoch  47 |   700/  737 batches | lr 0.00015 | ms/batch 18.82 | loss 0.00009531\n",
      "\n",
      "Val set: Average loss: 0.00010816\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  48 |    50/  737 batches | lr 0.00015 | ms/batch 18.81 | loss 0.00006667\n",
      "| Epoch  48 |   100/  737 batches | lr 0.00015 | ms/batch 18.61 | loss 0.00004825\n",
      "| Epoch  48 |   150/  737 batches | lr 0.00015 | ms/batch 19.04 | loss 0.00010645\n",
      "| Epoch  48 |   200/  737 batches | lr 0.00015 | ms/batch 20.24 | loss 0.00006379\n",
      "| Epoch  48 |   250/  737 batches | lr 0.00015 | ms/batch 18.69 | loss 0.00005899\n",
      "| Epoch  48 |   300/  737 batches | lr 0.00015 | ms/batch 18.70 | loss 0.00007974\n",
      "| Epoch  48 |   350/  737 batches | lr 0.00015 | ms/batch 18.69 | loss 0.00010875\n",
      "| Epoch  48 |   400/  737 batches | lr 0.00015 | ms/batch 18.87 | loss 0.00006262\n",
      "| Epoch  48 |   450/  737 batches | lr 0.00015 | ms/batch 20.33 | loss 0.00007660\n",
      "| Epoch  48 |   500/  737 batches | lr 0.00015 | ms/batch 18.73 | loss 0.00008724\n",
      "| Epoch  48 |   550/  737 batches | lr 0.00015 | ms/batch 18.89 | loss 0.00010558\n",
      "| Epoch  48 |   600/  737 batches | lr 0.00015 | ms/batch 18.71 | loss 0.00007360\n",
      "| Epoch  48 |   650/  737 batches | lr 0.00015 | ms/batch 18.83 | loss 0.00008140\n",
      "| Epoch  48 |   700/  737 batches | lr 0.00015 | ms/batch 20.59 | loss 0.00008630\n",
      "\n",
      "Val set: Average loss: 0.00012394\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  49 |    50/  737 batches | lr 0.00015 | ms/batch 18.72 | loss 0.00009119\n",
      "| Epoch  49 |   100/  737 batches | lr 0.00015 | ms/batch 16.68 | loss 0.00008835\n",
      "| Epoch  49 |   150/  737 batches | lr 0.00015 | ms/batch 18.42 | loss 0.00005301\n",
      "| Epoch  49 |   200/  737 batches | lr 0.00015 | ms/batch 18.57 | loss 0.00006159\n",
      "| Epoch  49 |   250/  737 batches | lr 0.00015 | ms/batch 18.34 | loss 0.00005591\n",
      "| Epoch  49 |   300/  737 batches | lr 0.00015 | ms/batch 16.71 | loss 0.00006629\n",
      "| Epoch  49 |   350/  737 batches | lr 0.00015 | ms/batch 16.76 | loss 0.00012769\n",
      "| Epoch  49 |   400/  737 batches | lr 0.00015 | ms/batch 20.32 | loss 0.00006356\n",
      "| Epoch  49 |   450/  737 batches | lr 0.00015 | ms/batch 18.87 | loss 0.00006129\n",
      "| Epoch  49 |   500/  737 batches | lr 0.00015 | ms/batch 16.80 | loss 0.00007017\n",
      "| Epoch  49 |   550/  737 batches | lr 0.00015 | ms/batch 16.91 | loss 0.00008260\n",
      "| Epoch  49 |   600/  737 batches | lr 0.00015 | ms/batch 19.96 | loss 0.00006499\n",
      "| Epoch  49 |   650/  737 batches | lr 0.00015 | ms/batch 16.71 | loss 0.00005667\n",
      "| Epoch  49 |   700/  737 batches | lr 0.00015 | ms/batch 18.60 | loss 0.00011026\n",
      "\n",
      "Val set: Average loss: 0.00011314\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  50 |    50/  737 batches | lr 0.00015 | ms/batch 20.41 | loss 0.00009273\n",
      "| Epoch  50 |   100/  737 batches | lr 0.00015 | ms/batch 18.60 | loss 0.00008565\n",
      "| Epoch  50 |   150/  737 batches | lr 0.00015 | ms/batch 18.73 | loss 0.00004859\n",
      "| Epoch  50 |   200/  737 batches | lr 0.00015 | ms/batch 18.63 | loss 0.00005427\n",
      "| Epoch  50 |   250/  737 batches | lr 0.00015 | ms/batch 18.68 | loss 0.00005416\n",
      "| Epoch  50 |   300/  737 batches | lr 0.00015 | ms/batch 20.40 | loss 0.00005397\n",
      "| Epoch  50 |   350/  737 batches | lr 0.00015 | ms/batch 18.73 | loss 0.00022830\n",
      "| Epoch  50 |   400/  737 batches | lr 0.00015 | ms/batch 18.67 | loss 0.00007714\n",
      "| Epoch  50 |   450/  737 batches | lr 0.00015 | ms/batch 18.89 | loss 0.00008015\n",
      "| Epoch  50 |   500/  737 batches | lr 0.00015 | ms/batch 18.57 | loss 0.00007009\n",
      "| Epoch  50 |   550/  737 batches | lr 0.00015 | ms/batch 20.27 | loss 0.00017084\n",
      "| Epoch  50 |   600/  737 batches | lr 0.00015 | ms/batch 16.78 | loss 0.00011356\n",
      "| Epoch  50 |   650/  737 batches | lr 0.00015 | ms/batch 18.27 | loss 0.00005655\n",
      "| Epoch  50 |   700/  737 batches | lr 0.00015 | ms/batch 16.84 | loss 0.00005524\n",
      "\n",
      "Val set: Average loss: 0.00016035\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch  51 |    50/  737 batches | lr 0.00015 | ms/batch 17.04 | loss 0.00015349\n",
      "| Epoch  51 |   100/  737 batches | lr 0.00015 | ms/batch 19.99 | loss 0.00012017\n",
      "| Epoch  51 |   150/  737 batches | lr 0.00015 | ms/batch 16.76 | loss 0.00006415\n",
      "| Epoch  51 |   200/  737 batches | lr 0.00015 | ms/batch 18.30 | loss 0.00010014\n",
      "| Epoch  51 |   250/  737 batches | lr 0.00015 | ms/batch 16.79 | loss 0.00008619\n",
      "| Epoch  51 |   300/  737 batches | lr 0.00015 | ms/batch 18.60 | loss 0.00007964\n",
      "| Epoch  51 |   350/  737 batches | lr 0.00015 | ms/batch 20.34 | loss 0.00006292\n",
      "| Epoch  51 |   400/  737 batches | lr 0.00015 | ms/batch 18.85 | loss 0.00007328\n",
      "| Epoch  51 |   450/  737 batches | lr 0.00015 | ms/batch 18.83 | loss 0.00009846\n",
      "| Epoch  51 |   500/  737 batches | lr 0.00015 | ms/batch 18.67 | loss 0.00009341\n",
      "| Epoch  51 |   550/  737 batches | lr 0.00015 | ms/batch 18.60 | loss 0.00010949\n",
      "| Epoch  51 |   600/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00006894\n",
      "| Epoch  51 |   650/  737 batches | lr 0.00015 | ms/batch 20.32 | loss 0.00005480\n",
      "| Epoch  51 |   700/  737 batches | lr 0.00015 | ms/batch 18.67 | loss 0.00004345\n",
      "\n",
      "Val set: Average loss: 0.00010025\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch  52 |    50/  737 batches | lr 0.00015 | ms/batch 18.77 | loss 0.00010099\n",
      "| Epoch  52 |   100/  737 batches | lr 0.00015 | ms/batch 16.80 | loss 0.00006855\n",
      "| Epoch  52 |   150/  737 batches | lr 0.00015 | ms/batch 18.33 | loss 0.00005506\n",
      "| Epoch  52 |   200/  737 batches | lr 0.00015 | ms/batch 18.54 | loss 0.00005272\n",
      "| Epoch  52 |   250/  737 batches | lr 0.00015 | ms/batch 18.36 | loss 0.00006720\n",
      "| Epoch  52 |   300/  737 batches | lr 0.00015 | ms/batch 16.98 | loss 0.00010208\n",
      "| Epoch  52 |   350/  737 batches | lr 0.00015 | ms/batch 18.69 | loss 0.00005320\n",
      "| Epoch  52 |   400/  737 batches | lr 0.00015 | ms/batch 18.76 | loss 0.00004021\n",
      "| Epoch  52 |   450/  737 batches | lr 0.00015 | ms/batch 18.91 | loss 0.00004551\n",
      "| Epoch  52 |   500/  737 batches | lr 0.00015 | ms/batch 16.97 | loss 0.00005843\n",
      "| Epoch  52 |   550/  737 batches | lr 0.00015 | ms/batch 16.80 | loss 0.00012655\n",
      "| Epoch  52 |   600/  737 batches | lr 0.00015 | ms/batch 20.18 | loss 0.00006549\n",
      "| Epoch  52 |   650/  737 batches | lr 0.00015 | ms/batch 18.36 | loss 0.00009205\n",
      "| Epoch  52 |   700/  737 batches | lr 0.00015 | ms/batch 16.68 | loss 0.00006952\n",
      "\n",
      "Val set: Average loss: 0.00006127\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch  53 |    50/  737 batches | lr 0.00015 | ms/batch 20.39 | loss 0.00004926\n",
      "| Epoch  53 |   100/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00007707\n",
      "| Epoch  53 |   150/  737 batches | lr 0.00015 | ms/batch 18.43 | loss 0.00005348\n",
      "| Epoch  53 |   200/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00005275\n",
      "| Epoch  53 |   250/  737 batches | lr 0.00015 | ms/batch 18.67 | loss 0.00013206\n",
      "| Epoch  53 |   300/  737 batches | lr 0.00015 | ms/batch 20.41 | loss 0.00008147\n",
      "| Epoch  53 |   350/  737 batches | lr 0.00015 | ms/batch 18.62 | loss 0.00006351\n",
      "| Epoch  53 |   400/  737 batches | lr 0.00015 | ms/batch 18.63 | loss 0.00004748\n",
      "| Epoch  53 |   450/  737 batches | lr 0.00015 | ms/batch 18.52 | loss 0.00007487\n",
      "| Epoch  53 |   500/  737 batches | lr 0.00015 | ms/batch 18.55 | loss 0.00011134\n",
      "| Epoch  53 |   550/  737 batches | lr 0.00015 | ms/batch 20.27 | loss 0.00013797\n",
      "| Epoch  53 |   600/  737 batches | lr 0.00015 | ms/batch 18.70 | loss 0.00009115\n",
      "| Epoch  53 |   650/  737 batches | lr 0.00015 | ms/batch 18.86 | loss 0.00006383\n",
      "| Epoch  53 |   700/  737 batches | lr 0.00015 | ms/batch 18.73 | loss 0.00006594\n",
      "\n",
      "Val set: Average loss: 0.00005167\n",
      "\n",
      "| Epoch  54 |    50/  737 batches | lr 0.00015 | ms/batch 18.99 | loss 0.00006100\n",
      "| Epoch  54 |   100/  737 batches | lr 0.00015 | ms/batch 20.26 | loss 0.00005602\n",
      "| Epoch  54 |   150/  737 batches | lr 0.00015 | ms/batch 18.42 | loss 0.00005121\n",
      "| Epoch  54 |   200/  737 batches | lr 0.00015 | ms/batch 16.66 | loss 0.00007434\n",
      "| Epoch  54 |   250/  737 batches | lr 0.00015 | ms/batch 16.93 | loss 0.00011224\n",
      "| Epoch  54 |   300/  737 batches | lr 0.00015 | ms/batch 20.05 | loss 0.00008329\n",
      "| Epoch  54 |   350/  737 batches | lr 0.00015 | ms/batch 16.74 | loss 0.00006836\n",
      "| Epoch  54 |   400/  737 batches | lr 0.00015 | ms/batch 18.20 | loss 0.00004692\n",
      "| Epoch  54 |   450/  737 batches | lr 0.00015 | ms/batch 16.65 | loss 0.00006440\n",
      "| Epoch  54 |   500/  737 batches | lr 0.00015 | ms/batch 18.41 | loss 0.00011390\n",
      "| Epoch  54 |   550/  737 batches | lr 0.00015 | ms/batch 20.18 | loss 0.00012906\n",
      "| Epoch  54 |   600/  737 batches | lr 0.00015 | ms/batch 18.47 | loss 0.00015389\n",
      "| Epoch  54 |   650/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00008206\n",
      "| Epoch  54 |   700/  737 batches | lr 0.00015 | ms/batch 18.80 | loss 0.00005261\n",
      "\n",
      "Val set: Average loss: 0.00018304\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  55 |    50/  737 batches | lr 0.00015 | ms/batch 18.91 | loss 0.00014704\n",
      "| Epoch  55 |   100/  737 batches | lr 0.00015 | ms/batch 18.55 | loss 0.00008216\n",
      "| Epoch  55 |   150/  737 batches | lr 0.00015 | ms/batch 16.63 | loss 0.00005264\n",
      "| Epoch  55 |   200/  737 batches | lr 0.00015 | ms/batch 18.43 | loss 0.00007204\n",
      "| Epoch  55 |   250/  737 batches | lr 0.00015 | ms/batch 18.55 | loss 0.00010610\n",
      "| Epoch  55 |   300/  737 batches | lr 0.00015 | ms/batch 18.69 | loss 0.00007932\n",
      "| Epoch  55 |   350/  737 batches | lr 0.00015 | ms/batch 16.72 | loss 0.00005190\n",
      "| Epoch  55 |   400/  737 batches | lr 0.00015 | ms/batch 16.76 | loss 0.00005255\n",
      "| Epoch  55 |   450/  737 batches | lr 0.00015 | ms/batch 20.32 | loss 0.00006031\n",
      "| Epoch  55 |   500/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00005567\n",
      "| Epoch  55 |   550/  737 batches | lr 0.00015 | ms/batch 17.17 | loss 0.00013303\n",
      "| Epoch  55 |   600/  737 batches | lr 0.00015 | ms/batch 16.88 | loss 0.00011943\n",
      "| Epoch  55 |   650/  737 batches | lr 0.00015 | ms/batch 18.69 | loss 0.00005102\n",
      "| Epoch  55 |   700/  737 batches | lr 0.00015 | ms/batch 18.90 | loss 0.00004326\n",
      "\n",
      "Val set: Average loss: 0.00012866\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  56 |    50/  737 batches | lr 0.00015 | ms/batch 17.17 | loss 0.00010002\n",
      "| Epoch  56 |   100/  737 batches | lr 0.00015 | ms/batch 16.67 | loss 0.00006697\n",
      "| Epoch  56 |   150/  737 batches | lr 0.00015 | ms/batch 20.06 | loss 0.00005190\n",
      "| Epoch  56 |   200/  737 batches | lr 0.00015 | ms/batch 18.51 | loss 0.00005133\n",
      "| Epoch  56 |   250/  737 batches | lr 0.00015 | ms/batch 16.69 | loss 0.00006555\n",
      "| Epoch  56 |   300/  737 batches | lr 0.00015 | ms/batch 16.74 | loss 0.00009592\n",
      "| Epoch  56 |   350/  737 batches | lr 0.00015 | ms/batch 18.73 | loss 0.00005788\n",
      "| Epoch  56 |   400/  737 batches | lr 0.00015 | ms/batch 18.53 | loss 0.00004295\n",
      "| Epoch  56 |   450/  737 batches | lr 0.00015 | ms/batch 18.44 | loss 0.00005073\n",
      "| Epoch  56 |   500/  737 batches | lr 0.00015 | ms/batch 18.56 | loss 0.00005757\n",
      "| Epoch  56 |   550/  737 batches | lr 0.00015 | ms/batch 16.67 | loss 0.00004481\n",
      "| Epoch  56 |   600/  737 batches | lr 0.00015 | ms/batch 18.34 | loss 0.00006287\n",
      "| Epoch  56 |   650/  737 batches | lr 0.00015 | ms/batch 18.62 | loss 0.00004809\n",
      "| Epoch  56 |   700/  737 batches | lr 0.00015 | ms/batch 18.36 | loss 0.00007635\n",
      "\n",
      "Val set: Average loss: 0.00006295\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  57 |    50/  737 batches | lr 0.00015 | ms/batch 19.09 | loss 0.00007103\n",
      "| Epoch  57 |   100/  737 batches | lr 0.00015 | ms/batch 20.32 | loss 0.00005364\n",
      "| Epoch  57 |   150/  737 batches | lr 0.00015 | ms/batch 18.86 | loss 0.00006965\n",
      "| Epoch  57 |   200/  737 batches | lr 0.00015 | ms/batch 18.40 | loss 0.00008941\n",
      "| Epoch  57 |   250/  737 batches | lr 0.00015 | ms/batch 18.66 | loss 0.00012197\n",
      "| Epoch  57 |   300/  737 batches | lr 0.00015 | ms/batch 18.39 | loss 0.00008760\n",
      "| Epoch  57 |   350/  737 batches | lr 0.00015 | ms/batch 18.54 | loss 0.00006198\n",
      "| Epoch  57 |   400/  737 batches | lr 0.00015 | ms/batch 20.00 | loss 0.00004918\n",
      "| Epoch  57 |   450/  737 batches | lr 0.00015 | ms/batch 18.43 | loss 0.00005140\n",
      "| Epoch  57 |   500/  737 batches | lr 0.00015 | ms/batch 18.55 | loss 0.00007373\n",
      "| Epoch  57 |   550/  737 batches | lr 0.00015 | ms/batch 18.50 | loss 0.00005884\n",
      "| Epoch  57 |   600/  737 batches | lr 0.00015 | ms/batch 18.63 | loss 0.00008190\n",
      "| Epoch  57 |   650/  737 batches | lr 0.00015 | ms/batch 20.18 | loss 0.00010654\n",
      "| Epoch  57 |   700/  737 batches | lr 0.00015 | ms/batch 18.69 | loss 0.00006007\n",
      "\n",
      "Val set: Average loss: 0.00005360\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  58 |    50/  737 batches | lr 0.00015 | ms/batch 18.97 | loss 0.00004947\n",
      "| Epoch  58 |   100/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00005130\n",
      "| Epoch  58 |   150/  737 batches | lr 0.00015 | ms/batch 18.45 | loss 0.00014465\n",
      "| Epoch  58 |   200/  737 batches | lr 0.00015 | ms/batch 18.32 | loss 0.00006276\n",
      "| Epoch  58 |   250/  737 batches | lr 0.00015 | ms/batch 16.65 | loss 0.00009037\n",
      "| Epoch  58 |   300/  737 batches | lr 0.00015 | ms/batch 18.43 | loss 0.00007058\n",
      "| Epoch  58 |   350/  737 batches | lr 0.00015 | ms/batch 18.58 | loss 0.00006430\n",
      "| Epoch  58 |   400/  737 batches | lr 0.00015 | ms/batch 18.34 | loss 0.00004891\n",
      "| Epoch  58 |   450/  737 batches | lr 0.00015 | ms/batch 16.65 | loss 0.00006529\n",
      "| Epoch  58 |   500/  737 batches | lr 0.00015 | ms/batch 16.64 | loss 0.00009644\n",
      "| Epoch  58 |   550/  737 batches | lr 0.00015 | ms/batch 20.11 | loss 0.00008369\n",
      "| Epoch  58 |   600/  737 batches | lr 0.00015 | ms/batch 18.61 | loss 0.00005390\n",
      "| Epoch  58 |   650/  737 batches | lr 0.00015 | ms/batch 16.93 | loss 0.00006033\n",
      "| Epoch  58 |   700/  737 batches | lr 0.00015 | ms/batch 16.78 | loss 0.00004871\n",
      "\n",
      "Val set: Average loss: 0.00005435\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  59 |    50/  737 batches | lr 0.00015 | ms/batch 17.03 | loss 0.00005212\n",
      "| Epoch  59 |   100/  737 batches | lr 0.00015 | ms/batch 19.97 | loss 0.00011664\n",
      "| Epoch  59 |   150/  737 batches | lr 0.00015 | ms/batch 18.61 | loss 0.00011143\n",
      "| Epoch  59 |   200/  737 batches | lr 0.00015 | ms/batch 16.73 | loss 0.00007295\n",
      "| Epoch  59 |   250/  737 batches | lr 0.00015 | ms/batch 16.65 | loss 0.00007996\n",
      "| Epoch  59 |   300/  737 batches | lr 0.00015 | ms/batch 19.99 | loss 0.00009284\n",
      "| Epoch  59 |   350/  737 batches | lr 0.00015 | ms/batch 16.76 | loss 0.00010768\n",
      "| Epoch  59 |   400/  737 batches | lr 0.00015 | ms/batch 18.37 | loss 0.00005467\n",
      "| Epoch  59 |   450/  737 batches | lr 0.00015 | ms/batch 18.48 | loss 0.00006130\n",
      "| Epoch  59 |   500/  737 batches | lr 0.00015 | ms/batch 16.65 | loss 0.00006224\n",
      "| Epoch  59 |   550/  737 batches | lr 0.00015 | ms/batch 18.34 | loss 0.00008879\n",
      "| Epoch  59 |   600/  737 batches | lr 0.00015 | ms/batch 18.82 | loss 0.00007954\n",
      "| Epoch  59 |   650/  737 batches | lr 0.00015 | ms/batch 18.89 | loss 0.00006312\n",
      "| Epoch  59 |   700/  737 batches | lr 0.00015 | ms/batch 17.01 | loss 0.00007597\n",
      "\n",
      "Val set: Average loss: 0.00008437\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch  60 |    50/  737 batches | lr 0.00015 | ms/batch 18.81 | loss 0.00006465\n",
      "| Epoch  60 |   100/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00007575\n",
      "| Epoch  60 |   150/  737 batches | lr 0.00015 | ms/batch 18.70 | loss 0.00005863\n",
      "| Epoch  60 |   200/  737 batches | lr 0.00015 | ms/batch 18.43 | loss 0.00005876\n",
      "| Epoch  60 |   250/  737 batches | lr 0.00015 | ms/batch 20.17 | loss 0.00007163\n",
      "| Epoch  60 |   300/  737 batches | lr 0.00015 | ms/batch 18.72 | loss 0.00009241\n",
      "| Epoch  60 |   350/  737 batches | lr 0.00015 | ms/batch 18.44 | loss 0.00010163\n",
      "| Epoch  60 |   400/  737 batches | lr 0.00015 | ms/batch 18.93 | loss 0.00006253\n",
      "| Epoch  60 |   450/  737 batches | lr 0.00015 | ms/batch 18.71 | loss 0.00004910\n",
      "| Epoch  60 |   500/  737 batches | lr 0.00015 | ms/batch 20.13 | loss 0.00005324\n",
      "| Epoch  60 |   550/  737 batches | lr 0.00015 | ms/batch 18.49 | loss 0.00007220\n",
      "| Epoch  60 |   600/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00005425\n",
      "| Epoch  60 |   650/  737 batches | lr 0.00015 | ms/batch 18.44 | loss 0.00004306\n",
      "| Epoch  60 |   700/  737 batches | lr 0.00015 | ms/batch 18.55 | loss 0.00004344\n",
      "\n",
      "Val set: Average loss: 0.00005446\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch  61 |    50/  737 batches | lr 0.00015 | ms/batch 18.96 | loss 0.00005190\n",
      "| Epoch  61 |   100/  737 batches | lr 0.00015 | ms/batch 16.88 | loss 0.00010695\n",
      "| Epoch  61 |   150/  737 batches | lr 0.00015 | ms/batch 16.97 | loss 0.00006001\n",
      "| Epoch  61 |   200/  737 batches | lr 0.00015 | ms/batch 18.48 | loss 0.00005951\n",
      "| Epoch  61 |   250/  737 batches | lr 0.00015 | ms/batch 18.33 | loss 0.00005131\n",
      "| Epoch  61 |   300/  737 batches | lr 0.00015 | ms/batch 18.56 | loss 0.00005113\n",
      "| Epoch  61 |   350/  737 batches | lr 0.00015 | ms/batch 18.27 | loss 0.00008601\n",
      "| Epoch  61 |   400/  737 batches | lr 0.00015 | ms/batch 16.65 | loss 0.00010675\n",
      "| Epoch  61 |   450/  737 batches | lr 0.00015 | ms/batch 18.33 | loss 0.00005095\n",
      "| Epoch  61 |   500/  737 batches | lr 0.00015 | ms/batch 18.38 | loss 0.00005762\n",
      "| Epoch  61 |   550/  737 batches | lr 0.00015 | ms/batch 18.28 | loss 0.00006366\n",
      "| Epoch  61 |   600/  737 batches | lr 0.00015 | ms/batch 16.73 | loss 0.00006799\n",
      "| Epoch  61 |   650/  737 batches | lr 0.00015 | ms/batch 16.62 | loss 0.00004045\n",
      "| Epoch  61 |   700/  737 batches | lr 0.00015 | ms/batch 20.35 | loss 0.00008445\n",
      "\n",
      "Val set: Average loss: 0.00005832\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch  62 |    50/  737 batches | lr 0.00015 | ms/batch 17.27 | loss 0.00007395\n",
      "| Epoch  62 |   100/  737 batches | lr 0.00015 | ms/batch 18.41 | loss 0.00011144\n",
      "| Epoch  62 |   150/  737 batches | lr 0.00015 | ms/batch 18.69 | loss 0.00007622\n",
      "| Epoch  62 |   200/  737 batches | lr 0.00015 | ms/batch 18.46 | loss 0.00005746\n",
      "| Epoch  62 |   250/  737 batches | lr 0.00015 | ms/batch 16.71 | loss 0.00005489\n",
      "| Epoch  62 |   300/  737 batches | lr 0.00015 | ms/batch 16.68 | loss 0.00005586\n",
      "| Epoch  62 |   350/  737 batches | lr 0.00015 | ms/batch 20.06 | loss 0.00006453\n",
      "| Epoch  62 |   400/  737 batches | lr 0.00015 | ms/batch 18.31 | loss 0.00008880\n",
      "| Epoch  62 |   450/  737 batches | lr 0.00015 | ms/batch 16.62 | loss 0.00006171\n",
      "| Epoch  62 |   500/  737 batches | lr 0.00015 | ms/batch 16.76 | loss 0.00005373\n",
      "| Epoch  62 |   550/  737 batches | lr 0.00015 | ms/batch 18.57 | loss 0.00007623\n",
      "| Epoch  62 |   600/  737 batches | lr 0.00015 | ms/batch 18.45 | loss 0.00006642\n",
      "| Epoch  62 |   650/  737 batches | lr 0.00015 | ms/batch 18.49 | loss 0.00004707\n",
      "| Epoch  62 |   700/  737 batches | lr 0.00015 | ms/batch 18.33 | loss 0.00006836\n",
      "\n",
      "Val set: Average loss: 0.00008849\n",
      "\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch  63 |    50/  737 batches | lr 0.00015 | ms/batch 18.90 | loss 0.00007602\n",
      "| Epoch  63 |   100/  737 batches | lr 0.00015 | ms/batch 20.19 | loss 0.00010299\n",
      "| Epoch  63 |   150/  737 batches | lr 0.00015 | ms/batch 18.41 | loss 0.00009678\n",
      "| Epoch  63 |   200/  737 batches | lr 0.00015 | ms/batch 18.46 | loss 0.00011411\n",
      "| Epoch  63 |   250/  737 batches | lr 0.00015 | ms/batch 18.67 | loss 0.00008869\n",
      "| Epoch  63 |   300/  737 batches | lr 0.00015 | ms/batch 18.66 | loss 0.00008655\n",
      "| Epoch  63 |   350/  737 batches | lr 0.00015 | ms/batch 20.51 | loss 0.00004726\n",
      "| Epoch  63 |   400/  737 batches | lr 0.00015 | ms/batch 19.02 | loss 0.00005116\n",
      "| Epoch  63 |   450/  737 batches | lr 0.00015 | ms/batch 18.74 | loss 0.00007379\n",
      "| Epoch  63 |   500/  737 batches | lr 0.00015 | ms/batch 19.04 | loss 0.00006080\n",
      "| Epoch  63 |   550/  737 batches | lr 0.00015 | ms/batch 18.83 | loss 0.00012472\n",
      "| Epoch  63 |   600/  737 batches | lr 0.00015 | ms/batch 20.37 | loss 0.00005930\n",
      "| Epoch  63 |   650/  737 batches | lr 0.00015 | ms/batch 18.62 | loss 0.00004320\n",
      "| Epoch  63 |   700/  737 batches | lr 0.00015 | ms/batch 18.43 | loss 0.00004770\n",
      "\n",
      "Val set: Average loss: 0.00014100\n",
      "\n",
      "EarlyStopping counter: 10 out of 20\n",
      "| Epoch  64 |    50/  737 batches | lr 0.00015 | ms/batch 20.39 | loss 0.00011348\n",
      "| Epoch  64 |   100/  737 batches | lr 0.00015 | ms/batch 18.31 | loss 0.00009462\n",
      "| Epoch  64 |   150/  737 batches | lr 0.00015 | ms/batch 16.73 | loss 0.00007077\n",
      "| Epoch  64 |   200/  737 batches | lr 0.00015 | ms/batch 16.67 | loss 0.00009870\n",
      "| Epoch  64 |   250/  737 batches | lr 0.00015 | ms/batch 20.09 | loss 0.00013621\n",
      "| Epoch  64 |   300/  737 batches | lr 0.00015 | ms/batch 16.75 | loss 0.00006357\n",
      "| Epoch  64 |   350/  737 batches | lr 0.00015 | ms/batch 18.56 | loss 0.00004778\n",
      "| Epoch  64 |   400/  737 batches | lr 0.00015 | ms/batch 16.68 | loss 0.00004441\n",
      "| Epoch  64 |   450/  737 batches | lr 0.00015 | ms/batch 18.68 | loss 0.00005788\n",
      "| Epoch  64 |   500/  737 batches | lr 0.00015 | ms/batch 20.51 | loss 0.00007789\n",
      "| Epoch  64 |   550/  737 batches | lr 0.00015 | ms/batch 18.83 | loss 0.00009237\n",
      "| Epoch  64 |   600/  737 batches | lr 0.00015 | ms/batch 18.83 | loss 0.00006334\n",
      "| Epoch  64 |   650/  737 batches | lr 0.00015 | ms/batch 18.72 | loss 0.00003990\n",
      "| Epoch  64 |   700/  737 batches | lr 0.00015 | ms/batch 18.41 | loss 0.00004014\n",
      "\n",
      "Val set: Average loss: 0.00013787\n",
      "\n",
      "EarlyStopping counter: 11 out of 20\n",
      "| Epoch  65 |    50/  737 batches | lr 0.00015 | ms/batch 18.81 | loss 0.00012613\n",
      "| Epoch  65 |   100/  737 batches | lr 0.00015 | ms/batch 16.64 | loss 0.00007979\n",
      "| Epoch  65 |   150/  737 batches | lr 0.00015 | ms/batch 16.70 | loss 0.00004224\n",
      "| Epoch  65 |   200/  737 batches | lr 0.00015 | ms/batch 18.44 | loss 0.00006707\n",
      "| Epoch  65 |   250/  737 batches | lr 0.00015 | ms/batch 18.49 | loss 0.00005584\n",
      "| Epoch  65 |   300/  737 batches | lr 0.00015 | ms/batch 18.37 | loss 0.00004862\n",
      "| Epoch  65 |   350/  737 batches | lr 0.00015 | ms/batch 18.62 | loss 0.00003669\n",
      "| Epoch  65 |   400/  737 batches | lr 0.00015 | ms/batch 16.99 | loss 0.00003588\n",
      "| Epoch  65 |   450/  737 batches | lr 0.00015 | ms/batch 18.46 | loss 0.00003975\n",
      "| Epoch  65 |   500/  737 batches | lr 0.00015 | ms/batch 18.66 | loss 0.00005125\n",
      "| Epoch  65 |   550/  737 batches | lr 0.00015 | ms/batch 18.64 | loss 0.00004340\n",
      "| Epoch  65 |   600/  737 batches | lr 0.00015 | ms/batch 17.17 | loss 0.00006184\n",
      "| Epoch  65 |   650/  737 batches | lr 0.00015 | ms/batch 16.84 | loss 0.00004376\n",
      "| Epoch  65 |   700/  737 batches | lr 0.00015 | ms/batch 20.21 | loss 0.00005773\n",
      "\n",
      "Val set: Average loss: 0.00009025\n",
      "\n",
      "EarlyStopping counter: 12 out of 20\n",
      "| Epoch  66 |    50/  737 batches | lr 0.00015 | ms/batch 17.08 | loss 0.00006624\n",
      "| Epoch  66 |   100/  737 batches | lr 0.00015 | ms/batch 18.25 | loss 0.00006195\n",
      "| Epoch  66 |   150/  737 batches | lr 0.00015 | ms/batch 18.41 | loss 0.00005909\n",
      "| Epoch  66 |   200/  737 batches | lr 0.00015 | ms/batch 18.33 | loss 0.00004516\n",
      "| Epoch  66 |   250/  737 batches | lr 0.00015 | ms/batch 16.65 | loss 0.00008862\n",
      "| Epoch  66 |   300/  737 batches | lr 0.00015 | ms/batch 16.77 | loss 0.00005177\n",
      "| Epoch  66 |   350/  737 batches | lr 0.00015 | ms/batch 18.32 | loss 0.00005857\n",
      "| Epoch  66 |   400/  737 batches | lr 0.00015 | ms/batch 17.00 | loss 0.00006029\n",
      "| Epoch  66 |   450/  737 batches | lr 0.00015 | ms/batch 18.85 | loss 0.00008251\n",
      "| Epoch  66 |   500/  737 batches | lr 0.00015 | ms/batch 18.66 | loss 0.00008194\n",
      "| Epoch  66 |   550/  737 batches | lr 0.00015 | ms/batch 20.42 | loss 0.00006688\n",
      "| Epoch  66 |   600/  737 batches | lr 0.00015 | ms/batch 18.93 | loss 0.00004196\n",
      "| Epoch  66 |   650/  737 batches | lr 0.00015 | ms/batch 18.67 | loss 0.00005176\n",
      "| Epoch  66 |   700/  737 batches | lr 0.00015 | ms/batch 18.46 | loss 0.00004812\n",
      "\n",
      "Val set: Average loss: 0.00006714\n",
      "\n",
      "EarlyStopping counter: 13 out of 20\n",
      "| Epoch  67 |    50/  737 batches | lr 0.00015 | ms/batch 17.23 | loss 0.00007537\n",
      "| Epoch  67 |   100/  737 batches | lr 0.00015 | ms/batch 18.35 | loss 0.00007000\n",
      "| Epoch  67 |   150/  737 batches | lr 0.00015 | ms/batch 16.69 | loss 0.00007710\n",
      "| Epoch  67 |   200/  737 batches | lr 0.00015 | ms/batch 18.39 | loss 0.00004980\n",
      "| Epoch  67 |   250/  737 batches | lr 0.00015 | ms/batch 18.48 | loss 0.00005356\n",
      "| Epoch  67 |   300/  737 batches | lr 0.00015 | ms/batch 20.14 | loss 0.00004978\n",
      "| Epoch  67 |   350/  737 batches | lr 0.00015 | ms/batch 18.50 | loss 0.00008996\n",
      "| Epoch  67 |   400/  737 batches | lr 0.00015 | ms/batch 18.57 | loss 0.00007297\n",
      "| Epoch  67 |   450/  737 batches | lr 0.00015 | ms/batch 18.43 | loss 0.00006232\n",
      "| Epoch  67 |   500/  737 batches | lr 0.00015 | ms/batch 18.55 | loss 0.00007661\n",
      "| Epoch  67 |   550/  737 batches | lr 0.00015 | ms/batch 20.01 | loss 0.00005645\n",
      "| Epoch  67 |   600/  737 batches | lr 0.00015 | ms/batch 18.62 | loss 0.00004299\n",
      "| Epoch  67 |   650/  737 batches | lr 0.00015 | ms/batch 18.62 | loss 0.00009406\n",
      "| Epoch  67 |   700/  737 batches | lr 0.00015 | ms/batch 18.44 | loss 0.00009207\n",
      "\n",
      "Val set: Average loss: 0.00009919\n",
      "\n",
      "EarlyStopping counter: 14 out of 20\n",
      "| Epoch  68 |    50/  737 batches | lr 0.00015 | ms/batch 17.24 | loss 0.00005665\n",
      "| Epoch  68 |   100/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00007440\n",
      "| Epoch  68 |   150/  737 batches | lr 0.00015 | ms/batch 18.58 | loss 0.00005598\n",
      "| Epoch  68 |   200/  737 batches | lr 0.00015 | ms/batch 16.98 | loss 0.00004959\n",
      "| Epoch  68 |   250/  737 batches | lr 0.00015 | ms/batch 18.62 | loss 0.00005193\n",
      "| Epoch  68 |   300/  737 batches | lr 0.00015 | ms/batch 18.47 | loss 0.00006625\n",
      "| Epoch  68 |   350/  737 batches | lr 0.00015 | ms/batch 18.53 | loss 0.00009469\n",
      "| Epoch  68 |   400/  737 batches | lr 0.00015 | ms/batch 16.70 | loss 0.00005202\n",
      "| Epoch  68 |   450/  737 batches | lr 0.00015 | ms/batch 16.69 | loss 0.00004892\n",
      "| Epoch  68 |   500/  737 batches | lr 0.00015 | ms/batch 20.02 | loss 0.00008569\n",
      "| Epoch  68 |   550/  737 batches | lr 0.00015 | ms/batch 18.57 | loss 0.00008654\n",
      "| Epoch  68 |   600/  737 batches | lr 0.00015 | ms/batch 16.67 | loss 0.00007375\n",
      "| Epoch  68 |   650/  737 batches | lr 0.00015 | ms/batch 16.79 | loss 0.00005133\n",
      "| Epoch  68 |   700/  737 batches | lr 0.00015 | ms/batch 19.99 | loss 0.00008160\n",
      "\n",
      "Val set: Average loss: 0.00008675\n",
      "\n",
      "EarlyStopping counter: 15 out of 20\n",
      "| Epoch  69 |    50/  737 batches | lr 0.00015 | ms/batch 17.20 | loss 0.00008108\n",
      "| Epoch  69 |   100/  737 batches | lr 0.00015 | ms/batch 16.80 | loss 0.00007126\n",
      "| Epoch  69 |   150/  737 batches | lr 0.00015 | ms/batch 20.29 | loss 0.00008785\n",
      "| Epoch  69 |   200/  737 batches | lr 0.00015 | ms/batch 18.58 | loss 0.00006963\n",
      "| Epoch  69 |   250/  737 batches | lr 0.00015 | ms/batch 16.79 | loss 0.00006525\n",
      "| Epoch  69 |   300/  737 batches | lr 0.00015 | ms/batch 16.92 | loss 0.00005887\n",
      "| Epoch  69 |   350/  737 batches | lr 0.00015 | ms/batch 20.16 | loss 0.00004284\n",
      "| Epoch  69 |   400/  737 batches | lr 0.00015 | ms/batch 16.85 | loss 0.00004032\n",
      "| Epoch  69 |   450/  737 batches | lr 0.00015 | ms/batch 18.22 | loss 0.00004641\n",
      "| Epoch  69 |   500/  737 batches | lr 0.00015 | ms/batch 16.67 | loss 0.00008644\n",
      "| Epoch  69 |   550/  737 batches | lr 0.00015 | ms/batch 18.39 | loss 0.00012914\n",
      "| Epoch  69 |   600/  737 batches | lr 0.00015 | ms/batch 19.98 | loss 0.00006176\n",
      "| Epoch  69 |   650/  737 batches | lr 0.00015 | ms/batch 18.61 | loss 0.00004456\n",
      "| Epoch  69 |   700/  737 batches | lr 0.00015 | ms/batch 18.76 | loss 0.00006188\n",
      "\n",
      "Val set: Average loss: 0.00007322\n",
      "\n",
      "EarlyStopping counter: 16 out of 20\n",
      "| Epoch  70 |    50/  737 batches | lr 0.00015 | ms/batch 20.62 | loss 0.00010082\n",
      "| Epoch  70 |   100/  737 batches | lr 0.00015 | ms/batch 18.58 | loss 0.00005524\n",
      "| Epoch  70 |   150/  737 batches | lr 0.00015 | ms/batch 16.67 | loss 0.00007024\n",
      "| Epoch  70 |   200/  737 batches | lr 0.00015 | ms/batch 16.67 | loss 0.00008945\n",
      "| Epoch  70 |   250/  737 batches | lr 0.00015 | ms/batch 20.03 | loss 0.00012659\n",
      "| Epoch  70 |   300/  737 batches | lr 0.00015 | ms/batch 16.73 | loss 0.00005879\n",
      "| Epoch  70 |   350/  737 batches | lr 0.00015 | ms/batch 18.27 | loss 0.00004083\n",
      "| Epoch  70 |   400/  737 batches | lr 0.00015 | ms/batch 16.90 | loss 0.00004068\n",
      "| Epoch  70 |   450/  737 batches | lr 0.00015 | ms/batch 18.50 | loss 0.00006357\n",
      "| Epoch  70 |   500/  737 batches | lr 0.00015 | ms/batch 18.53 | loss 0.00006401\n",
      "| Epoch  70 |   550/  737 batches | lr 0.00015 | ms/batch 20.06 | loss 0.00011174\n",
      "| Epoch  70 |   600/  737 batches | lr 0.00015 | ms/batch 18.60 | loss 0.00006737\n",
      "| Epoch  70 |   650/  737 batches | lr 0.00015 | ms/batch 18.68 | loss 0.00004003\n",
      "| Epoch  70 |   700/  737 batches | lr 0.00015 | ms/batch 18.82 | loss 0.00004457\n",
      "\n",
      "Val set: Average loss: 0.00009435\n",
      "\n",
      "EarlyStopping counter: 17 out of 20\n",
      "| Epoch  71 |    50/  737 batches | lr 0.00015 | ms/batch 18.94 | loss 0.00007681\n",
      "| Epoch  71 |   100/  737 batches | lr 0.00015 | ms/batch 16.68 | loss 0.00004405\n",
      "| Epoch  71 |   150/  737 batches | lr 0.00015 | ms/batch 16.67 | loss 0.00004830\n",
      "| Epoch  71 |   200/  737 batches | lr 0.00015 | ms/batch 18.49 | loss 0.00004524\n",
      "| Epoch  71 |   250/  737 batches | lr 0.00015 | ms/batch 18.23 | loss 0.00004214\n",
      "| Epoch  71 |   300/  737 batches | lr 0.00015 | ms/batch 18.38 | loss 0.00004904\n",
      "| Epoch  71 |   350/  737 batches | lr 0.00015 | ms/batch 18.31 | loss 0.00005766\n",
      "| Epoch  71 |   400/  737 batches | lr 0.00015 | ms/batch 16.65 | loss 0.00003303\n",
      "| Epoch  71 |   450/  737 batches | lr 0.00015 | ms/batch 18.30 | loss 0.00003482\n",
      "| Epoch  71 |   500/  737 batches | lr 0.00015 | ms/batch 18.62 | loss 0.00005076\n",
      "| Epoch  71 |   550/  737 batches | lr 0.00015 | ms/batch 18.35 | loss 0.00004610\n",
      "| Epoch  71 |   600/  737 batches | lr 0.00015 | ms/batch 16.69 | loss 0.00009031\n",
      "| Epoch  71 |   650/  737 batches | lr 0.00015 | ms/batch 16.66 | loss 0.00004177\n",
      "| Epoch  71 |   700/  737 batches | lr 0.00015 | ms/batch 20.34 | loss 0.00005406\n",
      "\n",
      "Val set: Average loss: 0.00005184\n",
      "\n",
      "EarlyStopping counter: 18 out of 20\n",
      "| Epoch  72 |    50/  737 batches | lr 0.00015 | ms/batch 17.24 | loss 0.00006198\n",
      "| Epoch  72 |   100/  737 batches | lr 0.00015 | ms/batch 18.52 | loss 0.00004861\n",
      "| Epoch  72 |   150/  737 batches | lr 0.00015 | ms/batch 18.48 | loss 0.00005678\n",
      "| Epoch  72 |   200/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00004989\n",
      "| Epoch  72 |   250/  737 batches | lr 0.00015 | ms/batch 16.68 | loss 0.00005587\n",
      "| Epoch  72 |   300/  737 batches | lr 0.00015 | ms/batch 16.66 | loss 0.00005807\n",
      "| Epoch  72 |   350/  737 batches | lr 0.00015 | ms/batch 20.25 | loss 0.00016189\n",
      "| Epoch  72 |   400/  737 batches | lr 0.00015 | ms/batch 18.46 | loss 0.00004779\n",
      "| Epoch  72 |   450/  737 batches | lr 0.00015 | ms/batch 16.91 | loss 0.00004088\n",
      "| Epoch  72 |   500/  737 batches | lr 0.00015 | ms/batch 16.81 | loss 0.00004167\n",
      "| Epoch  72 |   550/  737 batches | lr 0.00015 | ms/batch 18.67 | loss 0.00005166\n",
      "| Epoch  72 |   600/  737 batches | lr 0.00015 | ms/batch 20.24 | loss 0.00006819\n",
      "| Epoch  72 |   650/  737 batches | lr 0.00015 | ms/batch 18.59 | loss 0.00005474\n",
      "| Epoch  72 |   700/  737 batches | lr 0.00015 | ms/batch 18.77 | loss 0.00004982\n",
      "\n",
      "Val set: Average loss: 0.00008727\n",
      "\n",
      "EarlyStopping counter: 19 out of 20\n",
      "| Epoch  73 |    50/  737 batches | lr 0.00015 | ms/batch 19.11 | loss 0.00004925\n",
      "| Epoch  73 |   100/  737 batches | lr 0.00015 | ms/batch 18.64 | loss 0.00005646\n",
      "| Epoch  73 |   150/  737 batches | lr 0.00015 | ms/batch 16.68 | loss 0.00008137\n",
      "| Epoch  73 |   200/  737 batches | lr 0.00015 | ms/batch 18.34 | loss 0.00004878\n",
      "| Epoch  73 |   250/  737 batches | lr 0.00015 | ms/batch 18.45 | loss 0.00004616\n",
      "| Epoch  73 |   300/  737 batches | lr 0.00015 | ms/batch 18.52 | loss 0.00005631\n",
      "| Epoch  73 |   350/  737 batches | lr 0.00015 | ms/batch 16.75 | loss 0.00008927\n",
      "| Epoch  73 |   400/  737 batches | lr 0.00015 | ms/batch 16.70 | loss 0.00004002\n",
      "| Epoch  73 |   450/  737 batches | lr 0.00015 | ms/batch 19.96 | loss 0.00003408\n",
      "| Epoch  73 |   500/  737 batches | lr 0.00015 | ms/batch 18.32 | loss 0.00004842\n",
      "| Epoch  73 |   550/  737 batches | lr 0.00015 | ms/batch 16.93 | loss 0.00009488\n",
      "| Epoch  73 |   600/  737 batches | lr 0.00015 | ms/batch 16.70 | loss 0.00008223\n",
      "| Epoch  73 |   650/  737 batches | lr 0.00015 | ms/batch 18.39 | loss 0.00004605\n",
      "| Epoch  73 |   700/  737 batches | lr 0.00015 | ms/batch 18.32 | loss 0.00005646\n",
      "\n",
      "Val set: Average loss: 0.00006203\n",
      "\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Stopping at Epoch: 73\n"
     ]
    }
   ],
   "source": [
    "load = False\n",
    "save_model_path = '../models/final_heston_model.chkpt'\n",
    "val_err_df_path = '../results/val_final_heston_model.csv'\n",
    "\n",
    "if not load:\n",
    "  train_losses, val_losses = train(\n",
    "      epochs,\n",
    "      batch_size,\n",
    "      model,\n",
    "      optimizer,\n",
    "      loss_fn,\n",
    "      X_train,\n",
    "      y_train,\n",
    "      X_val,\n",
    "      y_val)\n",
    "  val_err_df = pd.DataFrame({\n",
    "      'Training': train_losses,\n",
    "      'Validation': val_losses})\n",
    "  val_err_df.to_csv(val_err_df_path)\n",
    "  torch.save(model.state_dict(), save_model_path)\n",
    "else:\n",
    "  model = Net(input_size, output_size, hidden_size, num_layers, act_fn)\n",
    "  model.load_state_dict(torch.load(save_model_path, map_location=device))\n",
    "  model = model.to(device)\n",
    "  val_err_df = pd.read_csv(val_err_df_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650894219921,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "KkenWSRYvDpl",
    "outputId": "37746aa5-0b46-4a80-d14d-397868e89361",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_err_df[1:].plot(xlabel='epoch', ylabel='MSE')\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlG5Ky25vDpm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1650894219922,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "TBZ4eHAdvDpn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 30\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_out = model(X_test[0:test_size].to(device))\n",
    "\n",
    "test_out = output_sc.inverse_transform(test_out.cpu().detach().numpy())\n",
    "real_out = output_sc.inverse_transform(y_test[0:test_size].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1650894219926,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "wK1WeGIZvDpo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['Price', 'Strike', 'Kappa', 'Rho', 'Theta', 'Xi', 'V_0',\n",
    "       'Interest Rate', 'Time to Expiration', 'C', 'P']\n",
    "test_options = pd.DataFrame(input_sc.inverse_transform(X_test[0:test_size].detach().cpu().numpy()), columns=cols)\n",
    "test_options['Prediction'] = test_out\n",
    "test_options['Real'] = real_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1650894220545,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "TNw5-1jgvDpr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_options['Moneyness'] = test_options.Price / test_options.Strike\n",
    "test_options['Abs Error'] = np.abs(test_options.Prediction - test_options.Real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1650894220546,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "lAC6AdeVvDpr",
    "outputId": "78bd13cc-c5a0-4746-f402-9e84d8b78696",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         Price      Strike     Kappa       Rho     Theta        Xi       V_0  \\\n22  101.000053  100.994873  1.306733 -0.282550  0.063884  0.056293  0.345962   \n3   115.996803   91.001205  0.809019 -0.214292  0.486074  0.305426  0.465927   \n18  100.000237   79.008804  1.886015 -0.080568  0.268567  0.037546  0.365491   \n14  101.000053  160.030640  0.441473 -0.495600  0.282729  0.419668  0.053532   \n10   86.998978  173.017532  1.793037 -0.898135  0.433934  0.209367  0.486851   \n15  100.000237   57.032486  0.881358 -0.379417  0.128283  0.063763  0.155870   \n11  100.000237   83.006271  1.107394 -0.298662  0.219369  0.441657  0.458826   \n26  100.000237   62.024445  0.214663 -0.574230  0.423929  0.022183  0.388191   \n28  110.998192  179.998474  0.894530 -0.789025  0.498756  0.010624  0.216169   \n16  100.000237   98.996140  1.843753 -0.362289  0.378694  0.173918  0.479750   \n29  100.000237  150.007721  1.003907 -0.005206  0.144630  0.167716  0.423445   \n12  111.999382  157.027664  0.162257 -0.346176  0.005966  0.096042  0.340065   \n8   100.000237   79.008804  1.810506 -0.489986  0.033022  0.424320  0.236206   \n5   119.000381  177.970490  1.700059 -0.272527  0.273446  0.499447  0.381089   \n4   100.000237   64.013428  0.520082 -0.871746  0.435766  0.169760  0.252438   \n19  100.000237   92.999939  1.198259 -0.110510  0.057824  0.484365  0.309789   \n13  100.000237   62.024445  0.899884 -0.772531  0.293228  0.100975  0.173560   \n17  101.000053  124.999176  0.079422 -0.249436  0.191908  0.210460  0.278809   \n24   99.000420   81.992279  1.697242 -0.454589  0.002021  0.099283  0.378426   \n21  100.000237   83.006271  1.073231 -0.038701  0.463104  0.392042  0.085235   \n23   89.995193   54.029510  0.506840 -0.728633  0.369394  0.112181  0.247809   \n25  100.000237  112.000099  1.618351 -0.427248  0.311758  0.265387  0.082825   \n20  100.000237   80.997787  0.331308 -0.006729  0.322010  0.444758  0.355917   \n27  100.000237  123.994934  0.853535 -0.363494  0.375735  0.181882  0.495855   \n7    98.001068   53.015518  1.452399 -0.810086  0.039505  0.261969  0.259040   \n9   100.000237  148.018738  0.100272 -0.180037  0.220708  0.111194  0.437267   \n0    91.997589  165.022598  1.689353 -0.744619  0.207761  0.196682  0.179774   \n6   100.000237  101.999115  1.182622 -0.249436  0.069802  0.078282  0.147880   \n1   100.000237   90.006714  1.781204 -0.265676  0.151816  0.362019  0.332520   \n2   119.000381  118.998100  0.743793 -0.631957  0.020763  0.349615  0.302988   \n\n    Interest Rate  Time to Expiration         C         P  Prediction  \\\n22       0.093031            0.107676  0.999997  0.000003    2.774369   \n3        0.075435            0.799731 -0.000003  1.000003    0.322973   \n18       0.098157            0.492871 -0.000003  1.000003    0.057032   \n14       0.067258            0.938086  0.999997  0.000003   -0.033219   \n10       0.027186            0.135572  0.999997  0.000003    0.041340   \n15       0.052979            0.476809 -0.000003  1.000003    0.047714   \n11       0.067987            1.057843 -0.000003  1.000003    0.514701   \n26       0.087424            0.768031 -0.000003  1.000003   -0.043929   \n28       0.059782            0.159805 -0.000003  1.000003   64.949570   \n16       0.090164            0.920052  0.999997  0.000003    4.346229   \n29       0.069522            0.455535  0.999997  0.000003    0.166275   \n12       0.040101            0.373818  0.999997  0.000003    0.496115   \n8        0.034151            0.187983 -0.000003  1.000003    0.175831   \n5        0.035153            0.197282  0.999997  0.000003    0.090009   \n4        0.041319            0.708434  0.999997  0.000003   34.642925   \n19       0.062862            0.422144  0.999997  0.000003    6.786076   \n13       0.062805            0.999514  0.999997  0.000003   35.173611   \n17       0.081613            0.361983 -0.000003  1.000003   22.789925   \n24       0.062373            0.476528  0.999997  0.000003   15.451066   \n21       0.053101            0.103731 -0.000003  1.000003    0.169466   \n23       0.053009            0.530735  0.999997  0.000003   34.415306   \n25       0.031588            0.560076 -0.000003  1.000003   11.234859   \n20       0.016986            0.419467  0.999997  0.000003   20.140936   \n27       0.065431            0.112466 -0.000003  1.000003   21.037516   \n7        0.057435            0.341413  0.999997  0.000003   42.543961   \n9        0.079532            0.365224 -0.000003  1.000003   45.086964   \n0        0.091636            0.966828 -0.000003  1.000003   67.006470   \n6        0.025334            0.582037  0.999997  0.000003    2.812880   \n1        0.010186            0.874121 -0.000003  1.000003    5.030976   \n2        0.066769            0.478711  0.999997  0.000003    4.317695   \n\n         Real  Moneyness  Abs Error  \n22   2.761820   1.000051   0.012548  \n3    0.336961   1.274673   0.013988  \n18   0.029883   1.265685   0.027149  \n14  -0.001884   0.631130   0.031335  \n10  -0.001884   0.502833   0.043224  \n15  -0.001884   1.753391   0.049598  \n11   0.464028   1.204731   0.050673  \n26   0.008705   1.612271   0.052633  \n28  65.003410   0.616662   0.053841  \n16   4.286623   1.010143   0.059607  \n29   0.104005   0.666634   0.062269  \n12   0.432261   0.713246   0.063854  \n8    0.241661   1.265685   0.065829  \n5    0.156950   0.668652   0.066940  \n4   34.719139   1.562176   0.076214  \n19   6.880905   1.075272   0.094829  \n13  35.280350   1.612271   0.106739  \n17  22.674259   0.808006   0.115665  \n24  15.578036   1.207436   0.126970  \n21   0.040472   1.204731   0.128994  \n23  34.560307   1.665667   0.145000  \n25  11.079407   0.892858   0.155453  \n20  19.984676   1.234605   0.156260  \n27  20.829142   0.806486   0.208374  \n7   42.787884   1.848536   0.243923  \n9   44.842133   0.675592   0.244831  \n0   66.739990   0.557485   0.266479  \n6    3.164198   0.980403   0.351318  \n1    5.499053   1.111031   0.468077  \n2    4.794890   1.000019   0.477195  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>Strike</th>\n      <th>Kappa</th>\n      <th>Rho</th>\n      <th>Theta</th>\n      <th>Xi</th>\n      <th>V_0</th>\n      <th>Interest Rate</th>\n      <th>Time to Expiration</th>\n      <th>C</th>\n      <th>P</th>\n      <th>Prediction</th>\n      <th>Real</th>\n      <th>Moneyness</th>\n      <th>Abs Error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22</th>\n      <td>101.000053</td>\n      <td>100.994873</td>\n      <td>1.306733</td>\n      <td>-0.282550</td>\n      <td>0.063884</td>\n      <td>0.056293</td>\n      <td>0.345962</td>\n      <td>0.093031</td>\n      <td>0.107676</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>2.774369</td>\n      <td>2.761820</td>\n      <td>1.000051</td>\n      <td>0.012548</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>115.996803</td>\n      <td>91.001205</td>\n      <td>0.809019</td>\n      <td>-0.214292</td>\n      <td>0.486074</td>\n      <td>0.305426</td>\n      <td>0.465927</td>\n      <td>0.075435</td>\n      <td>0.799731</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.322973</td>\n      <td>0.336961</td>\n      <td>1.274673</td>\n      <td>0.013988</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>100.000237</td>\n      <td>79.008804</td>\n      <td>1.886015</td>\n      <td>-0.080568</td>\n      <td>0.268567</td>\n      <td>0.037546</td>\n      <td>0.365491</td>\n      <td>0.098157</td>\n      <td>0.492871</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.057032</td>\n      <td>0.029883</td>\n      <td>1.265685</td>\n      <td>0.027149</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>101.000053</td>\n      <td>160.030640</td>\n      <td>0.441473</td>\n      <td>-0.495600</td>\n      <td>0.282729</td>\n      <td>0.419668</td>\n      <td>0.053532</td>\n      <td>0.067258</td>\n      <td>0.938086</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>-0.033219</td>\n      <td>-0.001884</td>\n      <td>0.631130</td>\n      <td>0.031335</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>86.998978</td>\n      <td>173.017532</td>\n      <td>1.793037</td>\n      <td>-0.898135</td>\n      <td>0.433934</td>\n      <td>0.209367</td>\n      <td>0.486851</td>\n      <td>0.027186</td>\n      <td>0.135572</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>0.041340</td>\n      <td>-0.001884</td>\n      <td>0.502833</td>\n      <td>0.043224</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>100.000237</td>\n      <td>57.032486</td>\n      <td>0.881358</td>\n      <td>-0.379417</td>\n      <td>0.128283</td>\n      <td>0.063763</td>\n      <td>0.155870</td>\n      <td>0.052979</td>\n      <td>0.476809</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.047714</td>\n      <td>-0.001884</td>\n      <td>1.753391</td>\n      <td>0.049598</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>100.000237</td>\n      <td>83.006271</td>\n      <td>1.107394</td>\n      <td>-0.298662</td>\n      <td>0.219369</td>\n      <td>0.441657</td>\n      <td>0.458826</td>\n      <td>0.067987</td>\n      <td>1.057843</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.514701</td>\n      <td>0.464028</td>\n      <td>1.204731</td>\n      <td>0.050673</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>100.000237</td>\n      <td>62.024445</td>\n      <td>0.214663</td>\n      <td>-0.574230</td>\n      <td>0.423929</td>\n      <td>0.022183</td>\n      <td>0.388191</td>\n      <td>0.087424</td>\n      <td>0.768031</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>-0.043929</td>\n      <td>0.008705</td>\n      <td>1.612271</td>\n      <td>0.052633</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>110.998192</td>\n      <td>179.998474</td>\n      <td>0.894530</td>\n      <td>-0.789025</td>\n      <td>0.498756</td>\n      <td>0.010624</td>\n      <td>0.216169</td>\n      <td>0.059782</td>\n      <td>0.159805</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>64.949570</td>\n      <td>65.003410</td>\n      <td>0.616662</td>\n      <td>0.053841</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>100.000237</td>\n      <td>98.996140</td>\n      <td>1.843753</td>\n      <td>-0.362289</td>\n      <td>0.378694</td>\n      <td>0.173918</td>\n      <td>0.479750</td>\n      <td>0.090164</td>\n      <td>0.920052</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>4.346229</td>\n      <td>4.286623</td>\n      <td>1.010143</td>\n      <td>0.059607</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>100.000237</td>\n      <td>150.007721</td>\n      <td>1.003907</td>\n      <td>-0.005206</td>\n      <td>0.144630</td>\n      <td>0.167716</td>\n      <td>0.423445</td>\n      <td>0.069522</td>\n      <td>0.455535</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>0.166275</td>\n      <td>0.104005</td>\n      <td>0.666634</td>\n      <td>0.062269</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>111.999382</td>\n      <td>157.027664</td>\n      <td>0.162257</td>\n      <td>-0.346176</td>\n      <td>0.005966</td>\n      <td>0.096042</td>\n      <td>0.340065</td>\n      <td>0.040101</td>\n      <td>0.373818</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>0.496115</td>\n      <td>0.432261</td>\n      <td>0.713246</td>\n      <td>0.063854</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>100.000237</td>\n      <td>79.008804</td>\n      <td>1.810506</td>\n      <td>-0.489986</td>\n      <td>0.033022</td>\n      <td>0.424320</td>\n      <td>0.236206</td>\n      <td>0.034151</td>\n      <td>0.187983</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.175831</td>\n      <td>0.241661</td>\n      <td>1.265685</td>\n      <td>0.065829</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>119.000381</td>\n      <td>177.970490</td>\n      <td>1.700059</td>\n      <td>-0.272527</td>\n      <td>0.273446</td>\n      <td>0.499447</td>\n      <td>0.381089</td>\n      <td>0.035153</td>\n      <td>0.197282</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>0.090009</td>\n      <td>0.156950</td>\n      <td>0.668652</td>\n      <td>0.066940</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100.000237</td>\n      <td>64.013428</td>\n      <td>0.520082</td>\n      <td>-0.871746</td>\n      <td>0.435766</td>\n      <td>0.169760</td>\n      <td>0.252438</td>\n      <td>0.041319</td>\n      <td>0.708434</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>34.642925</td>\n      <td>34.719139</td>\n      <td>1.562176</td>\n      <td>0.076214</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>100.000237</td>\n      <td>92.999939</td>\n      <td>1.198259</td>\n      <td>-0.110510</td>\n      <td>0.057824</td>\n      <td>0.484365</td>\n      <td>0.309789</td>\n      <td>0.062862</td>\n      <td>0.422144</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>6.786076</td>\n      <td>6.880905</td>\n      <td>1.075272</td>\n      <td>0.094829</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>100.000237</td>\n      <td>62.024445</td>\n      <td>0.899884</td>\n      <td>-0.772531</td>\n      <td>0.293228</td>\n      <td>0.100975</td>\n      <td>0.173560</td>\n      <td>0.062805</td>\n      <td>0.999514</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>35.173611</td>\n      <td>35.280350</td>\n      <td>1.612271</td>\n      <td>0.106739</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>101.000053</td>\n      <td>124.999176</td>\n      <td>0.079422</td>\n      <td>-0.249436</td>\n      <td>0.191908</td>\n      <td>0.210460</td>\n      <td>0.278809</td>\n      <td>0.081613</td>\n      <td>0.361983</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>22.789925</td>\n      <td>22.674259</td>\n      <td>0.808006</td>\n      <td>0.115665</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>99.000420</td>\n      <td>81.992279</td>\n      <td>1.697242</td>\n      <td>-0.454589</td>\n      <td>0.002021</td>\n      <td>0.099283</td>\n      <td>0.378426</td>\n      <td>0.062373</td>\n      <td>0.476528</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>15.451066</td>\n      <td>15.578036</td>\n      <td>1.207436</td>\n      <td>0.126970</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>100.000237</td>\n      <td>83.006271</td>\n      <td>1.073231</td>\n      <td>-0.038701</td>\n      <td>0.463104</td>\n      <td>0.392042</td>\n      <td>0.085235</td>\n      <td>0.053101</td>\n      <td>0.103731</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.169466</td>\n      <td>0.040472</td>\n      <td>1.204731</td>\n      <td>0.128994</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>89.995193</td>\n      <td>54.029510</td>\n      <td>0.506840</td>\n      <td>-0.728633</td>\n      <td>0.369394</td>\n      <td>0.112181</td>\n      <td>0.247809</td>\n      <td>0.053009</td>\n      <td>0.530735</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>34.415306</td>\n      <td>34.560307</td>\n      <td>1.665667</td>\n      <td>0.145000</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>100.000237</td>\n      <td>112.000099</td>\n      <td>1.618351</td>\n      <td>-0.427248</td>\n      <td>0.311758</td>\n      <td>0.265387</td>\n      <td>0.082825</td>\n      <td>0.031588</td>\n      <td>0.560076</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>11.234859</td>\n      <td>11.079407</td>\n      <td>0.892858</td>\n      <td>0.155453</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>100.000237</td>\n      <td>80.997787</td>\n      <td>0.331308</td>\n      <td>-0.006729</td>\n      <td>0.322010</td>\n      <td>0.444758</td>\n      <td>0.355917</td>\n      <td>0.016986</td>\n      <td>0.419467</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>20.140936</td>\n      <td>19.984676</td>\n      <td>1.234605</td>\n      <td>0.156260</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>100.000237</td>\n      <td>123.994934</td>\n      <td>0.853535</td>\n      <td>-0.363494</td>\n      <td>0.375735</td>\n      <td>0.181882</td>\n      <td>0.495855</td>\n      <td>0.065431</td>\n      <td>0.112466</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>21.037516</td>\n      <td>20.829142</td>\n      <td>0.806486</td>\n      <td>0.208374</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>98.001068</td>\n      <td>53.015518</td>\n      <td>1.452399</td>\n      <td>-0.810086</td>\n      <td>0.039505</td>\n      <td>0.261969</td>\n      <td>0.259040</td>\n      <td>0.057435</td>\n      <td>0.341413</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>42.543961</td>\n      <td>42.787884</td>\n      <td>1.848536</td>\n      <td>0.243923</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>100.000237</td>\n      <td>148.018738</td>\n      <td>0.100272</td>\n      <td>-0.180037</td>\n      <td>0.220708</td>\n      <td>0.111194</td>\n      <td>0.437267</td>\n      <td>0.079532</td>\n      <td>0.365224</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>45.086964</td>\n      <td>44.842133</td>\n      <td>0.675592</td>\n      <td>0.244831</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>91.997589</td>\n      <td>165.022598</td>\n      <td>1.689353</td>\n      <td>-0.744619</td>\n      <td>0.207761</td>\n      <td>0.196682</td>\n      <td>0.179774</td>\n      <td>0.091636</td>\n      <td>0.966828</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>67.006470</td>\n      <td>66.739990</td>\n      <td>0.557485</td>\n      <td>0.266479</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>100.000237</td>\n      <td>101.999115</td>\n      <td>1.182622</td>\n      <td>-0.249436</td>\n      <td>0.069802</td>\n      <td>0.078282</td>\n      <td>0.147880</td>\n      <td>0.025334</td>\n      <td>0.582037</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>2.812880</td>\n      <td>3.164198</td>\n      <td>0.980403</td>\n      <td>0.351318</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100.000237</td>\n      <td>90.006714</td>\n      <td>1.781204</td>\n      <td>-0.265676</td>\n      <td>0.151816</td>\n      <td>0.362019</td>\n      <td>0.332520</td>\n      <td>0.010186</td>\n      <td>0.874121</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>5.030976</td>\n      <td>5.499053</td>\n      <td>1.111031</td>\n      <td>0.468077</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>119.000381</td>\n      <td>118.998100</td>\n      <td>0.743793</td>\n      <td>-0.631957</td>\n      <td>0.020763</td>\n      <td>0.349615</td>\n      <td>0.302988</td>\n      <td>0.066769</td>\n      <td>0.478711</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>4.317695</td>\n      <td>4.794890</td>\n      <td>1.000019</td>\n      <td>0.477195</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_options.sort_values('Abs Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDJ-hkrSAqVh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MSE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1650895213041,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "GG64Xp3vAib-",
    "outputId": "f846ee65-029d-4a76-b006-d65e4ac07a48",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE on the test set is:  6.303134428616362e-05\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "    for batch, batch_labels in DataLoader(OptDataset(X_test, y_test), batch_size=batch_size):\n",
    "        out = model(batch.to(device))\n",
    "        loss = loss_fn(out, batch_labels.to(device))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    print('The MSE on the test set is: ', losses.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCLf2KQJAvoE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MAE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1650895213494,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "mZohsEyLAu7S",
    "outputId": "4d6bfba3-5751-478d-d53e-73751702414e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE on the test set is:  0.005738810971579565\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "mae_loss = nn.L1Loss()\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "    for batch, batch_labels in DataLoader(OptDataset(X_test, y_test), batch_size=batch_size):\n",
    "        out = model(batch.to(device))\n",
    "        loss = mae_loss(out, batch_labels.to(device))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "losses = np.array(losses)\n",
    "print('The MSE on the test set is: ', losses.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmsZ4aPaA1SJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### RSME on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "    for batch, batch_labels in DataLoader(OptDataset(X_test, y_test), batch_size=batch_size):\n",
    "        out = model(batch.to(device))\n",
    "        loss = loss_fn(out, batch_labels.to(device))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "losses = np.array(losses)\n",
    "print('The RMSE on the test set is: ', np.sqrt(losses.mean()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8Jxp0L4A5zb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MAPE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAPE on the test set is:  0.039601719367407986\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "    for batch, batch_labels in DataLoader(OptDataset(X_test, y_test), batch_size=batch_size):\n",
    "        out = model(batch.to(device))\n",
    "        loss = MAPELoss(out, batch_labels.to(device)).detach().cpu().item()\n",
    "        losses.append(loss)\n",
    "\n",
    "losses = np.array(losses)\n",
    "print('The MAPE on the test set is: ', losses.mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLLLqkX6BEle",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650895213820,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "sf8FmLxxBGcu",
    "outputId": "764313a9-78fc-46ca-dd08-9fb8834e4374",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the R^2 score is:  0.9999306096603625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(X_test[0:batch_size].to(device)).squeeze().cpu().detach().numpy()\n",
    "\n",
    "y_true = y_test[0:batch_size].cpu().squeeze().detach().numpy()\n",
    "\n",
    "r2 = r2_score(y_pred=out, y_true=y_true)\n",
    "\n",
    "print('the R^2 score is: ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1650895214553,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "awvWu7WxBIHB",
    "outputId": "1b3dccd3-90ef-47d5-9895-267fdf93020c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 648x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.scatter(\n",
    "    y=output_sc.inverse_transform(out.reshape(-1, 1)),\n",
    "    x=output_sc.inverse_transform(y_true.squeeze().reshape(-1, 1))\n",
    ")\n",
    "ax.set_xlabel('Actual Value')\n",
    "ax.set_ylabel('Predicted Value')\n",
    "\n",
    "ax.text(20, 80, f'$R^2$ = {np.round(r2, 6)}', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model-Testing-Heston.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}