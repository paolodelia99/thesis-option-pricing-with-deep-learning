{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ju-WUaXnY5cr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Models Testing on Heston data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1650891681415,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "hH3-v8y-AuXg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650891681417,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "QXEzFQ3iAyj_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891681418,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "gsH02HCWA0gC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_options_path = '../data/heston_synthetic_options.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1650891681421,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "jT4n95T6A4S6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3413,
     "status": "ok",
     "timestamp": 1650891684820,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "7H8lEQPCA5IS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paolo/PycharmProjects/th-bot/venv/lib/python3.8/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "synthetic_options = pd.read_csv(synthetic_options_path, index_col=0)\n",
    "synthetic_options = reduce_mem_usage(synthetic_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1650891684821,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "9TKRgvuOA8pC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_options = shuffle(synthetic_options, random_state=0)\n",
    "synthetic_options = synthetic_options.reset_index()\n",
    "synthetic_options = synthetic_options.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1650891684822,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "3HXliAsq132p",
    "outputId": "09bc2eb1-d20d-4471-ed50-5fa0eba727fb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         Price  Strike Type     Kappa       Rho     Theta        Xi       V_0  \\\n0           96    78.0    C  1.074219 -0.311279  0.375732  0.180298  0.367432   \n1          100    56.0    C  1.495117 -0.869629  0.011398  0.010925  0.257080   \n2          100    68.0    P  0.988770 -0.664551  0.344482  0.045258  0.328125   \n3          100    75.0    C  1.996094 -0.051880  0.417480  0.208374  0.328613   \n4          100    71.0    P  0.344971 -0.344727  0.227783  0.340820  0.054413   \n...        ...     ...  ...       ...       ...       ...       ...       ...   \n1059740    100    71.0    P  1.039062 -0.757324  0.227661  0.312500  0.445068   \n1059741    100    57.0    C  1.905273 -0.276855  0.322266  0.061432  0.120117   \n1059742     92   132.0    P  0.149536 -0.433350  0.262695  0.054565  0.404541   \n1059743    100   135.0    C  0.971680 -0.711426  0.270508  0.438477  0.483887   \n1059744    100   135.0    P  1.183594 -0.541504  0.479980  0.091614  0.104736   \n\n         Interest Rate  Time to Expiration  Option Price  \n0             0.045074            0.533203     17.828125  \n1             0.050201            0.977051     42.187500  \n2             0.051971            0.588867      0.026703  \n3             0.053741            0.215088     25.312500  \n4             0.016891            0.800781      0.070496  \n...                ...                 ...           ...  \n1059740       0.016006            0.487061      1.505859  \n1059741       0.025070            0.947754     41.687500  \n1059742       0.028809            0.551758     38.375000  \n1059743       0.097107            0.289307      0.180542  \n1059744       0.095215            0.864746     32.093750  \n\n[1059745 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>Strike</th>\n      <th>Type</th>\n      <th>Kappa</th>\n      <th>Rho</th>\n      <th>Theta</th>\n      <th>Xi</th>\n      <th>V_0</th>\n      <th>Interest Rate</th>\n      <th>Time to Expiration</th>\n      <th>Option Price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>96</td>\n      <td>78.0</td>\n      <td>C</td>\n      <td>1.074219</td>\n      <td>-0.311279</td>\n      <td>0.375732</td>\n      <td>0.180298</td>\n      <td>0.367432</td>\n      <td>0.045074</td>\n      <td>0.533203</td>\n      <td>17.828125</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100</td>\n      <td>56.0</td>\n      <td>C</td>\n      <td>1.495117</td>\n      <td>-0.869629</td>\n      <td>0.011398</td>\n      <td>0.010925</td>\n      <td>0.257080</td>\n      <td>0.050201</td>\n      <td>0.977051</td>\n      <td>42.187500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100</td>\n      <td>68.0</td>\n      <td>P</td>\n      <td>0.988770</td>\n      <td>-0.664551</td>\n      <td>0.344482</td>\n      <td>0.045258</td>\n      <td>0.328125</td>\n      <td>0.051971</td>\n      <td>0.588867</td>\n      <td>0.026703</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100</td>\n      <td>75.0</td>\n      <td>C</td>\n      <td>1.996094</td>\n      <td>-0.051880</td>\n      <td>0.417480</td>\n      <td>0.208374</td>\n      <td>0.328613</td>\n      <td>0.053741</td>\n      <td>0.215088</td>\n      <td>25.312500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100</td>\n      <td>71.0</td>\n      <td>P</td>\n      <td>0.344971</td>\n      <td>-0.344727</td>\n      <td>0.227783</td>\n      <td>0.340820</td>\n      <td>0.054413</td>\n      <td>0.016891</td>\n      <td>0.800781</td>\n      <td>0.070496</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1059740</th>\n      <td>100</td>\n      <td>71.0</td>\n      <td>P</td>\n      <td>1.039062</td>\n      <td>-0.757324</td>\n      <td>0.227661</td>\n      <td>0.312500</td>\n      <td>0.445068</td>\n      <td>0.016006</td>\n      <td>0.487061</td>\n      <td>1.505859</td>\n    </tr>\n    <tr>\n      <th>1059741</th>\n      <td>100</td>\n      <td>57.0</td>\n      <td>C</td>\n      <td>1.905273</td>\n      <td>-0.276855</td>\n      <td>0.322266</td>\n      <td>0.061432</td>\n      <td>0.120117</td>\n      <td>0.025070</td>\n      <td>0.947754</td>\n      <td>41.687500</td>\n    </tr>\n    <tr>\n      <th>1059742</th>\n      <td>92</td>\n      <td>132.0</td>\n      <td>P</td>\n      <td>0.149536</td>\n      <td>-0.433350</td>\n      <td>0.262695</td>\n      <td>0.054565</td>\n      <td>0.404541</td>\n      <td>0.028809</td>\n      <td>0.551758</td>\n      <td>38.375000</td>\n    </tr>\n    <tr>\n      <th>1059743</th>\n      <td>100</td>\n      <td>135.0</td>\n      <td>C</td>\n      <td>0.971680</td>\n      <td>-0.711426</td>\n      <td>0.270508</td>\n      <td>0.438477</td>\n      <td>0.483887</td>\n      <td>0.097107</td>\n      <td>0.289307</td>\n      <td>0.180542</td>\n    </tr>\n    <tr>\n      <th>1059744</th>\n      <td>100</td>\n      <td>135.0</td>\n      <td>P</td>\n      <td>1.183594</td>\n      <td>-0.541504</td>\n      <td>0.479980</td>\n      <td>0.091614</td>\n      <td>0.104736</td>\n      <td>0.095215</td>\n      <td>0.864746</td>\n      <td>32.093750</td>\n    </tr>\n  </tbody>\n</table>\n<p>1059745 rows Ã— 11 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pn28_RUMBAFH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1650891685322,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "32oPe6XUBCTF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_options = pd.get_dummies(synthetic_options, prefix='', prefix_sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1650891686346,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "_Tlc7k7xBDPV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_sc = StandardScaler()\n",
    "output_sc = StandardScaler()\n",
    "input_data = input_sc.fit_transform(synthetic_options.drop('Option Price', axis=1))\n",
    "output_data = output_sc.fit_transform(synthetic_options['Option Price'].values.reshape(-1, 1))\n",
    "\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "\n",
    "last_train_idx = int(np.round(len(input_data) * train_size))\n",
    "last_val_idx = last_train_idx + int(np.round(len(input_data) * val_size))\n",
    "\n",
    "X_train = input_data[0:last_train_idx]\n",
    "X_val = input_data[last_train_idx:last_val_idx]\n",
    "X_test = input_data[last_val_idx:]\n",
    "\n",
    "y_train = output_data[0:last_train_idx]\n",
    "y_val = output_data[last_train_idx:last_val_idx]\n",
    "y_test = output_data[last_val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650891686347,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "7nEEoQGvvDpL",
    "outputId": "88b4d863-d037-439b-e625-5ebb52ad41ef",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Price', 'Strike', 'Kappa', 'Rho', 'Theta', 'Xi', 'V_0',\n       'Interest Rate', 'Time to Expiration', 'C', 'P'],\n      dtype='object')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_options.drop('Option Price', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1650891686349,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "IwMVtvfABIhR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = Variable(torch.Tensor(X_train))\n",
    "X_val = Variable(torch.Tensor(X_val))\n",
    "X_test = Variable(torch.Tensor(X_test))\n",
    "\n",
    "y_train = Variable(torch.Tensor(y_train))\n",
    "y_val = Variable(torch.Tensor(y_val))\n",
    "y_test = Variable(torch.Tensor(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ra2l5P1nBVCz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891686350,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "xTLMLFoSBWDy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "device = 'cuda:0' if CUDA else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1650891686352,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "CQ9Gl3bUBWsj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, module):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.module = module\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.module(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891686353,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "YL3SicQPBYq0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, layer_size, act_fn):\n",
    "      super(HiddenLayer, self).__init__()\n",
    "      \n",
    "      if act_fn == 'ReLU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.ReLU())\n",
    "      elif act_fn == 'LeakyReLU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.LeakyReLU())\n",
    "      elif act_fn == 'ELU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.ELU())\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1650891686792,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "lHUFGf9xBawS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, output_size, hidden_size, num_layers, act_fn):\n",
    "    super(Net, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.output_size = output_size\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if act_fn == 'ReLU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ReLU())\n",
    "    elif act_fn == 'LeakyReLU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.LeakyReLU())\n",
    "    elif act_fn == 'ELU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ELU())\n",
    "\n",
    "    self.hidden_layers_list = []\n",
    "\n",
    "    for i in range(num_layers // 2):\n",
    "      self.hidden_layers_list.append(\n",
    "          ResBlock(\n",
    "            nn.Sequential(\n",
    "                HiddenLayer(self.hidden_size, act_fn),\n",
    "                HiddenLayer(self.hidden_size, act_fn)\n",
    "            )\n",
    "        )\n",
    "      )\n",
    "\n",
    "    self.hidden_layers = nn.Sequential(*self.hidden_layers_list)\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        self.initial_layer,\n",
    "        self.hidden_layers,\n",
    "        nn.Linear(self.hidden_size, self.output_size)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650891686793,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "zcq_lQrHBdH8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(m, init_m: str):\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_uniform(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.uniform_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_normal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.normal_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_xuniform(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.xavier_uniform_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_xnormal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.xavier_normal_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  if init_m == 'uniform':\n",
    "    m.apply(init_uniform)\n",
    "  elif init_m == 'normal':\n",
    "    m.apply(init_normal)\n",
    "  elif init_m == 'xaiver uniform':\n",
    "    m.apply(init_xuniform)\n",
    "  elif init_m == 'xavier normal':\n",
    "    m.apply(init_xnormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuCpyycNCKEZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650891686794,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "YbCOnCSNCL25",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 11\n",
    "output_size = 1\n",
    "num_layers = 4\n",
    "hidden_size = 600\n",
    "batch_size = 1141\n",
    "epochs = 2000\n",
    "lr = 0.00012243587926335812\n",
    "init_method = 'xaiver uniform'\n",
    "act_fn = 'LeakyReLU'\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers, act_fn)\n",
    "init_weights(model, init_method)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 7351,
     "status": "ok",
     "timestamp": 1650891694138,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "QqZbxrppvDpZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1650891694142,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "Q-9T0GArCMgp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class OptDataset(Dataset):\n",
    "\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X[idx], self.y[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6-hCH2ivDpb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Losses Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1650891694144,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "mVaO8TruHW4M",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def MAPELoss(output, target):\n",
    "  return torch.mean(torch.abs((target - output) / target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1650891694149,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "tVLW5dHhvDpe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, X_val, y_val):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X_val, y_val), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = loss_fn(out, batch_labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    print('\\nVal set: Average loss: {:.8f}\\n'.format(\n",
    "                losses.mean()))\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68eF5_EovDpf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Early Stopping class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650891694150,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "n2So2ffSvDpg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code took form: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, \n",
    "                 patience=10, \n",
    "                 verbose=False, \n",
    "                 delta=0, \n",
    "                 path='../models/final_heston_model.chkpt',\n",
    "                 trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL_xmnKXvDph",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1650891694152,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "DrBBTGKJvDph",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val\n",
    "):\n",
    "\n",
    "  training_losses = []\n",
    "  validation_losses = []\n",
    "\n",
    "  early_stopping = EarlyStopping(patience=20)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "\n",
    "    for batch, batch_labels in DataLoader(OptDataset(X_train, y_train), batch_size=batch_size):\n",
    "      out = model(batch.to(device))\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss = loss_fn(out, batch_labels.to(device))\n",
    "      epoch_losses.append(loss.item())\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if i > 0 and i % 50 == 0:\n",
    "        avg_loss = total_loss / 50\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| Epoch {:3d} | {:5d}/{:5d} batches | lr {:2.5f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.8f}'.format(\n",
    "              epoch, i, len(X_train) // batch_size+1, lr, elapsed * 1000 / 50,\n",
    "              avg_loss))\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "      i += 1\n",
    "\n",
    "    training_losses.append(np.array(epoch_losses).mean())\n",
    "    val_loss = evaluate(model, loss_fn, X_val, y_val)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Stopping at Epoch: {epoch}\")\n",
    "        break\n",
    "\n",
    "  return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2525796,
     "status": "ok",
     "timestamp": 1650894219916,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "hBETWoCfvDpj",
    "outputId": "30acf99f-3a1e-4d87-d1ff-13068ebe4cd4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 |    50/  744 batches | lr 0.00012 | ms/batch 25.62 | loss 0.23889832\n",
      "| Epoch   0 |   100/  744 batches | lr 0.00012 | ms/batch 21.52 | loss 0.01806687\n",
      "| Epoch   0 |   150/  744 batches | lr 0.00012 | ms/batch 21.50 | loss 0.00925729\n",
      "| Epoch   0 |   200/  744 batches | lr 0.00012 | ms/batch 18.97 | loss 0.00547197\n",
      "| Epoch   0 |   250/  744 batches | lr 0.00012 | ms/batch 17.93 | loss 0.00373161\n",
      "| Epoch   0 |   300/  744 batches | lr 0.00012 | ms/batch 18.68 | loss 0.00260794\n",
      "| Epoch   0 |   350/  744 batches | lr 0.00012 | ms/batch 19.04 | loss 0.00213065\n",
      "| Epoch   0 |   400/  744 batches | lr 0.00012 | ms/batch 19.01 | loss 0.00172211\n",
      "| Epoch   0 |   450/  744 batches | lr 0.00012 | ms/batch 17.23 | loss 0.00150703\n",
      "| Epoch   0 |   500/  744 batches | lr 0.00012 | ms/batch 18.99 | loss 0.00137163\n",
      "| Epoch   0 |   550/  744 batches | lr 0.00012 | ms/batch 18.51 | loss 0.00127231\n",
      "| Epoch   0 |   600/  744 batches | lr 0.00012 | ms/batch 18.89 | loss 0.00116748\n",
      "| Epoch   0 |   650/  744 batches | lr 0.00012 | ms/batch 18.97 | loss 0.00110798\n",
      "| Epoch   0 |   700/  744 batches | lr 0.00012 | ms/batch 18.81 | loss 0.00109563\n",
      "\n",
      "Val set: Average loss: 0.00141559\n",
      "\n",
      "| Epoch   1 |    50/  744 batches | lr 0.00012 | ms/batch 18.65 | loss 0.00099982\n",
      "| Epoch   1 |   100/  744 batches | lr 0.00012 | ms/batch 17.70 | loss 0.00087827\n",
      "| Epoch   1 |   150/  744 batches | lr 0.00012 | ms/batch 20.84 | loss 0.00082560\n",
      "| Epoch   1 |   200/  744 batches | lr 0.00012 | ms/batch 18.49 | loss 0.00083464\n",
      "| Epoch   1 |   250/  744 batches | lr 0.00012 | ms/batch 19.91 | loss 0.00088095\n",
      "| Epoch   1 |   300/  744 batches | lr 0.00012 | ms/batch 17.19 | loss 0.00072902\n",
      "| Epoch   1 |   350/  744 batches | lr 0.00012 | ms/batch 17.10 | loss 0.00100551\n",
      "| Epoch   1 |   400/  744 batches | lr 0.00012 | ms/batch 18.84 | loss 0.00074386\n",
      "| Epoch   1 |   450/  744 batches | lr 0.00012 | ms/batch 16.84 | loss 0.00073450\n",
      "| Epoch   1 |   500/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00081534\n",
      "| Epoch   1 |   550/  744 batches | lr 0.00012 | ms/batch 17.55 | loss 0.00088495\n",
      "| Epoch   1 |   600/  744 batches | lr 0.00012 | ms/batch 19.26 | loss 0.00082922\n",
      "| Epoch   1 |   650/  744 batches | lr 0.00012 | ms/batch 18.34 | loss 0.00067721\n",
      "| Epoch   1 |   700/  744 batches | lr 0.00012 | ms/batch 17.67 | loss 0.00065554\n",
      "\n",
      "Val set: Average loss: 0.00070967\n",
      "\n",
      "| Epoch   2 |    50/  744 batches | lr 0.00012 | ms/batch 19.45 | loss 0.00064455\n",
      "| Epoch   2 |   100/  744 batches | lr 0.00012 | ms/batch 19.10 | loss 0.00062518\n",
      "| Epoch   2 |   150/  744 batches | lr 0.00012 | ms/batch 21.32 | loss 0.00054747\n",
      "| Epoch   2 |   200/  744 batches | lr 0.00012 | ms/batch 19.23 | loss 0.00061765\n",
      "| Epoch   2 |   250/  744 batches | lr 0.00012 | ms/batch 19.57 | loss 0.00065632\n",
      "| Epoch   2 |   300/  744 batches | lr 0.00012 | ms/batch 23.69 | loss 0.00051674\n",
      "| Epoch   2 |   350/  744 batches | lr 0.00012 | ms/batch 20.65 | loss 0.00067155\n",
      "| Epoch   2 |   400/  744 batches | lr 0.00012 | ms/batch 19.43 | loss 0.00071121\n",
      "| Epoch   2 |   450/  744 batches | lr 0.00012 | ms/batch 22.08 | loss 0.00052215\n",
      "| Epoch   2 |   500/  744 batches | lr 0.00012 | ms/batch 20.44 | loss 0.00071522\n",
      "| Epoch   2 |   550/  744 batches | lr 0.00012 | ms/batch 20.19 | loss 0.00073359\n",
      "| Epoch   2 |   600/  744 batches | lr 0.00012 | ms/batch 21.12 | loss 0.00061241\n",
      "| Epoch   2 |   650/  744 batches | lr 0.00012 | ms/batch 22.48 | loss 0.00055282\n",
      "| Epoch   2 |   700/  744 batches | lr 0.00012 | ms/batch 21.13 | loss 0.00052110\n",
      "\n",
      "Val set: Average loss: 0.00053311\n",
      "\n",
      "| Epoch   3 |    50/  744 batches | lr 0.00012 | ms/batch 19.34 | loss 0.00048488\n",
      "| Epoch   3 |   100/  744 batches | lr 0.00012 | ms/batch 21.16 | loss 0.00048530\n",
      "| Epoch   3 |   150/  744 batches | lr 0.00012 | ms/batch 19.02 | loss 0.00046964\n",
      "| Epoch   3 |   200/  744 batches | lr 0.00012 | ms/batch 18.84 | loss 0.00056059\n",
      "| Epoch   3 |   250/  744 batches | lr 0.00012 | ms/batch 20.31 | loss 0.00058066\n",
      "| Epoch   3 |   300/  744 batches | lr 0.00012 | ms/batch 18.46 | loss 0.00041310\n",
      "| Epoch   3 |   350/  744 batches | lr 0.00012 | ms/batch 18.99 | loss 0.00064185\n",
      "| Epoch   3 |   400/  744 batches | lr 0.00012 | ms/batch 20.41 | loss 0.00057947\n",
      "| Epoch   3 |   450/  744 batches | lr 0.00012 | ms/batch 18.94 | loss 0.00044351\n",
      "| Epoch   3 |   500/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00057771\n",
      "| Epoch   3 |   550/  744 batches | lr 0.00012 | ms/batch 20.45 | loss 0.00057874\n",
      "| Epoch   3 |   600/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00049201\n",
      "| Epoch   3 |   650/  744 batches | lr 0.00012 | ms/batch 19.69 | loss 0.00043295\n",
      "| Epoch   3 |   700/  744 batches | lr 0.00012 | ms/batch 20.91 | loss 0.00043318\n",
      "\n",
      "Val set: Average loss: 0.00045901\n",
      "\n",
      "| Epoch   4 |    50/  744 batches | lr 0.00012 | ms/batch 18.79 | loss 0.00042044\n",
      "| Epoch   4 |   100/  744 batches | lr 0.00012 | ms/batch 18.64 | loss 0.00044035\n",
      "| Epoch   4 |   150/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00043667\n",
      "| Epoch   4 |   200/  744 batches | lr 0.00012 | ms/batch 17.00 | loss 0.00048315\n",
      "| Epoch   4 |   250/  744 batches | lr 0.00012 | ms/batch 18.58 | loss 0.00047623\n",
      "| Epoch   4 |   300/  744 batches | lr 0.00012 | ms/batch 18.60 | loss 0.00035621\n",
      "| Epoch   4 |   350/  744 batches | lr 0.00012 | ms/batch 18.46 | loss 0.00043601\n",
      "| Epoch   4 |   400/  744 batches | lr 0.00012 | ms/batch 19.22 | loss 0.00035715\n",
      "| Epoch   4 |   450/  744 batches | lr 0.00012 | ms/batch 16.94 | loss 0.00043743\n",
      "| Epoch   4 |   500/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00052908\n",
      "| Epoch   4 |   550/  744 batches | lr 0.00012 | ms/batch 18.56 | loss 0.00042486\n",
      "| Epoch   4 |   600/  744 batches | lr 0.00012 | ms/batch 18.76 | loss 0.00037862\n",
      "| Epoch   4 |   650/  744 batches | lr 0.00012 | ms/batch 18.71 | loss 0.00046035\n",
      "| Epoch   4 |   700/  744 batches | lr 0.00012 | ms/batch 16.93 | loss 0.00052329\n",
      "\n",
      "Val set: Average loss: 0.00041595\n",
      "\n",
      "| Epoch   5 |    50/  744 batches | lr 0.00012 | ms/batch 18.78 | loss 0.00038609\n",
      "| Epoch   5 |   100/  744 batches | lr 0.00012 | ms/batch 16.82 | loss 0.00045116\n",
      "| Epoch   5 |   150/  744 batches | lr 0.00012 | ms/batch 18.54 | loss 0.00043650\n",
      "| Epoch   5 |   200/  744 batches | lr 0.00012 | ms/batch 16.92 | loss 0.00041368\n",
      "| Epoch   5 |   250/  744 batches | lr 0.00012 | ms/batch 18.40 | loss 0.00047811\n",
      "| Epoch   5 |   300/  744 batches | lr 0.00012 | ms/batch 17.04 | loss 0.00038808\n",
      "| Epoch   5 |   350/  744 batches | lr 0.00012 | ms/batch 16.97 | loss 0.00047903\n",
      "| Epoch   5 |   400/  744 batches | lr 0.00012 | ms/batch 18.56 | loss 0.00038837\n",
      "| Epoch   5 |   450/  744 batches | lr 0.00012 | ms/batch 16.99 | loss 0.00032859\n",
      "| Epoch   5 |   500/  744 batches | lr 0.00012 | ms/batch 18.48 | loss 0.00040720\n",
      "| Epoch   5 |   550/  744 batches | lr 0.00012 | ms/batch 16.87 | loss 0.00032326\n",
      "| Epoch   5 |   600/  744 batches | lr 0.00012 | ms/batch 16.79 | loss 0.00032387\n",
      "| Epoch   5 |   650/  744 batches | lr 0.00012 | ms/batch 18.65 | loss 0.00040814\n",
      "| Epoch   5 |   700/  744 batches | lr 0.00012 | ms/batch 17.21 | loss 0.00045985\n",
      "\n",
      "Val set: Average loss: 0.00030649\n",
      "\n",
      "| Epoch   6 |    50/  744 batches | lr 0.00012 | ms/batch 20.93 | loss 0.00038183\n",
      "| Epoch   6 |   100/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00039933\n",
      "| Epoch   6 |   150/  744 batches | lr 0.00012 | ms/batch 19.29 | loss 0.00037971\n",
      "| Epoch   6 |   200/  744 batches | lr 0.00012 | ms/batch 21.27 | loss 0.00036419\n",
      "| Epoch   6 |   250/  744 batches | lr 0.00012 | ms/batch 18.97 | loss 0.00042804\n",
      "| Epoch   6 |   300/  744 batches | lr 0.00012 | ms/batch 18.72 | loss 0.00040445\n",
      "| Epoch   6 |   350/  744 batches | lr 0.00012 | ms/batch 20.33 | loss 0.00035182\n",
      "| Epoch   6 |   400/  744 batches | lr 0.00012 | ms/batch 19.01 | loss 0.00030349\n",
      "| Epoch   6 |   450/  744 batches | lr 0.00012 | ms/batch 18.94 | loss 0.00027506\n",
      "| Epoch   6 |   500/  744 batches | lr 0.00012 | ms/batch 20.38 | loss 0.00032704\n",
      "| Epoch   6 |   550/  744 batches | lr 0.00012 | ms/batch 18.70 | loss 0.00029034\n",
      "| Epoch   6 |   600/  744 batches | lr 0.00012 | ms/batch 18.98 | loss 0.00039665\n",
      "| Epoch   6 |   650/  744 batches | lr 0.00012 | ms/batch 20.45 | loss 0.00042318\n",
      "| Epoch   6 |   700/  744 batches | lr 0.00012 | ms/batch 18.87 | loss 0.00037323\n",
      "\n",
      "Val set: Average loss: 0.00028609\n",
      "\n",
      "| Epoch   7 |    50/  744 batches | lr 0.00012 | ms/batch 19.26 | loss 0.00032651\n",
      "| Epoch   7 |   100/  744 batches | lr 0.00012 | ms/batch 17.27 | loss 0.00035136\n",
      "| Epoch   7 |   150/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00033884\n",
      "| Epoch   7 |   200/  744 batches | lr 0.00012 | ms/batch 18.63 | loss 0.00030612\n",
      "| Epoch   7 |   250/  744 batches | lr 0.00012 | ms/batch 18.72 | loss 0.00029350\n",
      "| Epoch   7 |   300/  744 batches | lr 0.00012 | ms/batch 18.71 | loss 0.00034392\n",
      "| Epoch   7 |   350/  744 batches | lr 0.00012 | ms/batch 17.26 | loss 0.00030942\n",
      "| Epoch   7 |   400/  744 batches | lr 0.00012 | ms/batch 18.70 | loss 0.00029798\n",
      "| Epoch   7 |   450/  744 batches | lr 0.00012 | ms/batch 18.65 | loss 0.00027462\n",
      "| Epoch   7 |   500/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00036147\n",
      "| Epoch   7 |   550/  744 batches | lr 0.00012 | ms/batch 18.80 | loss 0.00026118\n",
      "| Epoch   7 |   600/  744 batches | lr 0.00012 | ms/batch 17.10 | loss 0.00038643\n",
      "| Epoch   7 |   650/  744 batches | lr 0.00012 | ms/batch 18.80 | loss 0.00038863\n",
      "| Epoch   7 |   700/  744 batches | lr 0.00012 | ms/batch 18.92 | loss 0.00034199\n",
      "\n",
      "Val set: Average loss: 0.00024633\n",
      "\n",
      "| Epoch   8 |    50/  744 batches | lr 0.00012 | ms/batch 17.33 | loss 0.00029381\n",
      "| Epoch   8 |   100/  744 batches | lr 0.00012 | ms/batch 18.57 | loss 0.00029047\n",
      "| Epoch   8 |   150/  744 batches | lr 0.00012 | ms/batch 17.04 | loss 0.00032864\n",
      "| Epoch   8 |   200/  744 batches | lr 0.00012 | ms/batch 18.39 | loss 0.00027368\n",
      "| Epoch   8 |   250/  744 batches | lr 0.00012 | ms/batch 17.08 | loss 0.00029306\n",
      "| Epoch   8 |   300/  744 batches | lr 0.00012 | ms/batch 18.55 | loss 0.00037359\n",
      "| Epoch   8 |   350/  744 batches | lr 0.00012 | ms/batch 16.88 | loss 0.00043638\n",
      "| Epoch   8 |   400/  744 batches | lr 0.00012 | ms/batch 16.84 | loss 0.00024564\n",
      "| Epoch   8 |   450/  744 batches | lr 0.00012 | ms/batch 18.62 | loss 0.00022688\n",
      "| Epoch   8 |   500/  744 batches | lr 0.00012 | ms/batch 16.94 | loss 0.00032627\n",
      "| Epoch   8 |   550/  744 batches | lr 0.00012 | ms/batch 18.68 | loss 0.00023904\n",
      "| Epoch   8 |   600/  744 batches | lr 0.00012 | ms/batch 17.02 | loss 0.00036449\n",
      "| Epoch   8 |   650/  744 batches | lr 0.00012 | ms/batch 17.00 | loss 0.00040110\n",
      "| Epoch   8 |   700/  744 batches | lr 0.00012 | ms/batch 18.42 | loss 0.00040324\n",
      "\n",
      "Val set: Average loss: 0.00026140\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch   9 |    50/  744 batches | lr 0.00012 | ms/batch 17.31 | loss 0.00025288\n",
      "| Epoch   9 |   100/  744 batches | lr 0.00012 | ms/batch 18.41 | loss 0.00031794\n",
      "| Epoch   9 |   150/  744 batches | lr 0.00012 | ms/batch 17.51 | loss 0.00028078\n",
      "| Epoch   9 |   200/  744 batches | lr 0.00012 | ms/batch 18.65 | loss 0.00025613\n",
      "| Epoch   9 |   250/  744 batches | lr 0.00012 | ms/batch 17.04 | loss 0.00030030\n",
      "| Epoch   9 |   300/  744 batches | lr 0.00012 | ms/batch 18.62 | loss 0.00037767\n",
      "| Epoch   9 |   350/  744 batches | lr 0.00012 | ms/batch 17.07 | loss 0.00032730\n",
      "| Epoch   9 |   400/  744 batches | lr 0.00012 | ms/batch 17.31 | loss 0.00021802\n",
      "| Epoch   9 |   450/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00020462\n",
      "| Epoch   9 |   500/  744 batches | lr 0.00012 | ms/batch 17.01 | loss 0.00022969\n",
      "| Epoch   9 |   550/  744 batches | lr 0.00012 | ms/batch 18.64 | loss 0.00022227\n",
      "| Epoch   9 |   600/  744 batches | lr 0.00012 | ms/batch 16.87 | loss 0.00025886\n",
      "| Epoch   9 |   650/  744 batches | lr 0.00012 | ms/batch 17.17 | loss 0.00026871\n",
      "| Epoch   9 |   700/  744 batches | lr 0.00012 | ms/batch 18.56 | loss 0.00032791\n",
      "\n",
      "Val set: Average loss: 0.00018937\n",
      "\n",
      "| Epoch  10 |    50/  744 batches | lr 0.00012 | ms/batch 19.13 | loss 0.00021457\n",
      "| Epoch  10 |   100/  744 batches | lr 0.00012 | ms/batch 20.39 | loss 0.00024187\n",
      "| Epoch  10 |   150/  744 batches | lr 0.00012 | ms/batch 18.93 | loss 0.00033973\n",
      "| Epoch  10 |   200/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00019549\n",
      "| Epoch  10 |   250/  744 batches | lr 0.00012 | ms/batch 20.80 | loss 0.00024638\n",
      "| Epoch  10 |   300/  744 batches | lr 0.00012 | ms/batch 18.81 | loss 0.00038336\n",
      "| Epoch  10 |   350/  744 batches | lr 0.00012 | ms/batch 19.01 | loss 0.00023919\n",
      "| Epoch  10 |   400/  744 batches | lr 0.00012 | ms/batch 20.67 | loss 0.00018496\n",
      "| Epoch  10 |   450/  744 batches | lr 0.00012 | ms/batch 19.11 | loss 0.00029021\n",
      "| Epoch  10 |   500/  744 batches | lr 0.00012 | ms/batch 19.06 | loss 0.00025144\n",
      "| Epoch  10 |   550/  744 batches | lr 0.00012 | ms/batch 20.61 | loss 0.00020619\n",
      "| Epoch  10 |   600/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00031984\n",
      "| Epoch  10 |   650/  744 batches | lr 0.00012 | ms/batch 18.93 | loss 0.00028916\n",
      "| Epoch  10 |   700/  744 batches | lr 0.00012 | ms/batch 20.55 | loss 0.00027523\n",
      "\n",
      "Val set: Average loss: 0.00040570\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  11 |    50/  744 batches | lr 0.00012 | ms/batch 20.79 | loss 0.00026477\n",
      "| Epoch  11 |   100/  744 batches | lr 0.00012 | ms/batch 18.72 | loss 0.00021630\n",
      "| Epoch  11 |   150/  744 batches | lr 0.00012 | ms/batch 18.91 | loss 0.00024094\n",
      "| Epoch  11 |   200/  744 batches | lr 0.00012 | ms/batch 20.69 | loss 0.00018767\n",
      "| Epoch  11 |   250/  744 batches | lr 0.00012 | ms/batch 19.35 | loss 0.00024661\n",
      "| Epoch  11 |   300/  744 batches | lr 0.00012 | ms/batch 18.92 | loss 0.00029969\n",
      "| Epoch  11 |   350/  744 batches | lr 0.00012 | ms/batch 20.44 | loss 0.00025191\n",
      "| Epoch  11 |   400/  744 batches | lr 0.00012 | ms/batch 18.96 | loss 0.00016967\n",
      "| Epoch  11 |   450/  744 batches | lr 0.00012 | ms/batch 19.00 | loss 0.00023091\n",
      "| Epoch  11 |   500/  744 batches | lr 0.00012 | ms/batch 20.54 | loss 0.00023590\n",
      "| Epoch  11 |   550/  744 batches | lr 0.00012 | ms/batch 18.92 | loss 0.00028523\n",
      "| Epoch  11 |   600/  744 batches | lr 0.00012 | ms/batch 19.46 | loss 0.00043415\n",
      "| Epoch  11 |   650/  744 batches | lr 0.00012 | ms/batch 21.34 | loss 0.00022768\n",
      "| Epoch  11 |   700/  744 batches | lr 0.00012 | ms/batch 19.02 | loss 0.00024752\n",
      "\n",
      "Val set: Average loss: 0.00028796\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  12 |    50/  744 batches | lr 0.00012 | ms/batch 19.32 | loss 0.00023290\n",
      "| Epoch  12 |   100/  744 batches | lr 0.00012 | ms/batch 18.76 | loss 0.00025503\n",
      "| Epoch  12 |   150/  744 batches | lr 0.00012 | ms/batch 20.44 | loss 0.00021473\n",
      "| Epoch  12 |   200/  744 batches | lr 0.00012 | ms/batch 18.86 | loss 0.00017624\n",
      "| Epoch  12 |   250/  744 batches | lr 0.00012 | ms/batch 18.94 | loss 0.00021827\n",
      "| Epoch  12 |   300/  744 batches | lr 0.00012 | ms/batch 20.80 | loss 0.00030653\n",
      "| Epoch  12 |   350/  744 batches | lr 0.00012 | ms/batch 18.75 | loss 0.00033018\n",
      "| Epoch  12 |   400/  744 batches | lr 0.00012 | ms/batch 18.89 | loss 0.00018101\n",
      "| Epoch  12 |   450/  744 batches | lr 0.00012 | ms/batch 20.48 | loss 0.00016872\n",
      "| Epoch  12 |   500/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00018229\n",
      "| Epoch  12 |   550/  744 batches | lr 0.00012 | ms/batch 18.75 | loss 0.00018165\n",
      "| Epoch  12 |   600/  744 batches | lr 0.00012 | ms/batch 20.56 | loss 0.00017596\n",
      "| Epoch  12 |   650/  744 batches | lr 0.00012 | ms/batch 18.80 | loss 0.00024350\n",
      "| Epoch  12 |   700/  744 batches | lr 0.00012 | ms/batch 18.65 | loss 0.00022751\n",
      "\n",
      "Val set: Average loss: 0.00020916\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  13 |    50/  744 batches | lr 0.00012 | ms/batch 19.18 | loss 0.00025642\n",
      "| Epoch  13 |   100/  744 batches | lr 0.00012 | ms/batch 20.50 | loss 0.00020001\n",
      "| Epoch  13 |   150/  744 batches | lr 0.00012 | ms/batch 18.92 | loss 0.00018595\n",
      "| Epoch  13 |   200/  744 batches | lr 0.00012 | ms/batch 18.91 | loss 0.00020200\n",
      "| Epoch  13 |   250/  744 batches | lr 0.00012 | ms/batch 20.94 | loss 0.00022741\n",
      "| Epoch  13 |   300/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00020083\n",
      "| Epoch  13 |   350/  744 batches | lr 0.00012 | ms/batch 18.90 | loss 0.00019266\n",
      "| Epoch  13 |   400/  744 batches | lr 0.00012 | ms/batch 20.41 | loss 0.00035938\n",
      "| Epoch  13 |   450/  744 batches | lr 0.00012 | ms/batch 18.86 | loss 0.00016884\n",
      "| Epoch  13 |   500/  744 batches | lr 0.00012 | ms/batch 20.06 | loss 0.00020823\n",
      "| Epoch  13 |   550/  744 batches | lr 0.00012 | ms/batch 20.48 | loss 0.00015328\n",
      "| Epoch  13 |   600/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00022131\n",
      "| Epoch  13 |   650/  744 batches | lr 0.00012 | ms/batch 18.93 | loss 0.00018817\n",
      "| Epoch  13 |   700/  744 batches | lr 0.00012 | ms/batch 20.52 | loss 0.00034315\n",
      "\n",
      "Val set: Average loss: 0.00019629\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  14 |    50/  744 batches | lr 0.00012 | ms/batch 20.95 | loss 0.00024426\n",
      "| Epoch  14 |   100/  744 batches | lr 0.00012 | ms/batch 19.14 | loss 0.00020469\n",
      "| Epoch  14 |   150/  744 batches | lr 0.00012 | ms/batch 18.86 | loss 0.00017225\n",
      "| Epoch  14 |   200/  744 batches | lr 0.00012 | ms/batch 20.69 | loss 0.00014418\n",
      "| Epoch  14 |   250/  744 batches | lr 0.00012 | ms/batch 19.07 | loss 0.00018171\n",
      "| Epoch  14 |   300/  744 batches | lr 0.00012 | ms/batch 18.92 | loss 0.00019323\n",
      "| Epoch  14 |   350/  744 batches | lr 0.00012 | ms/batch 20.81 | loss 0.00024665\n",
      "| Epoch  14 |   400/  744 batches | lr 0.00012 | ms/batch 19.25 | loss 0.00024839\n",
      "| Epoch  14 |   450/  744 batches | lr 0.00012 | ms/batch 19.37 | loss 0.00016900\n",
      "| Epoch  14 |   500/  744 batches | lr 0.00012 | ms/batch 20.55 | loss 0.00017067\n",
      "| Epoch  14 |   550/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00017162\n",
      "| Epoch  14 |   600/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00052377\n",
      "| Epoch  14 |   650/  744 batches | lr 0.00012 | ms/batch 20.40 | loss 0.00019891\n",
      "| Epoch  14 |   700/  744 batches | lr 0.00012 | ms/batch 18.94 | loss 0.00017634\n",
      "\n",
      "Val set: Average loss: 0.00063459\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  15 |    50/  744 batches | lr 0.00012 | ms/batch 20.89 | loss 0.00039172\n",
      "| Epoch  15 |   100/  744 batches | lr 0.00012 | ms/batch 18.92 | loss 0.00015969\n",
      "| Epoch  15 |   150/  744 batches | lr 0.00012 | ms/batch 18.87 | loss 0.00014939\n",
      "| Epoch  15 |   200/  744 batches | lr 0.00012 | ms/batch 20.50 | loss 0.00017411\n",
      "| Epoch  15 |   250/  744 batches | lr 0.00012 | ms/batch 19.56 | loss 0.00016977\n",
      "| Epoch  15 |   300/  744 batches | lr 0.00012 | ms/batch 19.14 | loss 0.00035858\n",
      "| Epoch  15 |   350/  744 batches | lr 0.00012 | ms/batch 21.28 | loss 0.00020401\n",
      "| Epoch  15 |   400/  744 batches | lr 0.00012 | ms/batch 18.93 | loss 0.00016240\n",
      "| Epoch  15 |   450/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00013786\n",
      "| Epoch  15 |   500/  744 batches | lr 0.00012 | ms/batch 20.81 | loss 0.00024990\n",
      "| Epoch  15 |   550/  744 batches | lr 0.00012 | ms/batch 18.87 | loss 0.00016865\n",
      "| Epoch  15 |   600/  744 batches | lr 0.00012 | ms/batch 18.77 | loss 0.00028726\n",
      "| Epoch  15 |   650/  744 batches | lr 0.00012 | ms/batch 20.46 | loss 0.00020687\n",
      "| Epoch  15 |   700/  744 batches | lr 0.00012 | ms/batch 18.98 | loss 0.00015373\n",
      "\n",
      "Val set: Average loss: 0.00023194\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch  16 |    50/  744 batches | lr 0.00012 | ms/batch 19.27 | loss 0.00024085\n",
      "| Epoch  16 |   100/  744 batches | lr 0.00012 | ms/batch 20.54 | loss 0.00017359\n",
      "| Epoch  16 |   150/  744 batches | lr 0.00012 | ms/batch 18.79 | loss 0.00012707\n",
      "| Epoch  16 |   200/  744 batches | lr 0.00012 | ms/batch 19.02 | loss 0.00011942\n",
      "| Epoch  16 |   250/  744 batches | lr 0.00012 | ms/batch 20.57 | loss 0.00016952\n",
      "| Epoch  16 |   300/  744 batches | lr 0.00012 | ms/batch 18.97 | loss 0.00034070\n",
      "| Epoch  16 |   350/  744 batches | lr 0.00012 | ms/batch 19.06 | loss 0.00017033\n",
      "| Epoch  16 |   400/  744 batches | lr 0.00012 | ms/batch 20.54 | loss 0.00011831\n",
      "| Epoch  16 |   450/  744 batches | lr 0.00012 | ms/batch 19.04 | loss 0.00011482\n",
      "| Epoch  16 |   500/  744 batches | lr 0.00012 | ms/batch 18.75 | loss 0.00013826\n",
      "| Epoch  16 |   550/  744 batches | lr 0.00012 | ms/batch 20.52 | loss 0.00017791\n",
      "| Epoch  16 |   600/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00026804\n",
      "| Epoch  16 |   650/  744 batches | lr 0.00012 | ms/batch 18.96 | loss 0.00017080\n",
      "| Epoch  16 |   700/  744 batches | lr 0.00012 | ms/batch 20.57 | loss 0.00016852\n",
      "\n",
      "Val set: Average loss: 0.00013145\n",
      "\n",
      "| Epoch  17 |    50/  744 batches | lr 0.00012 | ms/batch 20.90 | loss 0.00018496\n",
      "| Epoch  17 |   100/  744 batches | lr 0.00012 | ms/batch 19.06 | loss 0.00014387\n",
      "| Epoch  17 |   150/  744 batches | lr 0.00012 | ms/batch 18.97 | loss 0.00019327\n",
      "| Epoch  17 |   200/  744 batches | lr 0.00012 | ms/batch 20.57 | loss 0.00023611\n",
      "| Epoch  17 |   250/  744 batches | lr 0.00012 | ms/batch 18.86 | loss 0.00023037\n",
      "| Epoch  17 |   300/  744 batches | lr 0.00012 | ms/batch 18.96 | loss 0.00033936\n",
      "| Epoch  17 |   350/  744 batches | lr 0.00012 | ms/batch 20.66 | loss 0.00020378\n",
      "| Epoch  17 |   400/  744 batches | lr 0.00012 | ms/batch 18.98 | loss 0.00014753\n",
      "| Epoch  17 |   450/  744 batches | lr 0.00012 | ms/batch 18.91 | loss 0.00012605\n",
      "| Epoch  17 |   500/  744 batches | lr 0.00012 | ms/batch 20.59 | loss 0.00015677\n",
      "| Epoch  17 |   550/  744 batches | lr 0.00012 | ms/batch 19.16 | loss 0.00012377\n",
      "| Epoch  17 |   600/  744 batches | lr 0.00012 | ms/batch 19.26 | loss 0.00027855\n",
      "| Epoch  17 |   650/  744 batches | lr 0.00012 | ms/batch 20.46 | loss 0.00019748\n",
      "| Epoch  17 |   700/  744 batches | lr 0.00012 | ms/batch 18.99 | loss 0.00013652\n",
      "\n",
      "Val set: Average loss: 0.00018100\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  18 |    50/  744 batches | lr 0.00012 | ms/batch 20.76 | loss 0.00019532\n",
      "| Epoch  18 |   100/  744 batches | lr 0.00012 | ms/batch 19.15 | loss 0.00011771\n",
      "| Epoch  18 |   150/  744 batches | lr 0.00012 | ms/batch 19.04 | loss 0.00019869\n",
      "| Epoch  18 |   200/  744 batches | lr 0.00012 | ms/batch 20.50 | loss 0.00012924\n",
      "| Epoch  18 |   250/  744 batches | lr 0.00012 | ms/batch 19.13 | loss 0.00018322\n",
      "| Epoch  18 |   300/  744 batches | lr 0.00012 | ms/batch 19.15 | loss 0.00026360\n",
      "| Epoch  18 |   350/  744 batches | lr 0.00012 | ms/batch 20.56 | loss 0.00012135\n",
      "| Epoch  18 |   400/  744 batches | lr 0.00012 | ms/batch 19.09 | loss 0.00010787\n",
      "| Epoch  18 |   450/  744 batches | lr 0.00012 | ms/batch 18.95 | loss 0.00016993\n",
      "| Epoch  18 |   500/  744 batches | lr 0.00012 | ms/batch 20.71 | loss 0.00013707\n",
      "| Epoch  18 |   550/  744 batches | lr 0.00012 | ms/batch 18.96 | loss 0.00022821\n",
      "| Epoch  18 |   600/  744 batches | lr 0.00012 | ms/batch 19.03 | loss 0.00018089\n",
      "| Epoch  18 |   650/  744 batches | lr 0.00012 | ms/batch 20.60 | loss 0.00013068\n",
      "| Epoch  18 |   700/  744 batches | lr 0.00012 | ms/batch 18.94 | loss 0.00017597\n",
      "\n",
      "Val set: Average loss: 0.00022364\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  19 |    50/  744 batches | lr 0.00012 | ms/batch 19.23 | loss 0.00013636\n",
      "| Epoch  19 |   100/  744 batches | lr 0.00012 | ms/batch 18.99 | loss 0.00020391\n",
      "| Epoch  19 |   150/  744 batches | lr 0.00012 | ms/batch 20.49 | loss 0.00021983\n",
      "| Epoch  19 |   200/  744 batches | lr 0.00012 | ms/batch 18.97 | loss 0.00011227\n",
      "| Epoch  19 |   250/  744 batches | lr 0.00012 | ms/batch 18.90 | loss 0.00014201\n",
      "| Epoch  19 |   300/  744 batches | lr 0.00012 | ms/batch 20.63 | loss 0.00017274\n",
      "| Epoch  19 |   350/  744 batches | lr 0.00012 | ms/batch 18.86 | loss 0.00015447\n",
      "| Epoch  19 |   400/  744 batches | lr 0.00012 | ms/batch 19.12 | loss 0.00019723\n",
      "| Epoch  19 |   450/  744 batches | lr 0.00012 | ms/batch 20.61 | loss 0.00016269\n",
      "| Epoch  19 |   500/  744 batches | lr 0.00012 | ms/batch 18.96 | loss 0.00013546\n",
      "| Epoch  19 |   550/  744 batches | lr 0.00012 | ms/batch 18.93 | loss 0.00021210\n",
      "| Epoch  19 |   600/  744 batches | lr 0.00012 | ms/batch 20.58 | loss 0.00030835\n",
      "| Epoch  19 |   650/  744 batches | lr 0.00012 | ms/batch 19.02 | loss 0.00017166\n",
      "| Epoch  19 |   700/  744 batches | lr 0.00012 | ms/batch 18.95 | loss 0.00012928\n",
      "\n",
      "Val set: Average loss: 0.00040239\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  20 |    50/  744 batches | lr 0.00012 | ms/batch 19.39 | loss 0.00024264\n",
      "| Epoch  20 |   100/  744 batches | lr 0.00012 | ms/batch 20.57 | loss 0.00014134\n",
      "| Epoch  20 |   150/  744 batches | lr 0.00012 | ms/batch 19.05 | loss 0.00011390\n",
      "| Epoch  20 |   200/  744 batches | lr 0.00012 | ms/batch 19.08 | loss 0.00011224\n",
      "| Epoch  20 |   250/  744 batches | lr 0.00012 | ms/batch 20.50 | loss 0.00014234\n",
      "| Epoch  20 |   300/  744 batches | lr 0.00012 | ms/batch 18.98 | loss 0.00017834\n",
      "| Epoch  20 |   350/  744 batches | lr 0.00012 | ms/batch 19.05 | loss 0.00016601\n",
      "| Epoch  20 |   400/  744 batches | lr 0.00012 | ms/batch 20.61 | loss 0.00012339\n",
      "| Epoch  20 |   450/  744 batches | lr 0.00012 | ms/batch 18.94 | loss 0.00014496\n",
      "| Epoch  20 |   500/  744 batches | lr 0.00012 | ms/batch 18.91 | loss 0.00011841\n",
      "| Epoch  20 |   550/  744 batches | lr 0.00012 | ms/batch 20.72 | loss 0.00049429\n",
      "| Epoch  20 |   600/  744 batches | lr 0.00012 | ms/batch 18.95 | loss 0.00034793\n",
      "| Epoch  20 |   650/  744 batches | lr 0.00012 | ms/batch 19.14 | loss 0.00017709\n",
      "| Epoch  20 |   700/  744 batches | lr 0.00012 | ms/batch 20.59 | loss 0.00011483\n",
      "\n",
      "Val set: Average loss: 0.00022121\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  21 |    50/  744 batches | lr 0.00012 | ms/batch 21.07 | loss 0.00023140\n",
      "| Epoch  21 |   100/  744 batches | lr 0.00012 | ms/batch 18.84 | loss 0.00010973\n",
      "| Epoch  21 |   150/  744 batches | lr 0.00012 | ms/batch 18.97 | loss 0.00013973\n",
      "| Epoch  21 |   200/  744 batches | lr 0.00012 | ms/batch 20.49 | loss 0.00016495\n",
      "| Epoch  21 |   250/  744 batches | lr 0.00012 | ms/batch 18.90 | loss 0.00010908\n",
      "| Epoch  21 |   300/  744 batches | lr 0.00012 | ms/batch 19.09 | loss 0.00023379\n",
      "| Epoch  21 |   350/  744 batches | lr 0.00012 | ms/batch 20.45 | loss 0.00016496\n",
      "| Epoch  21 |   400/  744 batches | lr 0.00012 | ms/batch 18.90 | loss 0.00012676\n",
      "| Epoch  21 |   450/  744 batches | lr 0.00012 | ms/batch 18.98 | loss 0.00010079\n",
      "| Epoch  21 |   500/  744 batches | lr 0.00012 | ms/batch 20.43 | loss 0.00020112\n",
      "| Epoch  21 |   550/  744 batches | lr 0.00012 | ms/batch 19.15 | loss 0.00013721\n",
      "| Epoch  21 |   600/  744 batches | lr 0.00012 | ms/batch 19.20 | loss 0.00015776\n",
      "| Epoch  21 |   650/  744 batches | lr 0.00012 | ms/batch 20.55 | loss 0.00012472\n",
      "| Epoch  21 |   700/  744 batches | lr 0.00012 | ms/batch 18.95 | loss 0.00009402\n",
      "\n",
      "Val set: Average loss: 0.00009375\n",
      "\n",
      "| Epoch  22 |    50/  744 batches | lr 0.00012 | ms/batch 17.52 | loss 0.00011419\n",
      "| Epoch  22 |   100/  744 batches | lr 0.00012 | ms/batch 18.80 | loss 0.00011247\n",
      "| Epoch  22 |   150/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00015438\n",
      "| Epoch  22 |   200/  744 batches | lr 0.00012 | ms/batch 18.70 | loss 0.00018341\n",
      "| Epoch  22 |   250/  744 batches | lr 0.00012 | ms/batch 19.15 | loss 0.00022944\n",
      "| Epoch  22 |   300/  744 batches | lr 0.00012 | ms/batch 17.37 | loss 0.00018759\n",
      "| Epoch  22 |   350/  744 batches | lr 0.00012 | ms/batch 18.88 | loss 0.00011259\n",
      "| Epoch  22 |   400/  744 batches | lr 0.00012 | ms/batch 18.57 | loss 0.00009032\n",
      "| Epoch  22 |   450/  744 batches | lr 0.00012 | ms/batch 18.60 | loss 0.00010722\n",
      "| Epoch  22 |   500/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00012519\n",
      "| Epoch  22 |   550/  744 batches | lr 0.00012 | ms/batch 17.39 | loss 0.00017814\n",
      "| Epoch  22 |   600/  744 batches | lr 0.00012 | ms/batch 18.72 | loss 0.00021525\n",
      "| Epoch  22 |   650/  744 batches | lr 0.00012 | ms/batch 18.84 | loss 0.00011193\n",
      "| Epoch  22 |   700/  744 batches | lr 0.00012 | ms/batch 18.77 | loss 0.00012517\n",
      "\n",
      "Val set: Average loss: 0.00013476\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  23 |    50/  744 batches | lr 0.00012 | ms/batch 19.20 | loss 0.00012583\n",
      "| Epoch  23 |   100/  744 batches | lr 0.00012 | ms/batch 18.80 | loss 0.00019047\n",
      "| Epoch  23 |   150/  744 batches | lr 0.00012 | ms/batch 17.23 | loss 0.00021915\n",
      "| Epoch  23 |   200/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00011152\n",
      "| Epoch  23 |   250/  744 batches | lr 0.00012 | ms/batch 18.77 | loss 0.00012801\n",
      "| Epoch  23 |   300/  744 batches | lr 0.00012 | ms/batch 19.15 | loss 0.00016471\n",
      "| Epoch  23 |   350/  744 batches | lr 0.00012 | ms/batch 19.48 | loss 0.00010691\n",
      "| Epoch  23 |   400/  744 batches | lr 0.00012 | ms/batch 17.07 | loss 0.00013505\n",
      "| Epoch  23 |   450/  744 batches | lr 0.00012 | ms/batch 18.70 | loss 0.00013444\n",
      "| Epoch  23 |   500/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00019636\n",
      "| Epoch  23 |   550/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00013488\n",
      "| Epoch  23 |   600/  744 batches | lr 0.00012 | ms/batch 17.15 | loss 0.00012804\n",
      "| Epoch  23 |   650/  744 batches | lr 0.00012 | ms/batch 18.91 | loss 0.00011218\n",
      "| Epoch  23 |   700/  744 batches | lr 0.00012 | ms/batch 19.07 | loss 0.00013074\n",
      "\n",
      "Val set: Average loss: 0.00014126\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  24 |    50/  744 batches | lr 0.00012 | ms/batch 19.34 | loss 0.00017369\n",
      "| Epoch  24 |   100/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00014995\n",
      "| Epoch  24 |   150/  744 batches | lr 0.00012 | ms/batch 18.75 | loss 0.00027931\n",
      "| Epoch  24 |   200/  744 batches | lr 0.00012 | ms/batch 17.20 | loss 0.00014245\n",
      "| Epoch  24 |   250/  744 batches | lr 0.00012 | ms/batch 18.86 | loss 0.00013925\n",
      "| Epoch  24 |   300/  744 batches | lr 0.00012 | ms/batch 18.95 | loss 0.00026366\n",
      "| Epoch  24 |   350/  744 batches | lr 0.00012 | ms/batch 18.77 | loss 0.00014465\n",
      "| Epoch  24 |   400/  744 batches | lr 0.00012 | ms/batch 18.66 | loss 0.00011447\n",
      "| Epoch  24 |   450/  744 batches | lr 0.00012 | ms/batch 17.16 | loss 0.00008581\n",
      "| Epoch  24 |   500/  744 batches | lr 0.00012 | ms/batch 18.62 | loss 0.00017643\n",
      "| Epoch  24 |   550/  744 batches | lr 0.00012 | ms/batch 18.69 | loss 0.00012929\n",
      "| Epoch  24 |   600/  744 batches | lr 0.00012 | ms/batch 18.94 | loss 0.00013348\n",
      "| Epoch  24 |   650/  744 batches | lr 0.00012 | ms/batch 18.70 | loss 0.00011264\n",
      "| Epoch  24 |   700/  744 batches | lr 0.00012 | ms/batch 17.35 | loss 0.00013001\n",
      "\n",
      "Val set: Average loss: 0.00016274\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  25 |    50/  744 batches | lr 0.00012 | ms/batch 19.26 | loss 0.00016258\n",
      "| Epoch  25 |   100/  744 batches | lr 0.00012 | ms/batch 18.48 | loss 0.00011133\n",
      "| Epoch  25 |   150/  744 batches | lr 0.00012 | ms/batch 18.85 | loss 0.00018175\n",
      "| Epoch  25 |   200/  744 batches | lr 0.00012 | ms/batch 18.69 | loss 0.00009058\n",
      "| Epoch  25 |   250/  744 batches | lr 0.00012 | ms/batch 17.14 | loss 0.00009440\n",
      "| Epoch  25 |   300/  744 batches | lr 0.00012 | ms/batch 18.88 | loss 0.00012739\n",
      "| Epoch  25 |   350/  744 batches | lr 0.00012 | ms/batch 18.88 | loss 0.00010675\n",
      "| Epoch  25 |   400/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00015399\n",
      "| Epoch  25 |   450/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00014207\n",
      "| Epoch  25 |   500/  744 batches | lr 0.00012 | ms/batch 17.21 | loss 0.00016034\n",
      "| Epoch  25 |   550/  744 batches | lr 0.00012 | ms/batch 18.78 | loss 0.00009002\n",
      "| Epoch  25 |   600/  744 batches | lr 0.00012 | ms/batch 18.93 | loss 0.00009944\n",
      "| Epoch  25 |   650/  744 batches | lr 0.00012 | ms/batch 18.75 | loss 0.00017089\n",
      "| Epoch  25 |   700/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00009747\n",
      "\n",
      "Val set: Average loss: 0.00027987\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  26 |    50/  744 batches | lr 0.00012 | ms/batch 19.26 | loss 0.00018680\n",
      "| Epoch  26 |   100/  744 batches | lr 0.00012 | ms/batch 17.14 | loss 0.00009667\n",
      "| Epoch  26 |   150/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00016297\n",
      "| Epoch  26 |   200/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00008421\n",
      "| Epoch  26 |   250/  744 batches | lr 0.00012 | ms/batch 18.71 | loss 0.00011638\n",
      "| Epoch  26 |   300/  744 batches | lr 0.00012 | ms/batch 18.84 | loss 0.00013162\n",
      "| Epoch  26 |   350/  744 batches | lr 0.00012 | ms/batch 17.21 | loss 0.00009220\n",
      "| Epoch  26 |   400/  744 batches | lr 0.00012 | ms/batch 18.80 | loss 0.00017147\n",
      "| Epoch  26 |   450/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00020943\n",
      "| Epoch  26 |   500/  744 batches | lr 0.00012 | ms/batch 18.72 | loss 0.00028831\n",
      "| Epoch  26 |   550/  744 batches | lr 0.00012 | ms/batch 18.89 | loss 0.00011701\n",
      "| Epoch  26 |   600/  744 batches | lr 0.00012 | ms/batch 17.05 | loss 0.00011653\n",
      "| Epoch  26 |   650/  744 batches | lr 0.00012 | ms/batch 18.69 | loss 0.00017258\n",
      "| Epoch  26 |   700/  744 batches | lr 0.00012 | ms/batch 18.71 | loss 0.00011216\n",
      "\n",
      "Val set: Average loss: 0.00022918\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  27 |    50/  744 batches | lr 0.00012 | ms/batch 19.06 | loss 0.00019125\n",
      "| Epoch  27 |   100/  744 batches | lr 0.00012 | ms/batch 18.89 | loss 0.00009880\n",
      "| Epoch  27 |   150/  744 batches | lr 0.00012 | ms/batch 17.04 | loss 0.00014224\n",
      "| Epoch  27 |   200/  744 batches | lr 0.00012 | ms/batch 18.69 | loss 0.00013614\n",
      "| Epoch  27 |   250/  744 batches | lr 0.00012 | ms/batch 18.85 | loss 0.00010394\n",
      "| Epoch  27 |   300/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00026604\n",
      "| Epoch  27 |   350/  744 batches | lr 0.00012 | ms/batch 18.92 | loss 0.00010689\n",
      "| Epoch  27 |   400/  744 batches | lr 0.00012 | ms/batch 17.04 | loss 0.00010397\n",
      "| Epoch  27 |   450/  744 batches | lr 0.00012 | ms/batch 18.77 | loss 0.00007484\n",
      "| Epoch  27 |   500/  744 batches | lr 0.00012 | ms/batch 19.28 | loss 0.00011021\n",
      "| Epoch  27 |   550/  744 batches | lr 0.00012 | ms/batch 18.69 | loss 0.00009240\n",
      "| Epoch  27 |   600/  744 batches | lr 0.00012 | ms/batch 18.71 | loss 0.00008532\n",
      "| Epoch  27 |   650/  744 batches | lr 0.00012 | ms/batch 17.26 | loss 0.00010360\n",
      "| Epoch  27 |   700/  744 batches | lr 0.00012 | ms/batch 18.61 | loss 0.00009813\n",
      "\n",
      "Val set: Average loss: 0.00022891\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch  28 |    50/  744 batches | lr 0.00012 | ms/batch 19.11 | loss 0.00012617\n",
      "| Epoch  28 |   100/  744 batches | lr 0.00012 | ms/batch 18.87 | loss 0.00009944\n",
      "| Epoch  28 |   150/  744 batches | lr 0.00012 | ms/batch 18.71 | loss 0.00009756\n",
      "| Epoch  28 |   200/  744 batches | lr 0.00012 | ms/batch 17.10 | loss 0.00011741\n",
      "| Epoch  28 |   250/  744 batches | lr 0.00012 | ms/batch 19.16 | loss 0.00012278\n",
      "| Epoch  28 |   300/  744 batches | lr 0.00012 | ms/batch 18.81 | loss 0.00012783\n",
      "| Epoch  28 |   350/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00010815\n",
      "| Epoch  28 |   400/  744 batches | lr 0.00012 | ms/batch 19.10 | loss 0.00012506\n",
      "| Epoch  28 |   450/  744 batches | lr 0.00012 | ms/batch 17.22 | loss 0.00009507\n",
      "| Epoch  28 |   500/  744 batches | lr 0.00012 | ms/batch 18.95 | loss 0.00016869\n",
      "| Epoch  28 |   550/  744 batches | lr 0.00012 | ms/batch 18.76 | loss 0.00009272\n",
      "| Epoch  28 |   600/  744 batches | lr 0.00012 | ms/batch 18.78 | loss 0.00011659\n",
      "| Epoch  28 |   650/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00013186\n",
      "| Epoch  28 |   700/  744 batches | lr 0.00012 | ms/batch 17.04 | loss 0.00007724\n",
      "\n",
      "Val set: Average loss: 0.00010533\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch  29 |    50/  744 batches | lr 0.00012 | ms/batch 17.43 | loss 0.00014423\n",
      "| Epoch  29 |   100/  744 batches | lr 0.00012 | ms/batch 18.91 | loss 0.00015257\n",
      "| Epoch  29 |   150/  744 batches | lr 0.00012 | ms/batch 18.58 | loss 0.00010721\n",
      "| Epoch  29 |   200/  744 batches | lr 0.00012 | ms/batch 18.77 | loss 0.00009060\n",
      "| Epoch  29 |   250/  744 batches | lr 0.00012 | ms/batch 19.00 | loss 0.00018758\n",
      "| Epoch  29 |   300/  744 batches | lr 0.00012 | ms/batch 17.04 | loss 0.00022085\n",
      "| Epoch  29 |   350/  744 batches | lr 0.00012 | ms/batch 18.79 | loss 0.00010524\n",
      "| Epoch  29 |   400/  744 batches | lr 0.00012 | ms/batch 19.01 | loss 0.00006927\n",
      "| Epoch  29 |   450/  744 batches | lr 0.00012 | ms/batch 18.68 | loss 0.00006966\n",
      "| Epoch  29 |   500/  744 batches | lr 0.00012 | ms/batch 18.80 | loss 0.00016502\n",
      "| Epoch  29 |   550/  744 batches | lr 0.00012 | ms/batch 17.14 | loss 0.00018317\n",
      "| Epoch  29 |   600/  744 batches | lr 0.00012 | ms/batch 18.65 | loss 0.00010338\n",
      "| Epoch  29 |   650/  744 batches | lr 0.00012 | ms/batch 18.70 | loss 0.00010816\n",
      "| Epoch  29 |   700/  744 batches | lr 0.00012 | ms/batch 18.68 | loss 0.00015148\n",
      "\n",
      "Val set: Average loss: 0.00014128\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch  30 |    50/  744 batches | lr 0.00012 | ms/batch 19.02 | loss 0.00015593\n",
      "| Epoch  30 |   100/  744 batches | lr 0.00012 | ms/batch 17.21 | loss 0.00008883\n",
      "| Epoch  30 |   150/  744 batches | lr 0.00012 | ms/batch 19.42 | loss 0.00008562\n",
      "| Epoch  30 |   200/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00008581\n",
      "| Epoch  30 |   250/  744 batches | lr 0.00012 | ms/batch 18.71 | loss 0.00018531\n",
      "| Epoch  30 |   300/  744 batches | lr 0.00012 | ms/batch 18.81 | loss 0.00012965\n",
      "| Epoch  30 |   350/  744 batches | lr 0.00012 | ms/batch 17.14 | loss 0.00010450\n",
      "| Epoch  30 |   400/  744 batches | lr 0.00012 | ms/batch 18.94 | loss 0.00007252\n",
      "| Epoch  30 |   450/  744 batches | lr 0.00012 | ms/batch 18.69 | loss 0.00007640\n",
      "| Epoch  30 |   500/  744 batches | lr 0.00012 | ms/batch 18.79 | loss 0.00015482\n",
      "| Epoch  30 |   550/  744 batches | lr 0.00012 | ms/batch 18.80 | loss 0.00012134\n",
      "| Epoch  30 |   600/  744 batches | lr 0.00012 | ms/batch 17.13 | loss 0.00010323\n",
      "| Epoch  30 |   650/  744 batches | lr 0.00012 | ms/batch 18.77 | loss 0.00011173\n",
      "| Epoch  30 |   700/  744 batches | lr 0.00012 | ms/batch 18.70 | loss 0.00009032\n",
      "\n",
      "Val set: Average loss: 0.00035988\n",
      "\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch  31 |    50/  744 batches | lr 0.00012 | ms/batch 19.16 | loss 0.00032698\n",
      "| Epoch  31 |   100/  744 batches | lr 0.00012 | ms/batch 18.53 | loss 0.00009272\n",
      "| Epoch  31 |   150/  744 batches | lr 0.00012 | ms/batch 18.84 | loss 0.00011463\n",
      "| Epoch  31 |   200/  744 batches | lr 0.00012 | ms/batch 17.08 | loss 0.00011654\n",
      "| Epoch  31 |   250/  744 batches | lr 0.00012 | ms/batch 18.76 | loss 0.00010799\n",
      "| Epoch  31 |   300/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00023005\n",
      "| Epoch  31 |   350/  744 batches | lr 0.00012 | ms/batch 18.78 | loss 0.00009800\n",
      "| Epoch  31 |   400/  744 batches | lr 0.00012 | ms/batch 18.79 | loss 0.00009804\n",
      "| Epoch  31 |   450/  744 batches | lr 0.00012 | ms/batch 17.01 | loss 0.00006868\n",
      "| Epoch  31 |   500/  744 batches | lr 0.00012 | ms/batch 18.84 | loss 0.00009918\n",
      "| Epoch  31 |   550/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00009661\n",
      "| Epoch  31 |   600/  744 batches | lr 0.00012 | ms/batch 18.72 | loss 0.00010648\n",
      "| Epoch  31 |   650/  744 batches | lr 0.00012 | ms/batch 17.04 | loss 0.00011355\n",
      "| Epoch  31 |   700/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00014666\n",
      "\n",
      "Val set: Average loss: 0.00025546\n",
      "\n",
      "EarlyStopping counter: 10 out of 20\n",
      "| Epoch  32 |    50/  744 batches | lr 0.00012 | ms/batch 19.12 | loss 0.00017890\n",
      "| Epoch  32 |   100/  744 batches | lr 0.00012 | ms/batch 18.63 | loss 0.00009075\n",
      "| Epoch  32 |   150/  744 batches | lr 0.00012 | ms/batch 18.92 | loss 0.00007419\n",
      "| Epoch  32 |   200/  744 batches | lr 0.00012 | ms/batch 18.79 | loss 0.00007370\n",
      "| Epoch  32 |   250/  744 batches | lr 0.00012 | ms/batch 17.25 | loss 0.00011189\n",
      "| Epoch  32 |   300/  744 batches | lr 0.00012 | ms/batch 18.52 | loss 0.00011003\n",
      "| Epoch  32 |   350/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00007005\n",
      "| Epoch  32 |   400/  744 batches | lr 0.00012 | ms/batch 18.75 | loss 0.00023479\n",
      "| Epoch  32 |   450/  744 batches | lr 0.00012 | ms/batch 19.11 | loss 0.00008139\n",
      "| Epoch  32 |   500/  744 batches | lr 0.00012 | ms/batch 17.21 | loss 0.00013181\n",
      "| Epoch  32 |   550/  744 batches | lr 0.00012 | ms/batch 18.89 | loss 0.00009227\n",
      "| Epoch  32 |   600/  744 batches | lr 0.00012 | ms/batch 18.75 | loss 0.00009029\n",
      "| Epoch  32 |   650/  744 batches | lr 0.00012 | ms/batch 18.64 | loss 0.00008275\n",
      "| Epoch  32 |   700/  744 batches | lr 0.00012 | ms/batch 18.87 | loss 0.00010277\n",
      "\n",
      "Val set: Average loss: 0.00018477\n",
      "\n",
      "EarlyStopping counter: 11 out of 20\n",
      "| Epoch  33 |    50/  744 batches | lr 0.00012 | ms/batch 19.08 | loss 0.00013445\n",
      "| Epoch  33 |   100/  744 batches | lr 0.00012 | ms/batch 17.13 | loss 0.00009958\n",
      "| Epoch  33 |   150/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00009168\n",
      "| Epoch  33 |   200/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00009207\n",
      "| Epoch  33 |   250/  744 batches | lr 0.00012 | ms/batch 18.65 | loss 0.00014298\n",
      "| Epoch  33 |   300/  744 batches | lr 0.00012 | ms/batch 18.69 | loss 0.00019682\n",
      "| Epoch  33 |   350/  744 batches | lr 0.00012 | ms/batch 17.01 | loss 0.00007197\n",
      "| Epoch  33 |   400/  744 batches | lr 0.00012 | ms/batch 18.69 | loss 0.00009023\n",
      "| Epoch  33 |   450/  744 batches | lr 0.00012 | ms/batch 18.96 | loss 0.00006875\n",
      "| Epoch  33 |   500/  744 batches | lr 0.00012 | ms/batch 18.92 | loss 0.00015246\n",
      "| Epoch  33 |   550/  744 batches | lr 0.00012 | ms/batch 17.13 | loss 0.00009555\n",
      "| Epoch  33 |   600/  744 batches | lr 0.00012 | ms/batch 18.66 | loss 0.00008924\n",
      "| Epoch  33 |   650/  744 batches | lr 0.00012 | ms/batch 18.92 | loss 0.00007082\n",
      "| Epoch  33 |   700/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00012208\n",
      "\n",
      "Val set: Average loss: 0.00008469\n",
      "\n",
      "| Epoch  34 |    50/  744 batches | lr 0.00012 | ms/batch 17.39 | loss 0.00013874\n",
      "| Epoch  34 |   100/  744 batches | lr 0.00012 | ms/batch 18.85 | loss 0.00010302\n",
      "| Epoch  34 |   150/  744 batches | lr 0.00012 | ms/batch 17.28 | loss 0.00010312\n",
      "| Epoch  34 |   200/  744 batches | lr 0.00012 | ms/batch 19.07 | loss 0.00007286\n",
      "| Epoch  34 |   250/  744 batches | lr 0.00012 | ms/batch 17.22 | loss 0.00019494\n",
      "| Epoch  34 |   300/  744 batches | lr 0.00012 | ms/batch 18.80 | loss 0.00022719\n",
      "| Epoch  34 |   350/  744 batches | lr 0.00012 | ms/batch 17.05 | loss 0.00009220\n",
      "| Epoch  34 |   400/  744 batches | lr 0.00012 | ms/batch 16.94 | loss 0.00008367\n",
      "| Epoch  34 |   450/  744 batches | lr 0.00012 | ms/batch 18.64 | loss 0.00006932\n",
      "| Epoch  34 |   500/  744 batches | lr 0.00012 | ms/batch 17.23 | loss 0.00008159\n",
      "| Epoch  34 |   550/  744 batches | lr 0.00012 | ms/batch 18.42 | loss 0.00008018\n",
      "| Epoch  34 |   600/  744 batches | lr 0.00012 | ms/batch 17.09 | loss 0.00009126\n",
      "| Epoch  34 |   650/  744 batches | lr 0.00012 | ms/batch 16.89 | loss 0.00007646\n",
      "| Epoch  34 |   700/  744 batches | lr 0.00012 | ms/batch 18.70 | loss 0.00011370\n",
      "\n",
      "Val set: Average loss: 0.00031459\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  35 |    50/  744 batches | lr 0.00012 | ms/batch 17.37 | loss 0.00015730\n",
      "| Epoch  35 |   100/  744 batches | lr 0.00012 | ms/batch 18.89 | loss 0.00008422\n",
      "| Epoch  35 |   150/  744 batches | lr 0.00012 | ms/batch 16.84 | loss 0.00007478\n",
      "| Epoch  35 |   200/  744 batches | lr 0.00012 | ms/batch 18.61 | loss 0.00007648\n",
      "| Epoch  35 |   250/  744 batches | lr 0.00012 | ms/batch 17.24 | loss 0.00021312\n",
      "| Epoch  35 |   300/  744 batches | lr 0.00012 | ms/batch 17.12 | loss 0.00017624\n",
      "| Epoch  35 |   350/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00008956\n",
      "| Epoch  35 |   400/  744 batches | lr 0.00012 | ms/batch 17.67 | loss 0.00007059\n",
      "| Epoch  35 |   450/  744 batches | lr 0.00012 | ms/batch 18.40 | loss 0.00006867\n",
      "| Epoch  35 |   500/  744 batches | lr 0.00012 | ms/batch 17.08 | loss 0.00007271\n",
      "| Epoch  35 |   550/  744 batches | lr 0.00012 | ms/batch 18.50 | loss 0.00006992\n",
      "| Epoch  35 |   600/  744 batches | lr 0.00012 | ms/batch 17.10 | loss 0.00015296\n",
      "| Epoch  35 |   650/  744 batches | lr 0.00012 | ms/batch 17.02 | loss 0.00010994\n",
      "| Epoch  35 |   700/  744 batches | lr 0.00012 | ms/batch 18.59 | loss 0.00014594\n",
      "\n",
      "Val set: Average loss: 0.00014359\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  36 |    50/  744 batches | lr 0.00012 | ms/batch 17.34 | loss 0.00012143\n",
      "| Epoch  36 |   100/  744 batches | lr 0.00012 | ms/batch 18.70 | loss 0.00007866\n",
      "| Epoch  36 |   150/  744 batches | lr 0.00012 | ms/batch 17.01 | loss 0.00007627\n",
      "| Epoch  36 |   200/  744 batches | lr 0.00012 | ms/batch 18.60 | loss 0.00009938\n",
      "| Epoch  36 |   250/  744 batches | lr 0.00012 | ms/batch 16.87 | loss 0.00010867\n",
      "| Epoch  36 |   300/  744 batches | lr 0.00012 | ms/batch 17.16 | loss 0.00012201\n",
      "| Epoch  36 |   350/  744 batches | lr 0.00012 | ms/batch 18.51 | loss 0.00005841\n",
      "| Epoch  36 |   400/  744 batches | lr 0.00012 | ms/batch 17.14 | loss 0.00009733\n",
      "| Epoch  36 |   450/  744 batches | lr 0.00012 | ms/batch 18.49 | loss 0.00008309\n",
      "| Epoch  36 |   500/  744 batches | lr 0.00012 | ms/batch 17.02 | loss 0.00012222\n",
      "| Epoch  36 |   550/  744 batches | lr 0.00012 | ms/batch 18.53 | loss 0.00010243\n",
      "| Epoch  36 |   600/  744 batches | lr 0.00012 | ms/batch 17.24 | loss 0.00020849\n",
      "| Epoch  36 |   650/  744 batches | lr 0.00012 | ms/batch 17.20 | loss 0.00019974\n",
      "| Epoch  36 |   700/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00011259\n",
      "\n",
      "Val set: Average loss: 0.00008124\n",
      "\n",
      "| Epoch  37 |    50/  744 batches | lr 0.00012 | ms/batch 19.42 | loss 0.00013144\n",
      "| Epoch  37 |   100/  744 batches | lr 0.00012 | ms/batch 20.44 | loss 0.00009603\n",
      "| Epoch  37 |   150/  744 batches | lr 0.00012 | ms/batch 18.76 | loss 0.00008586\n",
      "| Epoch  37 |   200/  744 batches | lr 0.00012 | ms/batch 18.77 | loss 0.00010204\n",
      "| Epoch  37 |   250/  744 batches | lr 0.00012 | ms/batch 20.59 | loss 0.00008302\n",
      "| Epoch  37 |   300/  744 batches | lr 0.00012 | ms/batch 18.90 | loss 0.00011921\n",
      "| Epoch  37 |   350/  744 batches | lr 0.00012 | ms/batch 18.81 | loss 0.00007195\n",
      "| Epoch  37 |   400/  744 batches | lr 0.00012 | ms/batch 20.61 | loss 0.00006445\n",
      "| Epoch  37 |   450/  744 batches | lr 0.00012 | ms/batch 18.87 | loss 0.00006493\n",
      "| Epoch  37 |   500/  744 batches | lr 0.00012 | ms/batch 18.89 | loss 0.00008748\n",
      "| Epoch  37 |   550/  744 batches | lr 0.00012 | ms/batch 20.69 | loss 0.00009529\n",
      "| Epoch  37 |   600/  744 batches | lr 0.00012 | ms/batch 18.85 | loss 0.00014108\n",
      "| Epoch  37 |   650/  744 batches | lr 0.00012 | ms/batch 18.84 | loss 0.00008494\n",
      "| Epoch  37 |   700/  744 batches | lr 0.00012 | ms/batch 20.43 | loss 0.00008888\n",
      "\n",
      "Val set: Average loss: 0.00010524\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  38 |    50/  744 batches | lr 0.00012 | ms/batch 19.07 | loss 0.00011039\n",
      "| Epoch  38 |   100/  744 batches | lr 0.00012 | ms/batch 20.48 | loss 0.00007102\n",
      "| Epoch  38 |   150/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00014704\n",
      "| Epoch  38 |   200/  744 batches | lr 0.00012 | ms/batch 18.92 | loss 0.00006998\n",
      "| Epoch  38 |   250/  744 batches | lr 0.00012 | ms/batch 20.41 | loss 0.00006169\n",
      "| Epoch  38 |   300/  744 batches | lr 0.00012 | ms/batch 18.94 | loss 0.00009494\n",
      "| Epoch  38 |   350/  744 batches | lr 0.00012 | ms/batch 18.85 | loss 0.00013725\n",
      "| Epoch  38 |   400/  744 batches | lr 0.00012 | ms/batch 20.50 | loss 0.00007184\n",
      "| Epoch  38 |   450/  744 batches | lr 0.00012 | ms/batch 18.95 | loss 0.00009195\n",
      "| Epoch  38 |   500/  744 batches | lr 0.00012 | ms/batch 18.99 | loss 0.00010867\n",
      "| Epoch  38 |   550/  744 batches | lr 0.00012 | ms/batch 21.27 | loss 0.00008287\n",
      "| Epoch  38 |   600/  744 batches | lr 0.00012 | ms/batch 20.10 | loss 0.00011543\n",
      "| Epoch  38 |   650/  744 batches | lr 0.00012 | ms/batch 18.91 | loss 0.00006344\n",
      "| Epoch  38 |   700/  744 batches | lr 0.00012 | ms/batch 21.09 | loss 0.00011124\n",
      "\n",
      "Val set: Average loss: 0.00018938\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  39 |    50/  744 batches | lr 0.00012 | ms/batch 20.46 | loss 0.00026840\n",
      "| Epoch  39 |   100/  744 batches | lr 0.00012 | ms/batch 20.38 | loss 0.00006844\n",
      "| Epoch  39 |   150/  744 batches | lr 0.00012 | ms/batch 20.61 | loss 0.00007499\n",
      "| Epoch  39 |   200/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00009947\n",
      "| Epoch  39 |   250/  744 batches | lr 0.00012 | ms/batch 18.91 | loss 0.00007762\n",
      "| Epoch  39 |   300/  744 batches | lr 0.00012 | ms/batch 20.41 | loss 0.00014110\n",
      "| Epoch  39 |   350/  744 batches | lr 0.00012 | ms/batch 18.99 | loss 0.00006699\n",
      "| Epoch  39 |   400/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00006249\n",
      "| Epoch  39 |   450/  744 batches | lr 0.00012 | ms/batch 20.37 | loss 0.00008362\n",
      "| Epoch  39 |   500/  744 batches | lr 0.00012 | ms/batch 19.04 | loss 0.00013086\n",
      "| Epoch  39 |   550/  744 batches | lr 0.00012 | ms/batch 18.95 | loss 0.00018873\n",
      "| Epoch  39 |   600/  744 batches | lr 0.00012 | ms/batch 20.62 | loss 0.00009063\n",
      "| Epoch  39 |   650/  744 batches | lr 0.00012 | ms/batch 18.84 | loss 0.00012370\n",
      "| Epoch  39 |   700/  744 batches | lr 0.00012 | ms/batch 19.13 | loss 0.00007762\n",
      "\n",
      "Val set: Average loss: 0.00009832\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  40 |    50/  744 batches | lr 0.00012 | ms/batch 19.15 | loss 0.00011237\n",
      "| Epoch  40 |   100/  744 batches | lr 0.00012 | ms/batch 18.88 | loss 0.00007104\n",
      "| Epoch  40 |   150/  744 batches | lr 0.00012 | ms/batch 20.56 | loss 0.00006168\n",
      "| Epoch  40 |   200/  744 batches | lr 0.00012 | ms/batch 19.24 | loss 0.00006977\n",
      "| Epoch  40 |   250/  744 batches | lr 0.00012 | ms/batch 18.76 | loss 0.00005965\n",
      "| Epoch  40 |   300/  744 batches | lr 0.00012 | ms/batch 20.43 | loss 0.00009333\n",
      "| Epoch  40 |   350/  744 batches | lr 0.00012 | ms/batch 19.13 | loss 0.00010178\n",
      "| Epoch  40 |   400/  744 batches | lr 0.00012 | ms/batch 18.80 | loss 0.00009655\n",
      "| Epoch  40 |   450/  744 batches | lr 0.00012 | ms/batch 20.64 | loss 0.00010137\n",
      "| Epoch  40 |   500/  744 batches | lr 0.00012 | ms/batch 19.25 | loss 0.00006776\n",
      "| Epoch  40 |   550/  744 batches | lr 0.00012 | ms/batch 18.68 | loss 0.00009123\n",
      "| Epoch  40 |   600/  744 batches | lr 0.00012 | ms/batch 20.64 | loss 0.00009355\n",
      "| Epoch  40 |   650/  744 batches | lr 0.00012 | ms/batch 18.84 | loss 0.00006871\n",
      "| Epoch  40 |   700/  744 batches | lr 0.00012 | ms/batch 18.86 | loss 0.00009840\n",
      "\n",
      "Val set: Average loss: 0.00007454\n",
      "\n",
      "| Epoch  41 |    50/  744 batches | lr 0.00012 | ms/batch 19.22 | loss 0.00018016\n",
      "| Epoch  41 |   100/  744 batches | lr 0.00012 | ms/batch 19.47 | loss 0.00006355\n",
      "| Epoch  41 |   150/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00007008\n",
      "| Epoch  41 |   200/  744 batches | lr 0.00012 | ms/batch 17.29 | loss 0.00006591\n",
      "| Epoch  41 |   250/  744 batches | lr 0.00012 | ms/batch 18.95 | loss 0.00008982\n",
      "| Epoch  41 |   300/  744 batches | lr 0.00012 | ms/batch 18.85 | loss 0.00029230\n",
      "| Epoch  41 |   350/  744 batches | lr 0.00012 | ms/batch 18.87 | loss 0.00006448\n",
      "| Epoch  41 |   400/  744 batches | lr 0.00012 | ms/batch 17.13 | loss 0.00006302\n",
      "| Epoch  41 |   450/  744 batches | lr 0.00012 | ms/batch 18.93 | loss 0.00007316\n",
      "| Epoch  41 |   500/  744 batches | lr 0.00012 | ms/batch 18.79 | loss 0.00015712\n",
      "| Epoch  41 |   550/  744 batches | lr 0.00012 | ms/batch 18.86 | loss 0.00012162\n",
      "| Epoch  41 |   600/  744 batches | lr 0.00012 | ms/batch 19.05 | loss 0.00007792\n",
      "| Epoch  41 |   650/  744 batches | lr 0.00012 | ms/batch 17.12 | loss 0.00009162\n",
      "| Epoch  41 |   700/  744 batches | lr 0.00012 | ms/batch 18.72 | loss 0.00006141\n",
      "\n",
      "Val set: Average loss: 0.00006118\n",
      "\n",
      "| Epoch  42 |    50/  744 batches | lr 0.00012 | ms/batch 18.88 | loss 0.00008409\n",
      "| Epoch  42 |   100/  744 batches | lr 0.00012 | ms/batch 16.99 | loss 0.00005563\n",
      "| Epoch  42 |   150/  744 batches | lr 0.00012 | ms/batch 18.55 | loss 0.00010742\n",
      "| Epoch  42 |   200/  744 batches | lr 0.00012 | ms/batch 17.11 | loss 0.00017341\n",
      "| Epoch  42 |   250/  744 batches | lr 0.00012 | ms/batch 16.98 | loss 0.00010773\n",
      "| Epoch  42 |   300/  744 batches | lr 0.00012 | ms/batch 18.62 | loss 0.00011182\n",
      "| Epoch  42 |   350/  744 batches | lr 0.00012 | ms/batch 17.07 | loss 0.00005876\n",
      "| Epoch  42 |   400/  744 batches | lr 0.00012 | ms/batch 18.64 | loss 0.00005596\n",
      "| Epoch  42 |   450/  744 batches | lr 0.00012 | ms/batch 16.90 | loss 0.00006655\n",
      "| Epoch  42 |   500/  744 batches | lr 0.00012 | ms/batch 18.76 | loss 0.00007070\n",
      "| Epoch  42 |   550/  744 batches | lr 0.00012 | ms/batch 17.05 | loss 0.00008956\n",
      "| Epoch  42 |   600/  744 batches | lr 0.00012 | ms/batch 16.87 | loss 0.00008566\n",
      "| Epoch  42 |   650/  744 batches | lr 0.00012 | ms/batch 18.50 | loss 0.00007539\n",
      "| Epoch  42 |   700/  744 batches | lr 0.00012 | ms/batch 17.13 | loss 0.00012854\n",
      "\n",
      "Val set: Average loss: 0.00005517\n",
      "\n",
      "| Epoch  43 |    50/  744 batches | lr 0.00012 | ms/batch 20.83 | loss 0.00008346\n",
      "| Epoch  43 |   100/  744 batches | lr 0.00012 | ms/batch 19.36 | loss 0.00007699\n",
      "| Epoch  43 |   150/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00018018\n",
      "| Epoch  43 |   200/  744 batches | lr 0.00012 | ms/batch 20.45 | loss 0.00022457\n",
      "| Epoch  43 |   250/  744 batches | lr 0.00012 | ms/batch 18.94 | loss 0.00008687\n",
      "| Epoch  43 |   300/  744 batches | lr 0.00012 | ms/batch 18.97 | loss 0.00011051\n",
      "| Epoch  43 |   350/  744 batches | lr 0.00012 | ms/batch 20.58 | loss 0.00005737\n",
      "| Epoch  43 |   400/  744 batches | lr 0.00012 | ms/batch 18.96 | loss 0.00007499\n",
      "| Epoch  43 |   450/  744 batches | lr 0.00012 | ms/batch 18.81 | loss 0.00005431\n",
      "| Epoch  43 |   500/  744 batches | lr 0.00012 | ms/batch 20.60 | loss 0.00021909\n",
      "| Epoch  43 |   550/  744 batches | lr 0.00012 | ms/batch 18.81 | loss 0.00010247\n",
      "| Epoch  43 |   600/  744 batches | lr 0.00012 | ms/batch 19.01 | loss 0.00009765\n",
      "| Epoch  43 |   650/  744 batches | lr 0.00012 | ms/batch 20.66 | loss 0.00013378\n",
      "| Epoch  43 |   700/  744 batches | lr 0.00012 | ms/batch 18.91 | loss 0.00009193\n",
      "\n",
      "Val set: Average loss: 0.00006314\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  44 |    50/  744 batches | lr 0.00012 | ms/batch 19.25 | loss 0.00009579\n",
      "| Epoch  44 |   100/  744 batches | lr 0.00012 | ms/batch 18.69 | loss 0.00009073\n",
      "| Epoch  44 |   150/  744 batches | lr 0.00012 | ms/batch 20.41 | loss 0.00007704\n",
      "| Epoch  44 |   200/  744 batches | lr 0.00012 | ms/batch 19.07 | loss 0.00007986\n",
      "| Epoch  44 |   250/  744 batches | lr 0.00012 | ms/batch 19.12 | loss 0.00005277\n",
      "| Epoch  44 |   300/  744 batches | lr 0.00012 | ms/batch 20.57 | loss 0.00006736\n",
      "| Epoch  44 |   350/  744 batches | lr 0.00012 | ms/batch 18.97 | loss 0.00004710\n",
      "| Epoch  44 |   400/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00005098\n",
      "| Epoch  44 |   450/  744 batches | lr 0.00012 | ms/batch 20.41 | loss 0.00005357\n",
      "| Epoch  44 |   500/  744 batches | lr 0.00012 | ms/batch 19.12 | loss 0.00011407\n",
      "| Epoch  44 |   550/  744 batches | lr 0.00012 | ms/batch 18.85 | loss 0.00009411\n",
      "| Epoch  44 |   600/  744 batches | lr 0.00012 | ms/batch 20.40 | loss 0.00007445\n",
      "| Epoch  44 |   650/  744 batches | lr 0.00012 | ms/batch 18.90 | loss 0.00006620\n",
      "| Epoch  44 |   700/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00010126\n",
      "\n",
      "Val set: Average loss: 0.00005927\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  45 |    50/  744 batches | lr 0.00012 | ms/batch 19.27 | loss 0.00011378\n",
      "| Epoch  45 |   100/  744 batches | lr 0.00012 | ms/batch 20.42 | loss 0.00009832\n",
      "| Epoch  45 |   150/  744 batches | lr 0.00012 | ms/batch 18.88 | loss 0.00009634\n",
      "| Epoch  45 |   200/  744 batches | lr 0.00012 | ms/batch 19.16 | loss 0.00007284\n",
      "| Epoch  45 |   250/  744 batches | lr 0.00012 | ms/batch 20.50 | loss 0.00006107\n",
      "| Epoch  45 |   300/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00008680\n",
      "| Epoch  45 |   350/  744 batches | lr 0.00012 | ms/batch 18.75 | loss 0.00018286\n",
      "| Epoch  45 |   400/  744 batches | lr 0.00012 | ms/batch 20.55 | loss 0.00006001\n",
      "| Epoch  45 |   450/  744 batches | lr 0.00012 | ms/batch 18.97 | loss 0.00004994\n",
      "| Epoch  45 |   500/  744 batches | lr 0.00012 | ms/batch 18.95 | loss 0.00007763\n",
      "| Epoch  45 |   550/  744 batches | lr 0.00012 | ms/batch 20.36 | loss 0.00009526\n",
      "| Epoch  45 |   600/  744 batches | lr 0.00012 | ms/batch 19.01 | loss 0.00008801\n",
      "| Epoch  45 |   650/  744 batches | lr 0.00012 | ms/batch 19.51 | loss 0.00007210\n",
      "| Epoch  45 |   700/  744 batches | lr 0.00012 | ms/batch 23.56 | loss 0.00012008\n",
      "\n",
      "Val set: Average loss: 0.00005472\n",
      "\n",
      "| Epoch  46 |    50/  744 batches | lr 0.00012 | ms/batch 19.34 | loss 0.00013134\n",
      "| Epoch  46 |   100/  744 batches | lr 0.00012 | ms/batch 18.81 | loss 0.00010594\n",
      "| Epoch  46 |   150/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00008177\n",
      "| Epoch  46 |   200/  744 batches | lr 0.00012 | ms/batch 17.16 | loss 0.00005533\n",
      "| Epoch  46 |   250/  744 batches | lr 0.00012 | ms/batch 18.66 | loss 0.00005948\n",
      "| Epoch  46 |   300/  744 batches | lr 0.00012 | ms/batch 18.71 | loss 0.00008613\n",
      "| Epoch  46 |   350/  744 batches | lr 0.00012 | ms/batch 18.66 | loss 0.00010557\n",
      "| Epoch  46 |   400/  744 batches | lr 0.00012 | ms/batch 19.12 | loss 0.00005697\n",
      "| Epoch  46 |   450/  744 batches | lr 0.00012 | ms/batch 17.63 | loss 0.00006240\n",
      "| Epoch  46 |   500/  744 batches | lr 0.00012 | ms/batch 18.61 | loss 0.00007047\n",
      "| Epoch  46 |   550/  744 batches | lr 0.00012 | ms/batch 18.71 | loss 0.00009015\n",
      "| Epoch  46 |   600/  744 batches | lr 0.00012 | ms/batch 18.54 | loss 0.00006709\n",
      "| Epoch  46 |   650/  744 batches | lr 0.00012 | ms/batch 18.64 | loss 0.00006928\n",
      "| Epoch  46 |   700/  744 batches | lr 0.00012 | ms/batch 17.06 | loss 0.00017008\n",
      "\n",
      "Val set: Average loss: 0.00012883\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  47 |    50/  744 batches | lr 0.00012 | ms/batch 19.04 | loss 0.00009018\n",
      "| Epoch  47 |   100/  744 batches | lr 0.00012 | ms/batch 18.62 | loss 0.00006878\n",
      "| Epoch  47 |   150/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00006714\n",
      "| Epoch  47 |   200/  744 batches | lr 0.00012 | ms/batch 18.76 | loss 0.00005544\n",
      "| Epoch  47 |   250/  744 batches | lr 0.00012 | ms/batch 16.99 | loss 0.00005578\n",
      "| Epoch  47 |   300/  744 batches | lr 0.00012 | ms/batch 18.78 | loss 0.00010719\n",
      "| Epoch  47 |   350/  744 batches | lr 0.00012 | ms/batch 18.83 | loss 0.00005350\n",
      "| Epoch  47 |   400/  744 batches | lr 0.00012 | ms/batch 18.70 | loss 0.00005653\n",
      "| Epoch  47 |   450/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00013736\n",
      "| Epoch  47 |   500/  744 batches | lr 0.00012 | ms/batch 17.03 | loss 0.00010888\n",
      "| Epoch  47 |   550/  744 batches | lr 0.00012 | ms/batch 18.54 | loss 0.00010292\n",
      "| Epoch  47 |   600/  744 batches | lr 0.00012 | ms/batch 18.62 | loss 0.00006252\n",
      "| Epoch  47 |   650/  744 batches | lr 0.00012 | ms/batch 18.78 | loss 0.00009233\n",
      "| Epoch  47 |   700/  744 batches | lr 0.00012 | ms/batch 18.87 | loss 0.00006963\n",
      "\n",
      "Val set: Average loss: 0.00006402\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  48 |    50/  744 batches | lr 0.00012 | ms/batch 18.92 | loss 0.00005681\n",
      "| Epoch  48 |   100/  744 batches | lr 0.00012 | ms/batch 17.01 | loss 0.00013210\n",
      "| Epoch  48 |   150/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00010762\n",
      "| Epoch  48 |   200/  744 batches | lr 0.00012 | ms/batch 18.94 | loss 0.00006635\n",
      "| Epoch  48 |   250/  744 batches | lr 0.00012 | ms/batch 18.87 | loss 0.00006258\n",
      "| Epoch  48 |   300/  744 batches | lr 0.00012 | ms/batch 18.65 | loss 0.00009440\n",
      "| Epoch  48 |   350/  744 batches | lr 0.00012 | ms/batch 17.16 | loss 0.00007822\n",
      "| Epoch  48 |   400/  744 batches | lr 0.00012 | ms/batch 18.63 | loss 0.00011617\n",
      "| Epoch  48 |   450/  744 batches | lr 0.00012 | ms/batch 18.88 | loss 0.00009458\n",
      "| Epoch  48 |   500/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00014734\n",
      "| Epoch  48 |   550/  744 batches | lr 0.00012 | ms/batch 17.16 | loss 0.00011098\n",
      "| Epoch  48 |   600/  744 batches | lr 0.00012 | ms/batch 18.88 | loss 0.00007768\n",
      "| Epoch  48 |   650/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00010609\n",
      "| Epoch  48 |   700/  744 batches | lr 0.00012 | ms/batch 18.84 | loss 0.00006500\n",
      "\n",
      "Val set: Average loss: 0.00015690\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  49 |    50/  744 batches | lr 0.00012 | ms/batch 18.97 | loss 0.00007360\n",
      "| Epoch  49 |   100/  744 batches | lr 0.00012 | ms/batch 18.75 | loss 0.00006049\n",
      "| Epoch  49 |   150/  744 batches | lr 0.00012 | ms/batch 17.12 | loss 0.00005278\n",
      "| Epoch  49 |   200/  744 batches | lr 0.00012 | ms/batch 18.75 | loss 0.00005288\n",
      "| Epoch  49 |   250/  744 batches | lr 0.00012 | ms/batch 18.84 | loss 0.00005643\n",
      "| Epoch  49 |   300/  744 batches | lr 0.00012 | ms/batch 18.78 | loss 0.00006838\n",
      "| Epoch  49 |   350/  744 batches | lr 0.00012 | ms/batch 18.65 | loss 0.00011223\n",
      "| Epoch  49 |   400/  744 batches | lr 0.00012 | ms/batch 17.23 | loss 0.00007793\n",
      "| Epoch  49 |   450/  744 batches | lr 0.00012 | ms/batch 18.85 | loss 0.00008835\n",
      "| Epoch  49 |   500/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00005488\n",
      "| Epoch  49 |   550/  744 batches | lr 0.00012 | ms/batch 18.70 | loss 0.00005092\n",
      "| Epoch  49 |   600/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00021537\n",
      "| Epoch  49 |   650/  744 batches | lr 0.00012 | ms/batch 17.13 | loss 0.00013333\n",
      "| Epoch  49 |   700/  744 batches | lr 0.00012 | ms/batch 18.89 | loss 0.00006284\n",
      "\n",
      "Val set: Average loss: 0.00011925\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  50 |    50/  744 batches | lr 0.00012 | ms/batch 19.06 | loss 0.00014304\n",
      "| Epoch  50 |   100/  744 batches | lr 0.00012 | ms/batch 18.63 | loss 0.00009409\n",
      "| Epoch  50 |   150/  744 batches | lr 0.00012 | ms/batch 18.71 | loss 0.00008249\n",
      "| Epoch  50 |   200/  744 batches | lr 0.00012 | ms/batch 17.24 | loss 0.00011022\n",
      "| Epoch  50 |   250/  744 batches | lr 0.00012 | ms/batch 18.72 | loss 0.00006382\n",
      "| Epoch  50 |   300/  744 batches | lr 0.00012 | ms/batch 18.64 | loss 0.00008848\n",
      "| Epoch  50 |   350/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00006206\n",
      "| Epoch  50 |   400/  744 batches | lr 0.00012 | ms/batch 18.71 | loss 0.00005700\n",
      "| Epoch  50 |   450/  744 batches | lr 0.00012 | ms/batch 17.05 | loss 0.00009306\n",
      "| Epoch  50 |   500/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00013915\n",
      "| Epoch  50 |   550/  744 batches | lr 0.00012 | ms/batch 18.84 | loss 0.00009463\n",
      "| Epoch  50 |   600/  744 batches | lr 0.00012 | ms/batch 18.75 | loss 0.00007261\n",
      "| Epoch  50 |   650/  744 batches | lr 0.00012 | ms/batch 18.69 | loss 0.00012896\n",
      "| Epoch  50 |   700/  744 batches | lr 0.00012 | ms/batch 17.30 | loss 0.00009427\n",
      "\n",
      "Val set: Average loss: 0.00043409\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  51 |    50/  744 batches | lr 0.00012 | ms/batch 17.53 | loss 0.00018944\n",
      "| Epoch  51 |   100/  744 batches | lr 0.00012 | ms/batch 18.69 | loss 0.00005479\n",
      "| Epoch  51 |   150/  744 batches | lr 0.00012 | ms/batch 18.66 | loss 0.00006687\n",
      "| Epoch  51 |   200/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00006583\n",
      "| Epoch  51 |   250/  744 batches | lr 0.00012 | ms/batch 18.72 | loss 0.00006170\n",
      "| Epoch  51 |   300/  744 batches | lr 0.00012 | ms/batch 17.09 | loss 0.00009131\n",
      "| Epoch  51 |   350/  744 batches | lr 0.00012 | ms/batch 18.54 | loss 0.00006422\n",
      "| Epoch  51 |   400/  744 batches | lr 0.00012 | ms/batch 18.95 | loss 0.00007065\n",
      "| Epoch  51 |   450/  744 batches | lr 0.00012 | ms/batch 18.87 | loss 0.00006194\n",
      "| Epoch  51 |   500/  744 batches | lr 0.00012 | ms/batch 17.14 | loss 0.00010454\n",
      "| Epoch  51 |   550/  744 batches | lr 0.00012 | ms/batch 18.85 | loss 0.00007178\n",
      "| Epoch  51 |   600/  744 batches | lr 0.00012 | ms/batch 18.68 | loss 0.00005317\n",
      "| Epoch  51 |   650/  744 batches | lr 0.00012 | ms/batch 18.73 | loss 0.00004935\n",
      "| Epoch  51 |   700/  744 batches | lr 0.00012 | ms/batch 18.69 | loss 0.00004171\n",
      "\n",
      "Val set: Average loss: 0.00010048\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch  52 |    50/  744 batches | lr 0.00012 | ms/batch 19.21 | loss 0.00010689\n",
      "| Epoch  52 |   100/  744 batches | lr 0.00012 | ms/batch 17.16 | loss 0.00006008\n",
      "| Epoch  52 |   150/  744 batches | lr 0.00012 | ms/batch 18.71 | loss 0.00006427\n",
      "| Epoch  52 |   200/  744 batches | lr 0.00012 | ms/batch 19.00 | loss 0.00004604\n",
      "| Epoch  52 |   250/  744 batches | lr 0.00012 | ms/batch 18.97 | loss 0.00017002\n",
      "| Epoch  52 |   300/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00005531\n",
      "| Epoch  52 |   350/  744 batches | lr 0.00012 | ms/batch 17.18 | loss 0.00004791\n",
      "| Epoch  52 |   400/  744 batches | lr 0.00012 | ms/batch 18.77 | loss 0.00005253\n",
      "| Epoch  52 |   450/  744 batches | lr 0.00012 | ms/batch 18.74 | loss 0.00009345\n",
      "| Epoch  52 |   500/  744 batches | lr 0.00012 | ms/batch 18.84 | loss 0.00008472\n",
      "| Epoch  52 |   550/  744 batches | lr 0.00012 | ms/batch 18.70 | loss 0.00005517\n",
      "| Epoch  52 |   600/  744 batches | lr 0.00012 | ms/batch 17.02 | loss 0.00008591\n",
      "| Epoch  52 |   650/  744 batches | lr 0.00012 | ms/batch 18.69 | loss 0.00008419\n",
      "| Epoch  52 |   700/  744 batches | lr 0.00012 | ms/batch 18.96 | loss 0.00005098\n",
      "\n",
      "Val set: Average loss: 0.00006271\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch  53 |    50/  744 batches | lr 0.00012 | ms/batch 18.99 | loss 0.00014267\n",
      "| Epoch  53 |   100/  744 batches | lr 0.00012 | ms/batch 18.66 | loss 0.00012616\n",
      "| Epoch  53 |   150/  744 batches | lr 0.00012 | ms/batch 17.02 | loss 0.00007160\n",
      "| Epoch  53 |   200/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00005424\n",
      "| Epoch  53 |   250/  744 batches | lr 0.00012 | ms/batch 19.04 | loss 0.00007488\n",
      "| Epoch  53 |   300/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00011342\n",
      "| Epoch  53 |   350/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00007722\n",
      "| Epoch  53 |   400/  744 batches | lr 0.00012 | ms/batch 17.25 | loss 0.00005088\n",
      "| Epoch  53 |   450/  744 batches | lr 0.00012 | ms/batch 18.68 | loss 0.00007346\n",
      "| Epoch  53 |   500/  744 batches | lr 0.00012 | ms/batch 18.58 | loss 0.00008140\n",
      "| Epoch  53 |   550/  744 batches | lr 0.00012 | ms/batch 18.79 | loss 0.00005711\n",
      "| Epoch  53 |   600/  744 batches | lr 0.00012 | ms/batch 18.76 | loss 0.00008065\n",
      "| Epoch  53 |   650/  744 batches | lr 0.00012 | ms/batch 17.18 | loss 0.00009303\n",
      "| Epoch  53 |   700/  744 batches | lr 0.00012 | ms/batch 18.78 | loss 0.00005377\n",
      "\n",
      "Val set: Average loss: 0.00019941\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch  54 |    50/  744 batches | lr 0.00012 | ms/batch 19.22 | loss 0.00009724\n",
      "| Epoch  54 |   100/  744 batches | lr 0.00012 | ms/batch 18.90 | loss 0.00007890\n",
      "| Epoch  54 |   150/  744 batches | lr 0.00012 | ms/batch 19.04 | loss 0.00004655\n",
      "| Epoch  54 |   200/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00004689\n",
      "| Epoch  54 |   250/  744 batches | lr 0.00012 | ms/batch 17.30 | loss 0.00011069\n",
      "| Epoch  54 |   300/  744 batches | lr 0.00012 | ms/batch 18.71 | loss 0.00012730\n",
      "| Epoch  54 |   350/  744 batches | lr 0.00012 | ms/batch 19.05 | loss 0.00007711\n",
      "| Epoch  54 |   400/  744 batches | lr 0.00012 | ms/batch 18.68 | loss 0.00005144\n",
      "| Epoch  54 |   450/  744 batches | lr 0.00012 | ms/batch 18.61 | loss 0.00006216\n",
      "| Epoch  54 |   500/  744 batches | lr 0.00012 | ms/batch 17.00 | loss 0.00009778\n",
      "| Epoch  54 |   550/  744 batches | lr 0.00012 | ms/batch 19.28 | loss 0.00006564\n",
      "| Epoch  54 |   600/  744 batches | lr 0.00012 | ms/batch 18.68 | loss 0.00007715\n",
      "| Epoch  54 |   650/  744 batches | lr 0.00012 | ms/batch 18.70 | loss 0.00006704\n",
      "| Epoch  54 |   700/  744 batches | lr 0.00012 | ms/batch 17.05 | loss 0.00004350\n",
      "\n",
      "Val set: Average loss: 0.00007178\n",
      "\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch  55 |    50/  744 batches | lr 0.00012 | ms/batch 17.35 | loss 0.00013986\n",
      "| Epoch  55 |   100/  744 batches | lr 0.00012 | ms/batch 18.61 | loss 0.00007114\n",
      "| Epoch  55 |   150/  744 batches | lr 0.00012 | ms/batch 18.54 | loss 0.00005969\n",
      "| Epoch  55 |   200/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00006630\n",
      "| Epoch  55 |   250/  744 batches | lr 0.00012 | ms/batch 18.92 | loss 0.00007912\n",
      "| Epoch  55 |   300/  744 batches | lr 0.00012 | ms/batch 17.14 | loss 0.00014251\n",
      "| Epoch  55 |   350/  744 batches | lr 0.00012 | ms/batch 18.92 | loss 0.00007376\n",
      "| Epoch  55 |   400/  744 batches | lr 0.00012 | ms/batch 18.79 | loss 0.00005340\n",
      "| Epoch  55 |   450/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00005354\n",
      "| Epoch  55 |   500/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00012401\n",
      "| Epoch  55 |   550/  744 batches | lr 0.00012 | ms/batch 17.13 | loss 0.00006851\n",
      "| Epoch  55 |   600/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00004943\n",
      "| Epoch  55 |   650/  744 batches | lr 0.00012 | ms/batch 18.52 | loss 0.00008522\n",
      "| Epoch  55 |   700/  744 batches | lr 0.00012 | ms/batch 18.63 | loss 0.00005289\n",
      "\n",
      "Val set: Average loss: 0.00006586\n",
      "\n",
      "EarlyStopping counter: 10 out of 20\n",
      "| Epoch  56 |    50/  744 batches | lr 0.00012 | ms/batch 19.26 | loss 0.00008690\n",
      "| Epoch  56 |   100/  744 batches | lr 0.00012 | ms/batch 18.65 | loss 0.00005295\n",
      "| Epoch  56 |   150/  744 batches | lr 0.00012 | ms/batch 16.98 | loss 0.00004683\n",
      "| Epoch  56 |   200/  744 batches | lr 0.00012 | ms/batch 18.65 | loss 0.00005220\n",
      "| Epoch  56 |   250/  744 batches | lr 0.00012 | ms/batch 18.78 | loss 0.00008364\n",
      "| Epoch  56 |   300/  744 batches | lr 0.00012 | ms/batch 18.76 | loss 0.00007183\n",
      "| Epoch  56 |   350/  744 batches | lr 0.00012 | ms/batch 18.60 | loss 0.00007202\n",
      "| Epoch  56 |   400/  744 batches | lr 0.00012 | ms/batch 17.24 | loss 0.00006889\n",
      "| Epoch  56 |   450/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00005858\n",
      "| Epoch  56 |   500/  744 batches | lr 0.00012 | ms/batch 18.72 | loss 0.00010139\n",
      "| Epoch  56 |   550/  744 batches | lr 0.00012 | ms/batch 19.18 | loss 0.00011451\n",
      "| Epoch  56 |   600/  744 batches | lr 0.00012 | ms/batch 18.43 | loss 0.00004928\n",
      "| Epoch  56 |   650/  744 batches | lr 0.00012 | ms/batch 21.84 | loss 0.00004914\n",
      "| Epoch  56 |   700/  744 batches | lr 0.00012 | ms/batch 22.09 | loss 0.00006098\n",
      "\n",
      "Val set: Average loss: 0.00005218\n",
      "\n",
      "| Epoch  57 |    50/  744 batches | lr 0.00012 | ms/batch 20.87 | loss 0.00006933\n",
      "| Epoch  57 |   100/  744 batches | lr 0.00012 | ms/batch 19.43 | loss 0.00007733\n",
      "| Epoch  57 |   150/  744 batches | lr 0.00012 | ms/batch 18.67 | loss 0.00008808\n",
      "| Epoch  57 |   200/  744 batches | lr 0.00012 | ms/batch 20.45 | loss 0.00006624\n",
      "| Epoch  57 |   250/  744 batches | lr 0.00012 | ms/batch 22.32 | loss 0.00011118\n",
      "| Epoch  57 |   300/  744 batches | lr 0.00012 | ms/batch 21.73 | loss 0.00016921\n",
      "| Epoch  57 |   350/  744 batches | lr 0.00012 | ms/batch 22.52 | loss 0.00006381\n",
      "| Epoch  57 |   400/  744 batches | lr 0.00012 | ms/batch 22.20 | loss 0.00004746\n",
      "| Epoch  57 |   450/  744 batches | lr 0.00012 | ms/batch 19.04 | loss 0.00006145\n",
      "| Epoch  57 |   500/  744 batches | lr 0.00012 | ms/batch 18.38 | loss 0.00010958\n",
      "| Epoch  57 |   550/  744 batches | lr 0.00012 | ms/batch 19.95 | loss 0.00006432\n",
      "| Epoch  57 |   600/  744 batches | lr 0.00012 | ms/batch 18.39 | loss 0.00005739\n",
      "| Epoch  57 |   650/  744 batches | lr 0.00012 | ms/batch 20.33 | loss 0.00006280\n",
      "| Epoch  57 |   700/  744 batches | lr 0.00012 | ms/batch 18.22 | loss 0.00005914\n",
      "\n",
      "Val set: Average loss: 0.00021888\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  58 |    50/  744 batches | lr 0.00012 | ms/batch 19.77 | loss 0.00016488\n",
      "| Epoch  58 |   100/  744 batches | lr 0.00012 | ms/batch 18.01 | loss 0.00008154\n",
      "| Epoch  58 |   150/  744 batches | lr 0.00012 | ms/batch 17.36 | loss 0.00006833\n",
      "| Epoch  58 |   200/  744 batches | lr 0.00012 | ms/batch 19.10 | loss 0.00009079\n",
      "| Epoch  58 |   250/  744 batches | lr 0.00012 | ms/batch 17.52 | loss 0.00006802\n",
      "| Epoch  58 |   300/  744 batches | lr 0.00012 | ms/batch 19.07 | loss 0.00007303\n",
      "| Epoch  58 |   350/  744 batches | lr 0.00012 | ms/batch 17.42 | loss 0.00005327\n",
      "| Epoch  58 |   400/  744 batches | lr 0.00012 | ms/batch 19.10 | loss 0.00004336\n",
      "| Epoch  58 |   450/  744 batches | lr 0.00012 | ms/batch 17.53 | loss 0.00004685\n",
      "| Epoch  58 |   500/  744 batches | lr 0.00012 | ms/batch 17.43 | loss 0.00005228\n",
      "| Epoch  58 |   550/  744 batches | lr 0.00012 | ms/batch 18.87 | loss 0.00005466\n",
      "| Epoch  58 |   600/  744 batches | lr 0.00012 | ms/batch 17.41 | loss 0.00007439\n",
      "| Epoch  58 |   650/  744 batches | lr 0.00012 | ms/batch 19.12 | loss 0.00006506\n",
      "| Epoch  58 |   700/  744 batches | lr 0.00012 | ms/batch 17.32 | loss 0.00006042\n",
      "\n",
      "Val set: Average loss: 0.00013205\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  59 |    50/  744 batches | lr 0.00012 | ms/batch 19.26 | loss 0.00025369\n",
      "| Epoch  59 |   100/  744 batches | lr 0.00012 | ms/batch 17.21 | loss 0.00008472\n",
      "| Epoch  59 |   150/  744 batches | lr 0.00012 | ms/batch 17.35 | loss 0.00006072\n",
      "| Epoch  59 |   200/  744 batches | lr 0.00012 | ms/batch 18.96 | loss 0.00009877\n",
      "| Epoch  59 |   250/  744 batches | lr 0.00012 | ms/batch 17.60 | loss 0.00006242\n",
      "| Epoch  59 |   300/  744 batches | lr 0.00012 | ms/batch 18.87 | loss 0.00010270\n",
      "| Epoch  59 |   350/  744 batches | lr 0.00012 | ms/batch 17.39 | loss 0.00008398\n",
      "| Epoch  59 |   400/  744 batches | lr 0.00012 | ms/batch 17.45 | loss 0.00007843\n",
      "| Epoch  59 |   450/  744 batches | lr 0.00012 | ms/batch 19.03 | loss 0.00005345\n",
      "| Epoch  59 |   500/  744 batches | lr 0.00012 | ms/batch 17.58 | loss 0.00009074\n",
      "| Epoch  59 |   550/  744 batches | lr 0.00012 | ms/batch 19.08 | loss 0.00007528\n",
      "| Epoch  59 |   600/  744 batches | lr 0.00012 | ms/batch 17.42 | loss 0.00007173\n",
      "| Epoch  59 |   650/  744 batches | lr 0.00012 | ms/batch 19.17 | loss 0.00007929\n",
      "| Epoch  59 |   700/  744 batches | lr 0.00012 | ms/batch 17.33 | loss 0.00005993\n",
      "\n",
      "Val set: Average loss: 0.00006973\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  60 |    50/  744 batches | lr 0.00012 | ms/batch 19.44 | loss 0.00005698\n",
      "| Epoch  60 |   100/  744 batches | lr 0.00012 | ms/batch 17.24 | loss 0.00004548\n",
      "| Epoch  60 |   150/  744 batches | lr 0.00012 | ms/batch 17.83 | loss 0.00004839\n",
      "| Epoch  60 |   200/  744 batches | lr 0.00012 | ms/batch 19.10 | loss 0.00004250\n",
      "| Epoch  60 |   250/  744 batches | lr 0.00012 | ms/batch 17.53 | loss 0.00007165\n",
      "| Epoch  60 |   300/  744 batches | lr 0.00012 | ms/batch 19.32 | loss 0.00007406\n",
      "| Epoch  60 |   350/  744 batches | lr 0.00012 | ms/batch 17.29 | loss 0.00004292\n",
      "| Epoch  60 |   400/  744 batches | lr 0.00012 | ms/batch 17.69 | loss 0.00005005\n",
      "| Epoch  60 |   450/  744 batches | lr 0.00012 | ms/batch 19.34 | loss 0.00008054\n",
      "| Epoch  60 |   500/  744 batches | lr 0.00012 | ms/batch 17.27 | loss 0.00007821\n",
      "| Epoch  60 |   550/  744 batches | lr 0.00012 | ms/batch 19.37 | loss 0.00006986\n",
      "| Epoch  60 |   600/  744 batches | lr 0.00012 | ms/batch 17.35 | loss 0.00006417\n",
      "| Epoch  60 |   650/  744 batches | lr 0.00012 | ms/batch 18.93 | loss 0.00005196\n",
      "| Epoch  60 |   700/  744 batches | lr 0.00012 | ms/batch 17.43 | loss 0.00006548\n",
      "\n",
      "Val set: Average loss: 0.00030251\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  61 |    50/  744 batches | lr 0.00012 | ms/batch 19.40 | loss 0.00011492\n",
      "| Epoch  61 |   100/  744 batches | lr 0.00012 | ms/batch 17.31 | loss 0.00007637\n",
      "| Epoch  61 |   150/  744 batches | lr 0.00012 | ms/batch 17.13 | loss 0.00007596\n",
      "| Epoch  61 |   200/  744 batches | lr 0.00012 | ms/batch 18.82 | loss 0.00006943\n",
      "| Epoch  61 |   250/  744 batches | lr 0.00012 | ms/batch 17.13 | loss 0.00007804\n",
      "| Epoch  61 |   300/  744 batches | lr 0.00012 | ms/batch 18.86 | loss 0.00009918\n",
      "| Epoch  61 |   350/  744 batches | lr 0.00012 | ms/batch 17.26 | loss 0.00004791\n",
      "| Epoch  61 |   400/  744 batches | lr 0.00012 | ms/batch 17.47 | loss 0.00003963\n",
      "| Epoch  61 |   450/  744 batches | lr 0.00012 | ms/batch 18.89 | loss 0.00005662\n",
      "| Epoch  61 |   500/  744 batches | lr 0.00012 | ms/batch 17.19 | loss 0.00011650\n",
      "| Epoch  61 |   550/  744 batches | lr 0.00012 | ms/batch 18.93 | loss 0.00005478\n",
      "| Epoch  61 |   600/  744 batches | lr 0.00012 | ms/batch 17.51 | loss 0.00004607\n",
      "| Epoch  61 |   650/  744 batches | lr 0.00012 | ms/batch 18.91 | loss 0.00005017\n",
      "| Epoch  61 |   700/  744 batches | lr 0.00012 | ms/batch 17.24 | loss 0.00015086\n",
      "\n",
      "Val set: Average loss: 0.00030986\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  62 |    50/  744 batches | lr 0.00012 | ms/batch 17.77 | loss 0.00011133\n",
      "| Epoch  62 |   100/  744 batches | lr 0.00012 | ms/batch 18.99 | loss 0.00005120\n",
      "| Epoch  62 |   150/  744 batches | lr 0.00012 | ms/batch 17.22 | loss 0.00005697\n",
      "| Epoch  62 |   200/  744 batches | lr 0.00012 | ms/batch 20.01 | loss 0.00010055\n",
      "| Epoch  62 |   250/  744 batches | lr 0.00012 | ms/batch 18.07 | loss 0.00006999\n",
      "| Epoch  62 |   300/  744 batches | lr 0.00012 | ms/batch 19.13 | loss 0.00006876\n",
      "| Epoch  62 |   350/  744 batches | lr 0.00012 | ms/batch 17.50 | loss 0.00005332\n",
      "| Epoch  62 |   400/  744 batches | lr 0.00012 | ms/batch 17.26 | loss 0.00003915\n",
      "| Epoch  62 |   450/  744 batches | lr 0.00012 | ms/batch 19.00 | loss 0.00007550\n",
      "| Epoch  62 |   500/  744 batches | lr 0.00012 | ms/batch 17.41 | loss 0.00010312\n",
      "| Epoch  62 |   550/  744 batches | lr 0.00012 | ms/batch 19.03 | loss 0.00006676\n",
      "| Epoch  62 |   600/  744 batches | lr 0.00012 | ms/batch 17.57 | loss 0.00005099\n",
      "| Epoch  62 |   650/  744 batches | lr 0.00012 | ms/batch 17.63 | loss 0.00004832\n",
      "| Epoch  62 |   700/  744 batches | lr 0.00012 | ms/batch 19.02 | loss 0.00012381\n",
      "\n",
      "Val set: Average loss: 0.00007990\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch  63 |    50/  744 batches | lr 0.00012 | ms/batch 17.63 | loss 0.00007828\n",
      "| Epoch  63 |   100/  744 batches | lr 0.00012 | ms/batch 19.29 | loss 0.00005388\n",
      "| Epoch  63 |   150/  744 batches | lr 0.00012 | ms/batch 17.28 | loss 0.00005763\n",
      "| Epoch  63 |   200/  744 batches | lr 0.00012 | ms/batch 19.01 | loss 0.00005484\n",
      "| Epoch  63 |   250/  744 batches | lr 0.00012 | ms/batch 17.44 | loss 0.00005063\n",
      "| Epoch  63 |   300/  744 batches | lr 0.00012 | ms/batch 19.00 | loss 0.00005874\n",
      "| Epoch  63 |   350/  744 batches | lr 0.00012 | ms/batch 17.47 | loss 0.00004678\n",
      "| Epoch  63 |   400/  744 batches | lr 0.00012 | ms/batch 17.42 | loss 0.00005334\n",
      "| Epoch  63 |   450/  744 batches | lr 0.00012 | ms/batch 19.02 | loss 0.00009499\n",
      "| Epoch  63 |   500/  744 batches | lr 0.00012 | ms/batch 17.10 | loss 0.00013433\n",
      "| Epoch  63 |   550/  744 batches | lr 0.00012 | ms/batch 19.28 | loss 0.00007753\n",
      "| Epoch  63 |   600/  744 batches | lr 0.00012 | ms/batch 17.29 | loss 0.00007299\n",
      "| Epoch  63 |   650/  744 batches | lr 0.00012 | ms/batch 17.42 | loss 0.00008330\n",
      "| Epoch  63 |   700/  744 batches | lr 0.00012 | ms/batch 19.05 | loss 0.00008617\n",
      "\n",
      "Val set: Average loss: 0.00010331\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch  64 |    50/  744 batches | lr 0.00012 | ms/batch 17.81 | loss 0.00009521\n",
      "| Epoch  64 |   100/  744 batches | lr 0.00012 | ms/batch 19.74 | loss 0.00006648\n",
      "| Epoch  64 |   150/  744 batches | lr 0.00012 | ms/batch 17.39 | loss 0.00009225\n",
      "| Epoch  64 |   200/  744 batches | lr 0.00012 | ms/batch 18.93 | loss 0.00008351\n",
      "| Epoch  64 |   250/  744 batches | lr 0.00012 | ms/batch 17.36 | loss 0.00006237\n",
      "| Epoch  64 |   300/  744 batches | lr 0.00012 | ms/batch 18.04 | loss 0.00010976\n",
      "| Epoch  64 |   350/  744 batches | lr 0.00012 | ms/batch 19.61 | loss 0.00008104\n",
      "| Epoch  64 |   400/  744 batches | lr 0.00012 | ms/batch 17.40 | loss 0.00006582\n",
      "| Epoch  64 |   450/  744 batches | lr 0.00012 | ms/batch 19.04 | loss 0.00004701\n",
      "| Epoch  64 |   500/  744 batches | lr 0.00012 | ms/batch 17.56 | loss 0.00011503\n",
      "| Epoch  64 |   550/  744 batches | lr 0.00012 | ms/batch 19.05 | loss 0.00009253\n",
      "| Epoch  64 |   600/  744 batches | lr 0.00012 | ms/batch 17.51 | loss 0.00016078\n",
      "| Epoch  64 |   650/  744 batches | lr 0.00012 | ms/batch 17.46 | loss 0.00007536\n",
      "| Epoch  64 |   700/  744 batches | lr 0.00012 | ms/batch 19.22 | loss 0.00005789\n",
      "\n",
      "Val set: Average loss: 0.00013122\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch  65 |    50/  744 batches | lr 0.00012 | ms/batch 17.66 | loss 0.00008687\n",
      "| Epoch  65 |   100/  744 batches | lr 0.00012 | ms/batch 18.87 | loss 0.00005857\n",
      "| Epoch  65 |   150/  744 batches | lr 0.00012 | ms/batch 17.56 | loss 0.00007042\n",
      "| Epoch  65 |   200/  744 batches | lr 0.00012 | ms/batch 18.95 | loss 0.00007794\n",
      "| Epoch  65 |   250/  744 batches | lr 0.00012 | ms/batch 17.14 | loss 0.00007540\n",
      "| Epoch  65 |   300/  744 batches | lr 0.00012 | ms/batch 17.44 | loss 0.00006651\n",
      "| Epoch  65 |   350/  744 batches | lr 0.00012 | ms/batch 19.00 | loss 0.00004565\n",
      "| Epoch  65 |   400/  744 batches | lr 0.00012 | ms/batch 17.58 | loss 0.00006637\n",
      "| Epoch  65 |   450/  744 batches | lr 0.00012 | ms/batch 19.26 | loss 0.00005149\n",
      "| Epoch  65 |   500/  744 batches | lr 0.00012 | ms/batch 17.36 | loss 0.00011768\n",
      "| Epoch  65 |   550/  744 batches | lr 0.00012 | ms/batch 19.19 | loss 0.00009840\n",
      "| Epoch  65 |   600/  744 batches | lr 0.00012 | ms/batch 17.56 | loss 0.00009492\n",
      "| Epoch  65 |   650/  744 batches | lr 0.00012 | ms/batch 17.32 | loss 0.00005362\n",
      "| Epoch  65 |   700/  744 batches | lr 0.00012 | ms/batch 19.11 | loss 0.00004443\n",
      "\n",
      "Val set: Average loss: 0.00004580\n",
      "\n",
      "| Epoch  66 |    50/  744 batches | lr 0.00012 | ms/batch 19.46 | loss 0.00005771\n",
      "| Epoch  66 |   100/  744 batches | lr 0.00012 | ms/batch 20.92 | loss 0.00004526\n",
      "| Epoch  66 |   150/  744 batches | lr 0.00012 | ms/batch 19.38 | loss 0.00004949\n",
      "| Epoch  66 |   200/  744 batches | lr 0.00012 | ms/batch 19.27 | loss 0.00003873\n",
      "| Epoch  66 |   250/  744 batches | lr 0.00012 | ms/batch 20.90 | loss 0.00003820\n",
      "| Epoch  66 |   300/  744 batches | lr 0.00012 | ms/batch 19.47 | loss 0.00006170\n",
      "| Epoch  66 |   350/  744 batches | lr 0.00012 | ms/batch 19.67 | loss 0.00004901\n",
      "| Epoch  66 |   400/  744 batches | lr 0.00012 | ms/batch 21.24 | loss 0.00005467\n",
      "| Epoch  66 |   450/  744 batches | lr 0.00012 | ms/batch 19.47 | loss 0.00005197\n",
      "| Epoch  66 |   500/  744 batches | lr 0.00012 | ms/batch 19.20 | loss 0.00005697\n",
      "| Epoch  66 |   550/  744 batches | lr 0.00012 | ms/batch 20.93 | loss 0.00007066\n",
      "| Epoch  66 |   600/  744 batches | lr 0.00012 | ms/batch 19.12 | loss 0.00006984\n",
      "| Epoch  66 |   650/  744 batches | lr 0.00012 | ms/batch 19.27 | loss 0.00005106\n",
      "| Epoch  66 |   700/  744 batches | lr 0.00012 | ms/batch 21.76 | loss 0.00009255\n",
      "\n",
      "Val set: Average loss: 0.00006538\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  67 |    50/  744 batches | lr 0.00012 | ms/batch 19.98 | loss 0.00005515\n",
      "| Epoch  67 |   100/  744 batches | lr 0.00012 | ms/batch 20.97 | loss 0.00006820\n",
      "| Epoch  67 |   150/  744 batches | lr 0.00012 | ms/batch 19.78 | loss 0.00005556\n",
      "| Epoch  67 |   200/  744 batches | lr 0.00012 | ms/batch 19.36 | loss 0.00004982\n",
      "| Epoch  67 |   250/  744 batches | lr 0.00012 | ms/batch 20.88 | loss 0.00004794\n",
      "| Epoch  67 |   300/  744 batches | lr 0.00012 | ms/batch 19.24 | loss 0.00011678\n",
      "| Epoch  67 |   350/  744 batches | lr 0.00012 | ms/batch 19.24 | loss 0.00005306\n",
      "| Epoch  67 |   400/  744 batches | lr 0.00012 | ms/batch 20.76 | loss 0.00004354\n",
      "| Epoch  67 |   450/  744 batches | lr 0.00012 | ms/batch 19.34 | loss 0.00007359\n",
      "| Epoch  67 |   500/  744 batches | lr 0.00012 | ms/batch 19.11 | loss 0.00005532\n",
      "| Epoch  67 |   550/  744 batches | lr 0.00012 | ms/batch 20.92 | loss 0.00006485\n",
      "| Epoch  67 |   600/  744 batches | lr 0.00012 | ms/batch 19.32 | loss 0.00009575\n",
      "| Epoch  67 |   650/  744 batches | lr 0.00012 | ms/batch 19.33 | loss 0.00004763\n",
      "| Epoch  67 |   700/  744 batches | lr 0.00012 | ms/batch 20.90 | loss 0.00007495\n",
      "\n",
      "Val set: Average loss: 0.00013550\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  68 |    50/  744 batches | lr 0.00012 | ms/batch 19.65 | loss 0.00009803\n",
      "| Epoch  68 |   100/  744 batches | lr 0.00012 | ms/batch 19.41 | loss 0.00005030\n",
      "| Epoch  68 |   150/  744 batches | lr 0.00012 | ms/batch 20.72 | loss 0.00004445\n",
      "| Epoch  68 |   200/  744 batches | lr 0.00012 | ms/batch 19.21 | loss 0.00004632\n",
      "| Epoch  68 |   250/  744 batches | lr 0.00012 | ms/batch 19.20 | loss 0.00005367\n",
      "| Epoch  68 |   300/  744 batches | lr 0.00012 | ms/batch 21.24 | loss 0.00009923\n",
      "| Epoch  68 |   350/  744 batches | lr 0.00012 | ms/batch 19.30 | loss 0.00007263\n",
      "| Epoch  68 |   400/  744 batches | lr 0.00012 | ms/batch 19.24 | loss 0.00004105\n",
      "| Epoch  68 |   450/  744 batches | lr 0.00012 | ms/batch 20.98 | loss 0.00006832\n",
      "| Epoch  68 |   500/  744 batches | lr 0.00012 | ms/batch 19.16 | loss 0.00006404\n",
      "| Epoch  68 |   550/  744 batches | lr 0.00012 | ms/batch 19.32 | loss 0.00004529\n",
      "| Epoch  68 |   600/  744 batches | lr 0.00012 | ms/batch 21.40 | loss 0.00007040\n",
      "| Epoch  68 |   650/  744 batches | lr 0.00012 | ms/batch 19.35 | loss 0.00004383\n",
      "| Epoch  68 |   700/  744 batches | lr 0.00012 | ms/batch 19.24 | loss 0.00004712\n",
      "\n",
      "Val set: Average loss: 0.00020810\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  69 |    50/  744 batches | lr 0.00012 | ms/batch 19.68 | loss 0.00012888\n",
      "| Epoch  69 |   100/  744 batches | lr 0.00012 | ms/batch 19.21 | loss 0.00005240\n",
      "| Epoch  69 |   150/  744 batches | lr 0.00012 | ms/batch 20.78 | loss 0.00004685\n",
      "| Epoch  69 |   200/  744 batches | lr 0.00012 | ms/batch 19.26 | loss 0.00005032\n",
      "| Epoch  69 |   250/  744 batches | lr 0.00012 | ms/batch 19.41 | loss 0.00005159\n",
      "| Epoch  69 |   300/  744 batches | lr 0.00012 | ms/batch 21.07 | loss 0.00005324\n",
      "| Epoch  69 |   350/  744 batches | lr 0.00012 | ms/batch 19.24 | loss 0.00011317\n",
      "| Epoch  69 |   400/  744 batches | lr 0.00012 | ms/batch 19.01 | loss 0.00005336\n",
      "| Epoch  69 |   450/  744 batches | lr 0.00012 | ms/batch 21.04 | loss 0.00006106\n",
      "| Epoch  69 |   500/  744 batches | lr 0.00012 | ms/batch 19.39 | loss 0.00004386\n",
      "| Epoch  69 |   550/  744 batches | lr 0.00012 | ms/batch 19.14 | loss 0.00006274\n",
      "| Epoch  69 |   600/  744 batches | lr 0.00012 | ms/batch 21.08 | loss 0.00006929\n",
      "| Epoch  69 |   650/  744 batches | lr 0.00012 | ms/batch 19.18 | loss 0.00007081\n",
      "| Epoch  69 |   700/  744 batches | lr 0.00012 | ms/batch 19.33 | loss 0.00010324\n",
      "\n",
      "Val set: Average loss: 0.00007786\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  70 |    50/  744 batches | lr 0.00012 | ms/batch 19.54 | loss 0.00010884\n",
      "| Epoch  70 |   100/  744 batches | lr 0.00012 | ms/batch 20.90 | loss 0.00005968\n",
      "| Epoch  70 |   150/  744 batches | lr 0.00012 | ms/batch 19.11 | loss 0.00007813\n",
      "| Epoch  70 |   200/  744 batches | lr 0.00012 | ms/batch 19.28 | loss 0.00005804\n",
      "| Epoch  70 |   250/  744 batches | lr 0.00012 | ms/batch 21.03 | loss 0.00006589\n",
      "| Epoch  70 |   300/  744 batches | lr 0.00012 | ms/batch 19.19 | loss 0.00005438\n",
      "| Epoch  70 |   350/  744 batches | lr 0.00012 | ms/batch 19.08 | loss 0.00005768\n",
      "| Epoch  70 |   400/  744 batches | lr 0.00012 | ms/batch 21.08 | loss 0.00004269\n",
      "| Epoch  70 |   450/  744 batches | lr 0.00012 | ms/batch 19.31 | loss 0.00004170\n",
      "| Epoch  70 |   500/  744 batches | lr 0.00012 | ms/batch 19.47 | loss 0.00004387\n",
      "| Epoch  70 |   550/  744 batches | lr 0.00012 | ms/batch 20.77 | loss 0.00008613\n",
      "| Epoch  70 |   600/  744 batches | lr 0.00012 | ms/batch 19.27 | loss 0.00008416\n",
      "| Epoch  70 |   650/  744 batches | lr 0.00012 | ms/batch 19.10 | loss 0.00005535\n",
      "| Epoch  70 |   700/  744 batches | lr 0.00012 | ms/batch 21.37 | loss 0.00006921\n",
      "\n",
      "Val set: Average loss: 0.00023216\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  71 |    50/  744 batches | lr 0.00012 | ms/batch 21.18 | loss 0.00012706\n",
      "| Epoch  71 |   100/  744 batches | lr 0.00012 | ms/batch 19.13 | loss 0.00009423\n",
      "| Epoch  71 |   150/  744 batches | lr 0.00012 | ms/batch 19.23 | loss 0.00007018\n",
      "| Epoch  71 |   200/  744 batches | lr 0.00012 | ms/batch 21.01 | loss 0.00006637\n",
      "| Epoch  71 |   250/  744 batches | lr 0.00012 | ms/batch 19.25 | loss 0.00005044\n",
      "| Epoch  71 |   300/  744 batches | lr 0.00012 | ms/batch 19.19 | loss 0.00008671\n",
      "| Epoch  71 |   350/  744 batches | lr 0.00012 | ms/batch 21.10 | loss 0.00005910\n",
      "| Epoch  71 |   400/  744 batches | lr 0.00012 | ms/batch 19.42 | loss 0.00005219\n",
      "| Epoch  71 |   450/  744 batches | lr 0.00012 | ms/batch 20.09 | loss 0.00004374\n",
      "| Epoch  71 |   500/  744 batches | lr 0.00012 | ms/batch 20.97 | loss 0.00006210\n",
      "| Epoch  71 |   550/  744 batches | lr 0.00012 | ms/batch 19.24 | loss 0.00006548\n",
      "| Epoch  71 |   600/  744 batches | lr 0.00012 | ms/batch 19.02 | loss 0.00005271\n",
      "| Epoch  71 |   650/  744 batches | lr 0.00012 | ms/batch 20.99 | loss 0.00006520\n",
      "| Epoch  71 |   700/  744 batches | lr 0.00012 | ms/batch 19.43 | loss 0.00007152\n",
      "\n",
      "Val set: Average loss: 0.00005272\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch  72 |    50/  744 batches | lr 0.00012 | ms/batch 19.56 | loss 0.00004564\n",
      "| Epoch  72 |   100/  744 batches | lr 0.00012 | ms/batch 19.32 | loss 0.00003561\n",
      "| Epoch  72 |   150/  744 batches | lr 0.00012 | ms/batch 20.86 | loss 0.00003292\n",
      "| Epoch  72 |   200/  744 batches | lr 0.00012 | ms/batch 19.26 | loss 0.00004326\n",
      "| Epoch  72 |   250/  744 batches | lr 0.00012 | ms/batch 20.36 | loss 0.00006035\n",
      "| Epoch  72 |   300/  744 batches | lr 0.00012 | ms/batch 21.05 | loss 0.00006423\n",
      "| Epoch  72 |   350/  744 batches | lr 0.00012 | ms/batch 18.95 | loss 0.00006853\n",
      "| Epoch  72 |   400/  744 batches | lr 0.00012 | ms/batch 19.37 | loss 0.00006226\n",
      "| Epoch  72 |   450/  744 batches | lr 0.00012 | ms/batch 21.55 | loss 0.00004937\n",
      "| Epoch  72 |   500/  744 batches | lr 0.00012 | ms/batch 19.30 | loss 0.00006369\n",
      "| Epoch  72 |   550/  744 batches | lr 0.00012 | ms/batch 19.24 | loss 0.00009889\n",
      "| Epoch  72 |   600/  744 batches | lr 0.00012 | ms/batch 20.80 | loss 0.00005195\n",
      "| Epoch  72 |   650/  744 batches | lr 0.00012 | ms/batch 20.56 | loss 0.00004092\n",
      "| Epoch  72 |   700/  744 batches | lr 0.00012 | ms/batch 19.45 | loss 0.00005203\n",
      "\n",
      "Val set: Average loss: 0.00005162\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch  73 |    50/  744 batches | lr 0.00012 | ms/batch 19.62 | loss 0.00007352\n",
      "| Epoch  73 |   100/  744 batches | lr 0.00012 | ms/batch 20.93 | loss 0.00016635\n",
      "| Epoch  73 |   150/  744 batches | lr 0.00012 | ms/batch 19.44 | loss 0.00015367\n",
      "| Epoch  73 |   200/  744 batches | lr 0.00012 | ms/batch 19.27 | loss 0.00008345\n",
      "| Epoch  73 |   250/  744 batches | lr 0.00012 | ms/batch 20.76 | loss 0.00005108\n",
      "| Epoch  73 |   300/  744 batches | lr 0.00012 | ms/batch 19.04 | loss 0.00005704\n",
      "| Epoch  73 |   350/  744 batches | lr 0.00012 | ms/batch 19.26 | loss 0.00003602\n",
      "| Epoch  73 |   400/  744 batches | lr 0.00012 | ms/batch 21.08 | loss 0.00004261\n",
      "| Epoch  73 |   450/  744 batches | lr 0.00012 | ms/batch 19.45 | loss 0.00004226\n",
      "| Epoch  73 |   500/  744 batches | lr 0.00012 | ms/batch 19.40 | loss 0.00005339\n",
      "| Epoch  73 |   550/  744 batches | lr 0.00012 | ms/batch 21.22 | loss 0.00005862\n",
      "| Epoch  73 |   600/  744 batches | lr 0.00012 | ms/batch 19.63 | loss 0.00004419\n",
      "| Epoch  73 |   650/  744 batches | lr 0.00012 | ms/batch 19.22 | loss 0.00004027\n",
      "| Epoch  73 |   700/  744 batches | lr 0.00012 | ms/batch 21.26 | loss 0.00004227\n",
      "\n",
      "Val set: Average loss: 0.00006300\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch  74 |    50/  744 batches | lr 0.00012 | ms/batch 21.29 | loss 0.00008602\n",
      "| Epoch  74 |   100/  744 batches | lr 0.00012 | ms/batch 19.37 | loss 0.00006699\n",
      "| Epoch  74 |   150/  744 batches | lr 0.00012 | ms/batch 19.71 | loss 0.00003766\n",
      "| Epoch  74 |   200/  744 batches | lr 0.00012 | ms/batch 20.85 | loss 0.00006238\n",
      "| Epoch  74 |   250/  744 batches | lr 0.00012 | ms/batch 19.11 | loss 0.00008238\n",
      "| Epoch  74 |   300/  744 batches | lr 0.00012 | ms/batch 19.53 | loss 0.00007935\n",
      "| Epoch  74 |   350/  744 batches | lr 0.00012 | ms/batch 21.02 | loss 0.00004432\n",
      "| Epoch  74 |   400/  744 batches | lr 0.00012 | ms/batch 19.21 | loss 0.00004887\n",
      "| Epoch  74 |   450/  744 batches | lr 0.00012 | ms/batch 19.39 | loss 0.00007206\n",
      "| Epoch  74 |   500/  744 batches | lr 0.00012 | ms/batch 20.91 | loss 0.00006966\n",
      "| Epoch  74 |   550/  744 batches | lr 0.00012 | ms/batch 19.55 | loss 0.00005217\n",
      "| Epoch  74 |   600/  744 batches | lr 0.00012 | ms/batch 19.31 | loss 0.00003819\n",
      "| Epoch  74 |   650/  744 batches | lr 0.00012 | ms/batch 20.94 | loss 0.00003327\n",
      "| Epoch  74 |   700/  744 batches | lr 0.00012 | ms/batch 19.45 | loss 0.00016274\n",
      "\n",
      "Val set: Average loss: 0.00007950\n",
      "\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch  75 |    50/  744 batches | lr 0.00012 | ms/batch 19.72 | loss 0.00007339\n",
      "| Epoch  75 |   100/  744 batches | lr 0.00012 | ms/batch 19.81 | loss 0.00005317\n",
      "| Epoch  75 |   150/  744 batches | lr 0.00012 | ms/batch 20.97 | loss 0.00004651\n",
      "| Epoch  75 |   200/  744 batches | lr 0.00012 | ms/batch 19.30 | loss 0.00004538\n",
      "| Epoch  75 |   250/  744 batches | lr 0.00012 | ms/batch 19.19 | loss 0.00004003\n",
      "| Epoch  75 |   300/  744 batches | lr 0.00012 | ms/batch 20.75 | loss 0.00004281\n",
      "| Epoch  75 |   350/  744 batches | lr 0.00012 | ms/batch 19.39 | loss 0.00006162\n",
      "| Epoch  75 |   400/  744 batches | lr 0.00012 | ms/batch 21.20 | loss 0.00007060\n",
      "| Epoch  75 |   450/  744 batches | lr 0.00012 | ms/batch 21.75 | loss 0.00011097\n",
      "| Epoch  75 |   500/  744 batches | lr 0.00012 | ms/batch 19.22 | loss 0.00006618\n",
      "| Epoch  75 |   550/  744 batches | lr 0.00012 | ms/batch 19.08 | loss 0.00005961\n",
      "| Epoch  75 |   600/  744 batches | lr 0.00012 | ms/batch 21.72 | loss 0.00004433\n",
      "| Epoch  75 |   650/  744 batches | lr 0.00012 | ms/batch 19.25 | loss 0.00004046\n",
      "| Epoch  75 |   700/  744 batches | lr 0.00012 | ms/batch 19.40 | loss 0.00010950\n",
      "\n",
      "Val set: Average loss: 0.00005802\n",
      "\n",
      "EarlyStopping counter: 10 out of 20\n",
      "| Epoch  76 |    50/  744 batches | lr 0.00012 | ms/batch 19.56 | loss 0.00004618\n",
      "| Epoch  76 |   100/  744 batches | lr 0.00012 | ms/batch 19.54 | loss 0.00003873\n",
      "| Epoch  76 |   150/  744 batches | lr 0.00012 | ms/batch 20.92 | loss 0.00004061\n",
      "| Epoch  76 |   200/  744 batches | lr 0.00012 | ms/batch 19.40 | loss 0.00005494\n",
      "| Epoch  76 |   250/  744 batches | lr 0.00012 | ms/batch 19.19 | loss 0.00005119\n",
      "| Epoch  76 |   300/  744 batches | lr 0.00012 | ms/batch 20.94 | loss 0.00004029\n",
      "| Epoch  76 |   350/  744 batches | lr 0.00012 | ms/batch 19.34 | loss 0.00011173\n",
      "| Epoch  76 |   400/  744 batches | lr 0.00012 | ms/batch 19.42 | loss 0.00007490\n",
      "| Epoch  76 |   450/  744 batches | lr 0.00012 | ms/batch 20.76 | loss 0.00009504\n",
      "| Epoch  76 |   500/  744 batches | lr 0.00012 | ms/batch 19.34 | loss 0.00009230\n",
      "| Epoch  76 |   550/  744 batches | lr 0.00012 | ms/batch 19.40 | loss 0.00008795\n",
      "| Epoch  76 |   600/  744 batches | lr 0.00012 | ms/batch 20.83 | loss 0.00009556\n",
      "| Epoch  76 |   650/  744 batches | lr 0.00012 | ms/batch 19.35 | loss 0.00009168\n",
      "| Epoch  76 |   700/  744 batches | lr 0.00012 | ms/batch 19.28 | loss 0.00005934\n",
      "\n",
      "Val set: Average loss: 0.00011465\n",
      "\n",
      "EarlyStopping counter: 11 out of 20\n",
      "| Epoch  77 |    50/  744 batches | lr 0.00012 | ms/batch 21.42 | loss 0.00007913\n",
      "| Epoch  77 |   100/  744 batches | lr 0.00012 | ms/batch 19.12 | loss 0.00005576\n",
      "| Epoch  77 |   150/  744 batches | lr 0.00012 | ms/batch 19.15 | loss 0.00004390\n",
      "| Epoch  77 |   200/  744 batches | lr 0.00012 | ms/batch 20.79 | loss 0.00005742\n",
      "| Epoch  77 |   250/  744 batches | lr 0.00012 | ms/batch 19.34 | loss 0.00004486\n",
      "| Epoch  77 |   300/  744 batches | lr 0.00012 | ms/batch 19.58 | loss 0.00005094\n",
      "| Epoch  77 |   350/  744 batches | lr 0.00012 | ms/batch 21.31 | loss 0.00004264\n",
      "| Epoch  77 |   400/  744 batches | lr 0.00012 | ms/batch 19.43 | loss 0.00005260\n",
      "| Epoch  77 |   450/  744 batches | lr 0.00012 | ms/batch 19.10 | loss 0.00003976\n",
      "| Epoch  77 |   500/  744 batches | lr 0.00012 | ms/batch 20.65 | loss 0.00004159\n",
      "| Epoch  77 |   550/  744 batches | lr 0.00012 | ms/batch 19.80 | loss 0.00003543\n",
      "| Epoch  77 |   600/  744 batches | lr 0.00012 | ms/batch 19.22 | loss 0.00003835\n",
      "| Epoch  77 |   650/  744 batches | lr 0.00012 | ms/batch 21.46 | loss 0.00007240\n",
      "| Epoch  77 |   700/  744 batches | lr 0.00012 | ms/batch 19.25 | loss 0.00005029\n",
      "\n",
      "Val set: Average loss: 0.00004915\n",
      "\n",
      "EarlyStopping counter: 12 out of 20\n",
      "| Epoch  78 |    50/  744 batches | lr 0.00012 | ms/batch 21.35 | loss 0.00008233\n",
      "| Epoch  78 |   100/  744 batches | lr 0.00012 | ms/batch 19.18 | loss 0.00003909\n",
      "| Epoch  78 |   150/  744 batches | lr 0.00012 | ms/batch 19.26 | loss 0.00007905\n",
      "| Epoch  78 |   200/  744 batches | lr 0.00012 | ms/batch 20.85 | loss 0.00008011\n",
      "| Epoch  78 |   250/  744 batches | lr 0.00012 | ms/batch 19.10 | loss 0.00004256\n",
      "| Epoch  78 |   300/  744 batches | lr 0.00012 | ms/batch 19.15 | loss 0.00004999\n",
      "| Epoch  78 |   350/  744 batches | lr 0.00012 | ms/batch 20.93 | loss 0.00006744\n",
      "| Epoch  78 |   400/  744 batches | lr 0.00012 | ms/batch 19.15 | loss 0.00005905\n",
      "| Epoch  78 |   450/  744 batches | lr 0.00012 | ms/batch 19.20 | loss 0.00005514\n",
      "| Epoch  78 |   500/  744 batches | lr 0.00012 | ms/batch 20.83 | loss 0.00004257\n",
      "| Epoch  78 |   550/  744 batches | lr 0.00012 | ms/batch 19.40 | loss 0.00004795\n",
      "| Epoch  78 |   600/  744 batches | lr 0.00012 | ms/batch 19.05 | loss 0.00007141\n",
      "| Epoch  78 |   650/  744 batches | lr 0.00012 | ms/batch 21.07 | loss 0.00008745\n",
      "| Epoch  78 |   700/  744 batches | lr 0.00012 | ms/batch 19.35 | loss 0.00006560\n",
      "\n",
      "Val set: Average loss: 0.00013845\n",
      "\n",
      "EarlyStopping counter: 13 out of 20\n",
      "| Epoch  79 |    50/  744 batches | lr 0.00012 | ms/batch 19.23 | loss 0.00013288\n",
      "| Epoch  79 |   100/  744 batches | lr 0.00012 | ms/batch 19.38 | loss 0.00006024\n",
      "| Epoch  79 |   150/  744 batches | lr 0.00012 | ms/batch 20.77 | loss 0.00005710\n",
      "| Epoch  79 |   200/  744 batches | lr 0.00012 | ms/batch 19.27 | loss 0.00004230\n",
      "| Epoch  79 |   250/  744 batches | lr 0.00012 | ms/batch 19.25 | loss 0.00003598\n",
      "| Epoch  79 |   300/  744 batches | lr 0.00012 | ms/batch 20.85 | loss 0.00004990\n",
      "| Epoch  79 |   350/  744 batches | lr 0.00012 | ms/batch 19.23 | loss 0.00006540\n",
      "| Epoch  79 |   400/  744 batches | lr 0.00012 | ms/batch 19.10 | loss 0.00005009\n",
      "| Epoch  79 |   450/  744 batches | lr 0.00012 | ms/batch 21.02 | loss 0.00003896\n",
      "| Epoch  79 |   500/  744 batches | lr 0.00012 | ms/batch 19.55 | loss 0.00003866\n",
      "| Epoch  79 |   550/  744 batches | lr 0.00012 | ms/batch 19.15 | loss 0.00014229\n",
      "| Epoch  79 |   600/  744 batches | lr 0.00012 | ms/batch 20.65 | loss 0.00008310\n",
      "| Epoch  79 |   650/  744 batches | lr 0.00012 | ms/batch 19.12 | loss 0.00009083\n",
      "| Epoch  79 |   700/  744 batches | lr 0.00012 | ms/batch 19.20 | loss 0.00006006\n",
      "\n",
      "Val set: Average loss: 0.00006204\n",
      "\n",
      "EarlyStopping counter: 14 out of 20\n",
      "| Epoch  80 |    50/  744 batches | lr 0.00012 | ms/batch 19.54 | loss 0.00006523\n",
      "| Epoch  80 |   100/  744 batches | lr 0.00012 | ms/batch 21.11 | loss 0.00004645\n",
      "| Epoch  80 |   150/  744 batches | lr 0.00012 | ms/batch 19.14 | loss 0.00004050\n",
      "| Epoch  80 |   200/  744 batches | lr 0.00012 | ms/batch 19.39 | loss 0.00003782\n",
      "| Epoch  80 |   250/  744 batches | lr 0.00012 | ms/batch 20.83 | loss 0.00003540\n",
      "| Epoch  80 |   300/  744 batches | lr 0.00012 | ms/batch 19.19 | loss 0.00005232\n",
      "| Epoch  80 |   350/  744 batches | lr 0.00012 | ms/batch 19.22 | loss 0.00006393\n",
      "| Epoch  80 |   400/  744 batches | lr 0.00012 | ms/batch 20.93 | loss 0.00005640\n",
      "| Epoch  80 |   450/  744 batches | lr 0.00012 | ms/batch 19.21 | loss 0.00005235\n",
      "| Epoch  80 |   500/  744 batches | lr 0.00012 | ms/batch 18.95 | loss 0.00004557\n",
      "| Epoch  80 |   550/  744 batches | lr 0.00012 | ms/batch 21.03 | loss 0.00006415\n",
      "| Epoch  80 |   600/  744 batches | lr 0.00012 | ms/batch 19.26 | loss 0.00005377\n",
      "| Epoch  80 |   650/  744 batches | lr 0.00012 | ms/batch 19.32 | loss 0.00005267\n",
      "| Epoch  80 |   700/  744 batches | lr 0.00012 | ms/batch 21.28 | loss 0.00004059\n",
      "\n",
      "Val set: Average loss: 0.00009201\n",
      "\n",
      "EarlyStopping counter: 15 out of 20\n",
      "| Epoch  81 |    50/  744 batches | lr 0.00012 | ms/batch 21.02 | loss 0.00010150\n",
      "| Epoch  81 |   100/  744 batches | lr 0.00012 | ms/batch 19.32 | loss 0.00003897\n",
      "| Epoch  81 |   150/  744 batches | lr 0.00012 | ms/batch 19.55 | loss 0.00003438\n",
      "| Epoch  81 |   200/  744 batches | lr 0.00012 | ms/batch 20.86 | loss 0.00003448\n",
      "| Epoch  81 |   250/  744 batches | lr 0.00012 | ms/batch 19.05 | loss 0.00005459\n",
      "| Epoch  81 |   300/  744 batches | lr 0.00012 | ms/batch 19.22 | loss 0.00008296\n",
      "| Epoch  81 |   350/  744 batches | lr 0.00012 | ms/batch 21.06 | loss 0.00010747\n",
      "| Epoch  81 |   400/  744 batches | lr 0.00012 | ms/batch 19.34 | loss 0.00004643\n",
      "| Epoch  81 |   450/  744 batches | lr 0.00012 | ms/batch 19.29 | loss 0.00004835\n",
      "| Epoch  81 |   500/  744 batches | lr 0.00012 | ms/batch 20.93 | loss 0.00006660\n",
      "| Epoch  81 |   550/  744 batches | lr 0.00012 | ms/batch 19.02 | loss 0.00004689\n",
      "| Epoch  81 |   600/  744 batches | lr 0.00012 | ms/batch 19.31 | loss 0.00006016\n",
      "| Epoch  81 |   650/  744 batches | lr 0.00012 | ms/batch 20.85 | loss 0.00004728\n",
      "| Epoch  81 |   700/  744 batches | lr 0.00012 | ms/batch 19.26 | loss 0.00004417\n",
      "\n",
      "Val set: Average loss: 0.00006262\n",
      "\n",
      "EarlyStopping counter: 16 out of 20\n",
      "| Epoch  82 |    50/  744 batches | lr 0.00012 | ms/batch 19.45 | loss 0.00011687\n",
      "| Epoch  82 |   100/  744 batches | lr 0.00012 | ms/batch 19.45 | loss 0.00004985\n",
      "| Epoch  82 |   150/  744 batches | lr 0.00012 | ms/batch 20.96 | loss 0.00004286\n",
      "| Epoch  82 |   200/  744 batches | lr 0.00012 | ms/batch 19.27 | loss 0.00003388\n",
      "| Epoch  82 |   250/  744 batches | lr 0.00012 | ms/batch 19.22 | loss 0.00003906\n",
      "| Epoch  82 |   300/  744 batches | lr 0.00012 | ms/batch 21.03 | loss 0.00008507\n",
      "| Epoch  82 |   350/  744 batches | lr 0.00012 | ms/batch 19.58 | loss 0.00006063\n",
      "| Epoch  82 |   400/  744 batches | lr 0.00012 | ms/batch 19.70 | loss 0.00004232\n",
      "| Epoch  82 |   450/  744 batches | lr 0.00012 | ms/batch 22.33 | loss 0.00003640\n",
      "| Epoch  82 |   500/  744 batches | lr 0.00012 | ms/batch 19.38 | loss 0.00004045\n",
      "| Epoch  82 |   550/  744 batches | lr 0.00012 | ms/batch 19.34 | loss 0.00008949\n",
      "| Epoch  82 |   600/  744 batches | lr 0.00012 | ms/batch 22.00 | loss 0.00009439\n",
      "| Epoch  82 |   650/  744 batches | lr 0.00012 | ms/batch 19.35 | loss 0.00005468\n",
      "| Epoch  82 |   700/  744 batches | lr 0.00012 | ms/batch 19.29 | loss 0.00004804\n",
      "\n",
      "Val set: Average loss: 0.00011857\n",
      "\n",
      "EarlyStopping counter: 17 out of 20\n",
      "| Epoch  83 |    50/  744 batches | lr 0.00012 | ms/batch 19.66 | loss 0.00011510\n",
      "| Epoch  83 |   100/  744 batches | lr 0.00012 | ms/batch 20.77 | loss 0.00005317\n",
      "| Epoch  83 |   150/  744 batches | lr 0.00012 | ms/batch 19.37 | loss 0.00004310\n",
      "| Epoch  83 |   200/  744 batches | lr 0.00012 | ms/batch 19.41 | loss 0.00003610\n",
      "| Epoch  83 |   250/  744 batches | lr 0.00012 | ms/batch 21.43 | loss 0.00003484\n",
      "| Epoch  83 |   300/  744 batches | lr 0.00012 | ms/batch 19.99 | loss 0.00008837\n",
      "| Epoch  83 |   350/  744 batches | lr 0.00012 | ms/batch 19.20 | loss 0.00008085\n",
      "| Epoch  83 |   400/  744 batches | lr 0.00012 | ms/batch 21.21 | loss 0.00005082\n",
      "| Epoch  83 |   450/  744 batches | lr 0.00012 | ms/batch 19.29 | loss 0.00006897\n",
      "| Epoch  83 |   500/  744 batches | lr 0.00012 | ms/batch 19.53 | loss 0.00005534\n",
      "| Epoch  83 |   550/  744 batches | lr 0.00012 | ms/batch 20.93 | loss 0.00006249\n",
      "| Epoch  83 |   600/  744 batches | lr 0.00012 | ms/batch 19.07 | loss 0.00007271\n",
      "| Epoch  83 |   650/  744 batches | lr 0.00012 | ms/batch 19.15 | loss 0.00010570\n",
      "| Epoch  83 |   700/  744 batches | lr 0.00012 | ms/batch 20.93 | loss 0.00006040\n",
      "\n",
      "Val set: Average loss: 0.00004862\n",
      "\n",
      "EarlyStopping counter: 18 out of 20\n",
      "| Epoch  84 |    50/  744 batches | lr 0.00012 | ms/batch 21.30 | loss 0.00008256\n",
      "| Epoch  84 |   100/  744 batches | lr 0.00012 | ms/batch 19.28 | loss 0.00006588\n",
      "| Epoch  84 |   150/  744 batches | lr 0.00012 | ms/batch 19.11 | loss 0.00004600\n",
      "| Epoch  84 |   200/  744 batches | lr 0.00012 | ms/batch 20.92 | loss 0.00008099\n",
      "| Epoch  84 |   250/  744 batches | lr 0.00012 | ms/batch 19.41 | loss 0.00004896\n",
      "| Epoch  84 |   300/  744 batches | lr 0.00012 | ms/batch 19.19 | loss 0.00006305\n",
      "| Epoch  84 |   350/  744 batches | lr 0.00012 | ms/batch 21.10 | loss 0.00004691\n",
      "| Epoch  84 |   400/  744 batches | lr 0.00012 | ms/batch 19.21 | loss 0.00004254\n",
      "| Epoch  84 |   450/  744 batches | lr 0.00012 | ms/batch 19.11 | loss 0.00004966\n",
      "| Epoch  84 |   500/  744 batches | lr 0.00012 | ms/batch 22.35 | loss 0.00007381\n",
      "| Epoch  84 |   550/  744 batches | lr 0.00012 | ms/batch 19.92 | loss 0.00007955\n",
      "| Epoch  84 |   600/  744 batches | lr 0.00012 | ms/batch 19.22 | loss 0.00013302\n",
      "| Epoch  84 |   650/  744 batches | lr 0.00012 | ms/batch 20.81 | loss 0.00006534\n",
      "| Epoch  84 |   700/  744 batches | lr 0.00012 | ms/batch 19.17 | loss 0.00005978\n",
      "\n",
      "Val set: Average loss: 0.00009991\n",
      "\n",
      "EarlyStopping counter: 19 out of 20\n",
      "| Epoch  85 |    50/  744 batches | lr 0.00012 | ms/batch 21.45 | loss 0.00006153\n",
      "| Epoch  85 |   100/  744 batches | lr 0.00012 | ms/batch 19.20 | loss 0.00007439\n",
      "| Epoch  85 |   150/  744 batches | lr 0.00012 | ms/batch 19.21 | loss 0.00007641\n",
      "| Epoch  85 |   200/  744 batches | lr 0.00012 | ms/batch 21.30 | loss 0.00006543\n",
      "| Epoch  85 |   250/  744 batches | lr 0.00012 | ms/batch 19.25 | loss 0.00004410\n",
      "| Epoch  85 |   300/  744 batches | lr 0.00012 | ms/batch 19.10 | loss 0.00005730\n",
      "| Epoch  85 |   350/  744 batches | lr 0.00012 | ms/batch 20.80 | loss 0.00003812\n",
      "| Epoch  85 |   400/  744 batches | lr 0.00012 | ms/batch 19.37 | loss 0.00006094\n",
      "| Epoch  85 |   450/  744 batches | lr 0.00012 | ms/batch 19.28 | loss 0.00006149\n",
      "| Epoch  85 |   500/  744 batches | lr 0.00012 | ms/batch 20.90 | loss 0.00013539\n",
      "| Epoch  85 |   550/  744 batches | lr 0.00012 | ms/batch 19.30 | loss 0.00007533\n",
      "| Epoch  85 |   600/  744 batches | lr 0.00012 | ms/batch 19.33 | loss 0.00005449\n",
      "| Epoch  85 |   650/  744 batches | lr 0.00012 | ms/batch 20.76 | loss 0.00005765\n",
      "| Epoch  85 |   700/  744 batches | lr 0.00012 | ms/batch 19.56 | loss 0.00005892\n",
      "\n",
      "Val set: Average loss: 0.00004842\n",
      "\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Stopping at Epoch: 85\n"
     ]
    }
   ],
   "source": [
    "load = False\n",
    "save_model_path = '../models/final_heston_model.chkpt'\n",
    "val_err_df_path = '../results/val_final_heston_model.csv'\n",
    "\n",
    "if not load:\n",
    "  train_losses, val_losses = train(\n",
    "      epochs,\n",
    "      batch_size,\n",
    "      model,\n",
    "      optimizer,\n",
    "      loss_fn,\n",
    "      X_train,\n",
    "      y_train,\n",
    "      X_val,\n",
    "      y_val)\n",
    "  val_err_df = pd.DataFrame({\n",
    "      'Training': train_losses,\n",
    "      'Validation': val_losses})\n",
    "  val_err_df.to_csv(val_err_df_path)\n",
    "  torch.save(model.state_dict(), save_model_path)\n",
    "else:\n",
    "  model = Net(input_size, output_size, hidden_size, num_layers, act_fn)\n",
    "  model.load_state_dict(torch.load(save_model_path, map_location=device))\n",
    "  model = model.to(device)\n",
    "  val_err_df = pd.read_csv(val_err_df_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650894219921,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "KkenWSRYvDpl",
    "outputId": "37746aa5-0b46-4a80-d14d-397868e89361",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_err_df[1:].plot(xlabel='epoch', ylabel='MSE')\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlG5Ky25vDpm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1650894219922,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "TBZ4eHAdvDpn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 30\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_out = model(X_test[0:test_size].to(device))\n",
    "\n",
    "test_out = output_sc.inverse_transform(test_out.cpu().detach().numpy())\n",
    "real_out = output_sc.inverse_transform(y_test[0:test_size].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1650894219926,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "wK1WeGIZvDpo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['Price', 'Strike', 'Kappa', 'Rho', 'Theta', 'Xi', 'V_0',\n",
    "       'Interest Rate', 'Time to Expiration', 'C', 'P']\n",
    "test_options = pd.DataFrame(input_sc.inverse_transform(X_test[0:test_size].detach().cpu().numpy()), columns=cols)\n",
    "test_options['Prediction'] = test_out\n",
    "test_options['Real'] = real_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1650894220545,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "TNw5-1jgvDpr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_options['Moneyness'] = test_options.Price / test_options.Strike\n",
    "test_options['Abs Error'] = np.abs(test_options.Prediction - test_options.Real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1650894220546,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "lAC6AdeVvDpr",
    "outputId": "78bd13cc-c5a0-4746-f402-9e84d8b78696",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         Price      Strike     Kappa       Rho     Theta        Xi       V_0  \\\n12  111.999382  157.027664  0.162257 -0.346176  0.005966  0.096042  0.340065   \n21  100.000237   83.006271  1.073231 -0.038701  0.463104  0.392042  0.085235   \n9   100.000237  148.018738  0.100272 -0.180037  0.220708  0.111194  0.437267   \n16  100.000237   98.996140  1.843753 -0.362289  0.378694  0.173918  0.479750   \n29  100.000237  150.007721  1.003907 -0.005206  0.144630  0.167716  0.423445   \n14  101.000053  160.030640  0.441473 -0.495600  0.282729  0.419668  0.053532   \n0    91.997589  165.022598  1.689353 -0.744619  0.207761  0.196682  0.179774   \n7    98.001068   53.015518  1.452399 -0.810086  0.039505  0.261969  0.259040   \n10   86.998978  173.017532  1.793037 -0.898135  0.433934  0.209367  0.486851   \n8   100.000237   79.008804  1.810506 -0.489986  0.033022  0.424320  0.236206   \n23   89.995193   54.029510  0.506840 -0.728633  0.369394  0.112181  0.247809   \n27  100.000237  123.994934  0.853535 -0.363494  0.375735  0.181882  0.495855   \n22  101.000053  100.994873  1.306733 -0.282550  0.063884  0.056293  0.345962   \n5   119.000381  177.970490  1.700059 -0.272527  0.273446  0.499447  0.381089   \n13  100.000237   62.024445  0.899884 -0.772531  0.293228  0.100975  0.173560   \n3   115.996803   91.001205  0.809019 -0.214292  0.486074  0.305426  0.465927   \n24   99.000420   81.992279  1.697242 -0.454589  0.002021  0.099283  0.378426   \n4   100.000237   64.013428  0.520082 -0.871746  0.435766  0.169760  0.252438   \n18  100.000237   79.008804  1.886015 -0.080568  0.268567  0.037546  0.365491   \n26  100.000237   62.024445  0.214663 -0.574230  0.423929  0.022183  0.388191   \n15  100.000237   57.032486  0.881358 -0.379417  0.128283  0.063763  0.155870   \n19  100.000237   92.999939  1.198259 -0.110510  0.057824  0.484365  0.309789   \n25  100.000237  112.000099  1.618351 -0.427248  0.311758  0.265387  0.082825   \n11  100.000237   83.006271  1.107394 -0.298662  0.219369  0.441657  0.458826   \n1   100.000237   90.006714  1.781204 -0.265676  0.151816  0.362019  0.332520   \n17  101.000053  124.999176  0.079422 -0.249436  0.191908  0.210460  0.278809   \n28  110.998192  179.998474  0.894530 -0.789025  0.498756  0.010624  0.216169   \n20  100.000237   80.997787  0.331308 -0.006729  0.322010  0.444758  0.355917   \n2   119.000381  118.998100  0.743793 -0.631957  0.020763  0.349615  0.302988   \n6   100.000237  101.999115  1.182622 -0.249436  0.069802  0.078282  0.147880   \n\n    Interest Rate  Time to Expiration         C         P  Prediction  \\\n12       0.040101            0.373818  0.999997  0.000003    0.442342   \n21       0.053101            0.103731 -0.000003  1.000003    0.015902   \n9        0.079532            0.365224 -0.000003  1.000003   44.869846   \n16       0.090164            0.920052  0.999997  0.000003    4.327912   \n29       0.069522            0.455535  0.999997  0.000003    0.056064   \n14       0.067258            0.938086  0.999997  0.000003   -0.051184   \n0        0.091636            0.966828 -0.000003  1.000003   66.684967   \n7        0.057435            0.341413  0.999997  0.000003   42.848530   \n10       0.027186            0.135572  0.999997  0.000003    0.060742   \n8        0.034151            0.187983 -0.000003  1.000003    0.307016   \n23       0.053009            0.530735  0.999997  0.000003   34.486374   \n27       0.065431            0.112466 -0.000003  1.000003   20.743811   \n22       0.093031            0.107676  0.999997  0.000003    2.674942   \n5        0.035153            0.197282  0.999997  0.000003    0.068439   \n13       0.062805            0.999514  0.999997  0.000003   35.394611   \n3        0.075435            0.799731 -0.000003  1.000003    0.453975   \n24       0.062373            0.476528  0.999997  0.000003   15.458697   \n4        0.041319            0.708434  0.999997  0.000003   34.846703   \n18       0.098157            0.492871 -0.000003  1.000003    0.164833   \n26       0.087424            0.768031 -0.000003  1.000003    0.163287   \n15       0.052979            0.476809 -0.000003  1.000003    0.164068   \n19       0.062862            0.422144  0.999997  0.000003    7.049657   \n25       0.031588            0.560076 -0.000003  1.000003   11.255805   \n11       0.067987            1.057843 -0.000003  1.000003    0.655141   \n1        0.010186            0.874121 -0.000003  1.000003    5.274084   \n17       0.081613            0.361983 -0.000003  1.000003   22.910194   \n28       0.059782            0.159805 -0.000003  1.000003   65.287148   \n20       0.016986            0.419467  0.999997  0.000003   20.328989   \n2        0.066769            0.478711  0.999997  0.000003    4.311134   \n6        0.025334            0.582037  0.999997  0.000003    2.661062   \n\n         Real  Moneyness  Abs Error  \n12   0.432261   0.713246   0.010081  \n21   0.040472   1.204731   0.024570  \n9   44.842133   0.675592   0.027714  \n16   4.286623   1.010143   0.041289  \n29   0.104005   0.666634   0.047941  \n14  -0.001884   0.631130   0.049300  \n0   66.739990   0.557485   0.055023  \n7   42.787884   1.848536   0.060646  \n10  -0.001884   0.502833   0.062626  \n8    0.241661   1.265685   0.065355  \n23  34.560307   1.665667   0.073933  \n27  20.829142   0.806486   0.085331  \n22   2.761820   1.000051   0.086878  \n5    0.156950   0.668652   0.088511  \n13  35.280350   1.612271   0.114262  \n3    0.336961   1.274673   0.117014  \n24  15.578036   1.207436   0.119339  \n4   34.719139   1.562176   0.127563  \n18   0.029883   1.265685   0.134950  \n26   0.008705   1.612271   0.154582  \n15  -0.001884   1.753391   0.165952  \n19   6.880905   1.075272   0.168753  \n25  11.079407   0.892858   0.176398  \n11   0.464028   1.204731   0.191113  \n1    5.499053   1.111031   0.224969  \n17  22.674259   0.808006   0.235935  \n28  65.003410   0.616662   0.283737  \n20  19.984676   1.234605   0.344313  \n2    4.794890   1.000019   0.483756  \n6    3.164198   0.980403   0.503137  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>Strike</th>\n      <th>Kappa</th>\n      <th>Rho</th>\n      <th>Theta</th>\n      <th>Xi</th>\n      <th>V_0</th>\n      <th>Interest Rate</th>\n      <th>Time to Expiration</th>\n      <th>C</th>\n      <th>P</th>\n      <th>Prediction</th>\n      <th>Real</th>\n      <th>Moneyness</th>\n      <th>Abs Error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>111.999382</td>\n      <td>157.027664</td>\n      <td>0.162257</td>\n      <td>-0.346176</td>\n      <td>0.005966</td>\n      <td>0.096042</td>\n      <td>0.340065</td>\n      <td>0.040101</td>\n      <td>0.373818</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>0.442342</td>\n      <td>0.432261</td>\n      <td>0.713246</td>\n      <td>0.010081</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>100.000237</td>\n      <td>83.006271</td>\n      <td>1.073231</td>\n      <td>-0.038701</td>\n      <td>0.463104</td>\n      <td>0.392042</td>\n      <td>0.085235</td>\n      <td>0.053101</td>\n      <td>0.103731</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.015902</td>\n      <td>0.040472</td>\n      <td>1.204731</td>\n      <td>0.024570</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>100.000237</td>\n      <td>148.018738</td>\n      <td>0.100272</td>\n      <td>-0.180037</td>\n      <td>0.220708</td>\n      <td>0.111194</td>\n      <td>0.437267</td>\n      <td>0.079532</td>\n      <td>0.365224</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>44.869846</td>\n      <td>44.842133</td>\n      <td>0.675592</td>\n      <td>0.027714</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>100.000237</td>\n      <td>98.996140</td>\n      <td>1.843753</td>\n      <td>-0.362289</td>\n      <td>0.378694</td>\n      <td>0.173918</td>\n      <td>0.479750</td>\n      <td>0.090164</td>\n      <td>0.920052</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>4.327912</td>\n      <td>4.286623</td>\n      <td>1.010143</td>\n      <td>0.041289</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>100.000237</td>\n      <td>150.007721</td>\n      <td>1.003907</td>\n      <td>-0.005206</td>\n      <td>0.144630</td>\n      <td>0.167716</td>\n      <td>0.423445</td>\n      <td>0.069522</td>\n      <td>0.455535</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>0.056064</td>\n      <td>0.104005</td>\n      <td>0.666634</td>\n      <td>0.047941</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>101.000053</td>\n      <td>160.030640</td>\n      <td>0.441473</td>\n      <td>-0.495600</td>\n      <td>0.282729</td>\n      <td>0.419668</td>\n      <td>0.053532</td>\n      <td>0.067258</td>\n      <td>0.938086</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>-0.051184</td>\n      <td>-0.001884</td>\n      <td>0.631130</td>\n      <td>0.049300</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>91.997589</td>\n      <td>165.022598</td>\n      <td>1.689353</td>\n      <td>-0.744619</td>\n      <td>0.207761</td>\n      <td>0.196682</td>\n      <td>0.179774</td>\n      <td>0.091636</td>\n      <td>0.966828</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>66.684967</td>\n      <td>66.739990</td>\n      <td>0.557485</td>\n      <td>0.055023</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>98.001068</td>\n      <td>53.015518</td>\n      <td>1.452399</td>\n      <td>-0.810086</td>\n      <td>0.039505</td>\n      <td>0.261969</td>\n      <td>0.259040</td>\n      <td>0.057435</td>\n      <td>0.341413</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>42.848530</td>\n      <td>42.787884</td>\n      <td>1.848536</td>\n      <td>0.060646</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>86.998978</td>\n      <td>173.017532</td>\n      <td>1.793037</td>\n      <td>-0.898135</td>\n      <td>0.433934</td>\n      <td>0.209367</td>\n      <td>0.486851</td>\n      <td>0.027186</td>\n      <td>0.135572</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>0.060742</td>\n      <td>-0.001884</td>\n      <td>0.502833</td>\n      <td>0.062626</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>100.000237</td>\n      <td>79.008804</td>\n      <td>1.810506</td>\n      <td>-0.489986</td>\n      <td>0.033022</td>\n      <td>0.424320</td>\n      <td>0.236206</td>\n      <td>0.034151</td>\n      <td>0.187983</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.307016</td>\n      <td>0.241661</td>\n      <td>1.265685</td>\n      <td>0.065355</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>89.995193</td>\n      <td>54.029510</td>\n      <td>0.506840</td>\n      <td>-0.728633</td>\n      <td>0.369394</td>\n      <td>0.112181</td>\n      <td>0.247809</td>\n      <td>0.053009</td>\n      <td>0.530735</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>34.486374</td>\n      <td>34.560307</td>\n      <td>1.665667</td>\n      <td>0.073933</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>100.000237</td>\n      <td>123.994934</td>\n      <td>0.853535</td>\n      <td>-0.363494</td>\n      <td>0.375735</td>\n      <td>0.181882</td>\n      <td>0.495855</td>\n      <td>0.065431</td>\n      <td>0.112466</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>20.743811</td>\n      <td>20.829142</td>\n      <td>0.806486</td>\n      <td>0.085331</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>101.000053</td>\n      <td>100.994873</td>\n      <td>1.306733</td>\n      <td>-0.282550</td>\n      <td>0.063884</td>\n      <td>0.056293</td>\n      <td>0.345962</td>\n      <td>0.093031</td>\n      <td>0.107676</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>2.674942</td>\n      <td>2.761820</td>\n      <td>1.000051</td>\n      <td>0.086878</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>119.000381</td>\n      <td>177.970490</td>\n      <td>1.700059</td>\n      <td>-0.272527</td>\n      <td>0.273446</td>\n      <td>0.499447</td>\n      <td>0.381089</td>\n      <td>0.035153</td>\n      <td>0.197282</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>0.068439</td>\n      <td>0.156950</td>\n      <td>0.668652</td>\n      <td>0.088511</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>100.000237</td>\n      <td>62.024445</td>\n      <td>0.899884</td>\n      <td>-0.772531</td>\n      <td>0.293228</td>\n      <td>0.100975</td>\n      <td>0.173560</td>\n      <td>0.062805</td>\n      <td>0.999514</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>35.394611</td>\n      <td>35.280350</td>\n      <td>1.612271</td>\n      <td>0.114262</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>115.996803</td>\n      <td>91.001205</td>\n      <td>0.809019</td>\n      <td>-0.214292</td>\n      <td>0.486074</td>\n      <td>0.305426</td>\n      <td>0.465927</td>\n      <td>0.075435</td>\n      <td>0.799731</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.453975</td>\n      <td>0.336961</td>\n      <td>1.274673</td>\n      <td>0.117014</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>99.000420</td>\n      <td>81.992279</td>\n      <td>1.697242</td>\n      <td>-0.454589</td>\n      <td>0.002021</td>\n      <td>0.099283</td>\n      <td>0.378426</td>\n      <td>0.062373</td>\n      <td>0.476528</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>15.458697</td>\n      <td>15.578036</td>\n      <td>1.207436</td>\n      <td>0.119339</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100.000237</td>\n      <td>64.013428</td>\n      <td>0.520082</td>\n      <td>-0.871746</td>\n      <td>0.435766</td>\n      <td>0.169760</td>\n      <td>0.252438</td>\n      <td>0.041319</td>\n      <td>0.708434</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>34.846703</td>\n      <td>34.719139</td>\n      <td>1.562176</td>\n      <td>0.127563</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>100.000237</td>\n      <td>79.008804</td>\n      <td>1.886015</td>\n      <td>-0.080568</td>\n      <td>0.268567</td>\n      <td>0.037546</td>\n      <td>0.365491</td>\n      <td>0.098157</td>\n      <td>0.492871</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.164833</td>\n      <td>0.029883</td>\n      <td>1.265685</td>\n      <td>0.134950</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>100.000237</td>\n      <td>62.024445</td>\n      <td>0.214663</td>\n      <td>-0.574230</td>\n      <td>0.423929</td>\n      <td>0.022183</td>\n      <td>0.388191</td>\n      <td>0.087424</td>\n      <td>0.768031</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.163287</td>\n      <td>0.008705</td>\n      <td>1.612271</td>\n      <td>0.154582</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>100.000237</td>\n      <td>57.032486</td>\n      <td>0.881358</td>\n      <td>-0.379417</td>\n      <td>0.128283</td>\n      <td>0.063763</td>\n      <td>0.155870</td>\n      <td>0.052979</td>\n      <td>0.476809</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.164068</td>\n      <td>-0.001884</td>\n      <td>1.753391</td>\n      <td>0.165952</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>100.000237</td>\n      <td>92.999939</td>\n      <td>1.198259</td>\n      <td>-0.110510</td>\n      <td>0.057824</td>\n      <td>0.484365</td>\n      <td>0.309789</td>\n      <td>0.062862</td>\n      <td>0.422144</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>7.049657</td>\n      <td>6.880905</td>\n      <td>1.075272</td>\n      <td>0.168753</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>100.000237</td>\n      <td>112.000099</td>\n      <td>1.618351</td>\n      <td>-0.427248</td>\n      <td>0.311758</td>\n      <td>0.265387</td>\n      <td>0.082825</td>\n      <td>0.031588</td>\n      <td>0.560076</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>11.255805</td>\n      <td>11.079407</td>\n      <td>0.892858</td>\n      <td>0.176398</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>100.000237</td>\n      <td>83.006271</td>\n      <td>1.107394</td>\n      <td>-0.298662</td>\n      <td>0.219369</td>\n      <td>0.441657</td>\n      <td>0.458826</td>\n      <td>0.067987</td>\n      <td>1.057843</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.655141</td>\n      <td>0.464028</td>\n      <td>1.204731</td>\n      <td>0.191113</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100.000237</td>\n      <td>90.006714</td>\n      <td>1.781204</td>\n      <td>-0.265676</td>\n      <td>0.151816</td>\n      <td>0.362019</td>\n      <td>0.332520</td>\n      <td>0.010186</td>\n      <td>0.874121</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>5.274084</td>\n      <td>5.499053</td>\n      <td>1.111031</td>\n      <td>0.224969</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>101.000053</td>\n      <td>124.999176</td>\n      <td>0.079422</td>\n      <td>-0.249436</td>\n      <td>0.191908</td>\n      <td>0.210460</td>\n      <td>0.278809</td>\n      <td>0.081613</td>\n      <td>0.361983</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>22.910194</td>\n      <td>22.674259</td>\n      <td>0.808006</td>\n      <td>0.235935</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>110.998192</td>\n      <td>179.998474</td>\n      <td>0.894530</td>\n      <td>-0.789025</td>\n      <td>0.498756</td>\n      <td>0.010624</td>\n      <td>0.216169</td>\n      <td>0.059782</td>\n      <td>0.159805</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>65.287148</td>\n      <td>65.003410</td>\n      <td>0.616662</td>\n      <td>0.283737</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>100.000237</td>\n      <td>80.997787</td>\n      <td>0.331308</td>\n      <td>-0.006729</td>\n      <td>0.322010</td>\n      <td>0.444758</td>\n      <td>0.355917</td>\n      <td>0.016986</td>\n      <td>0.419467</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>20.328989</td>\n      <td>19.984676</td>\n      <td>1.234605</td>\n      <td>0.344313</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>119.000381</td>\n      <td>118.998100</td>\n      <td>0.743793</td>\n      <td>-0.631957</td>\n      <td>0.020763</td>\n      <td>0.349615</td>\n      <td>0.302988</td>\n      <td>0.066769</td>\n      <td>0.478711</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>4.311134</td>\n      <td>4.794890</td>\n      <td>1.000019</td>\n      <td>0.483756</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>100.000237</td>\n      <td>101.999115</td>\n      <td>1.182622</td>\n      <td>-0.249436</td>\n      <td>0.069802</td>\n      <td>0.078282</td>\n      <td>0.147880</td>\n      <td>0.025334</td>\n      <td>0.582037</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>2.661062</td>\n      <td>3.164198</td>\n      <td>0.980403</td>\n      <td>0.503137</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_options.sort_values('Abs Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDJ-hkrSAqVh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MSE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1650895213041,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "GG64Xp3vAib-",
    "outputId": "f846ee65-029d-4a76-b006-d65e4ac07a48",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE on the test set is:  4.8956547474791545e-05\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "    for batch, batch_labels in DataLoader(OptDataset(X_test, y_test), batch_size=batch_size):\n",
    "        out = model(batch.to(device))\n",
    "        loss = loss_fn(out, batch_labels.to(device))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    print('The MSE on the test set is: ', losses.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCLf2KQJAvoE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MAE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1650895213494,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "mZohsEyLAu7S",
    "outputId": "4d6bfba3-5751-478d-d53e-73751702414e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE on the test set is:  0.0052217841098305354\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "mae_loss = nn.L1Loss()\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "    for batch, batch_labels in DataLoader(OptDataset(X_test, y_test), batch_size=batch_size):\n",
    "        out = model(batch.to(device))\n",
    "        loss = mae_loss(out, batch_labels.to(device))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "losses = np.array(losses)\n",
    "print('The MSE on the test set is: ', losses.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmsZ4aPaA1SJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### RSME on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the test set is:  0.006996895559803044\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "    for batch, batch_labels in DataLoader(OptDataset(X_test, y_test), batch_size=batch_size):\n",
    "        out = model(batch.to(device))\n",
    "        loss = loss_fn(out, batch_labels.to(device))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "losses = np.array(losses)\n",
    "print('The RMSE on the test set is: ', np.sqrt(losses.mean()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8Jxp0L4A5zb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MAPE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAPE on the test set is:  0.03515058406378313\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "    for batch, batch_labels in DataLoader(OptDataset(X_test, y_test), batch_size=batch_size):\n",
    "        out = model(batch.to(device))\n",
    "        loss = MAPELoss(out, batch_labels.to(device)).detach().cpu().item()\n",
    "        losses.append(loss)\n",
    "\n",
    "losses = np.array(losses)\n",
    "print('The MAPE on the test set is: ', losses.mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLLLqkX6BEle",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650895213820,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "sf8FmLxxBGcu",
    "outputId": "764313a9-78fc-46ca-dd08-9fb8834e4374",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the R^2 score is:  0.9999494159607865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(X_test[0:batch_size].to(device)).squeeze().cpu().detach().numpy()\n",
    "\n",
    "y_true = y_test[0:batch_size].cpu().squeeze().detach().numpy()\n",
    "\n",
    "r2 = r2_score(y_pred=out, y_true=y_true)\n",
    "\n",
    "print('the R^2 score is: ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1650895214553,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "awvWu7WxBIHB",
    "outputId": "1b3dccd3-90ef-47d5-9895-267fdf93020c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 648x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.scatter(\n",
    "    y=output_sc.inverse_transform(out.reshape(-1, 1)),\n",
    "    x=output_sc.inverse_transform(y_true.squeeze().reshape(-1, 1))\n",
    ")\n",
    "ax.set_xlabel('Actual Value')\n",
    "ax.set_ylabel('Predicted Value')\n",
    "\n",
    "ax.text(20, 80, f'$R^2$ = {np.round(r2, 6)}', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model-Testing-Heston.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}