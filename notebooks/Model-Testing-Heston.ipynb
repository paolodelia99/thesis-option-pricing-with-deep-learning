{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ju-WUaXnY5cr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Models Testing on Heston data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1650891681415,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "hH3-v8y-AuXg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650891681417,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "QXEzFQ3iAyj_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891681418,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "gsH02HCWA0gC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_options_path = '../data/heston_synthetic_options.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1650891681421,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "jT4n95T6A4S6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3413,
     "status": "ok",
     "timestamp": 1650891684820,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "7H8lEQPCA5IS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paolo/PycharmProjects/th-bot/venv/lib/python3.8/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "synthetic_options = pd.read_csv(synthetic_options_path, index_col=0)\n",
    "synthetic_options = reduce_mem_usage(synthetic_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1650891684821,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "9TKRgvuOA8pC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_options = shuffle(synthetic_options, random_state=0)\n",
    "synthetic_options = synthetic_options.reset_index()\n",
    "synthetic_options = synthetic_options.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1650891684822,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "3HXliAsq132p",
    "outputId": "09bc2eb1-d20d-4471-ed50-5fa0eba727fb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         Price  Strike Type     Kappa       Rho     Theta        Xi       V_0  \\\n0           96    78.0    C  1.074219 -0.311279  0.375732  0.180298  0.367432   \n1          100    56.0    C  1.495117 -0.869629  0.011398  0.010925  0.257080   \n2          100    68.0    P  0.988770 -0.664551  0.344482  0.045258  0.328125   \n3          100    75.0    C  1.996094 -0.051880  0.417480  0.208374  0.328613   \n4          100    71.0    P  0.344971 -0.344727  0.227783  0.340820  0.054413   \n...        ...     ...  ...       ...       ...       ...       ...       ...   \n1059740    100    71.0    P  1.039062 -0.757324  0.227661  0.312500  0.445068   \n1059741    100    57.0    C  1.905273 -0.276855  0.322266  0.061432  0.120117   \n1059742     92   132.0    P  0.149536 -0.433350  0.262695  0.054565  0.404541   \n1059743    100   135.0    C  0.971680 -0.711426  0.270508  0.438477  0.483887   \n1059744    100   135.0    P  1.183594 -0.541504  0.479980  0.091614  0.104736   \n\n         Interest Rate  Time to Expiration  Option Price  \n0             0.045074            0.533203     17.828125  \n1             0.050201            0.977051     42.187500  \n2             0.051971            0.588867      0.026703  \n3             0.053741            0.215088     25.312500  \n4             0.016891            0.800781      0.070496  \n...                ...                 ...           ...  \n1059740       0.016006            0.487061      1.505859  \n1059741       0.025070            0.947754     41.687500  \n1059742       0.028809            0.551758     38.375000  \n1059743       0.097107            0.289307      0.180542  \n1059744       0.095215            0.864746     32.093750  \n\n[1059745 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>Strike</th>\n      <th>Type</th>\n      <th>Kappa</th>\n      <th>Rho</th>\n      <th>Theta</th>\n      <th>Xi</th>\n      <th>V_0</th>\n      <th>Interest Rate</th>\n      <th>Time to Expiration</th>\n      <th>Option Price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>96</td>\n      <td>78.0</td>\n      <td>C</td>\n      <td>1.074219</td>\n      <td>-0.311279</td>\n      <td>0.375732</td>\n      <td>0.180298</td>\n      <td>0.367432</td>\n      <td>0.045074</td>\n      <td>0.533203</td>\n      <td>17.828125</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100</td>\n      <td>56.0</td>\n      <td>C</td>\n      <td>1.495117</td>\n      <td>-0.869629</td>\n      <td>0.011398</td>\n      <td>0.010925</td>\n      <td>0.257080</td>\n      <td>0.050201</td>\n      <td>0.977051</td>\n      <td>42.187500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100</td>\n      <td>68.0</td>\n      <td>P</td>\n      <td>0.988770</td>\n      <td>-0.664551</td>\n      <td>0.344482</td>\n      <td>0.045258</td>\n      <td>0.328125</td>\n      <td>0.051971</td>\n      <td>0.588867</td>\n      <td>0.026703</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100</td>\n      <td>75.0</td>\n      <td>C</td>\n      <td>1.996094</td>\n      <td>-0.051880</td>\n      <td>0.417480</td>\n      <td>0.208374</td>\n      <td>0.328613</td>\n      <td>0.053741</td>\n      <td>0.215088</td>\n      <td>25.312500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100</td>\n      <td>71.0</td>\n      <td>P</td>\n      <td>0.344971</td>\n      <td>-0.344727</td>\n      <td>0.227783</td>\n      <td>0.340820</td>\n      <td>0.054413</td>\n      <td>0.016891</td>\n      <td>0.800781</td>\n      <td>0.070496</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1059740</th>\n      <td>100</td>\n      <td>71.0</td>\n      <td>P</td>\n      <td>1.039062</td>\n      <td>-0.757324</td>\n      <td>0.227661</td>\n      <td>0.312500</td>\n      <td>0.445068</td>\n      <td>0.016006</td>\n      <td>0.487061</td>\n      <td>1.505859</td>\n    </tr>\n    <tr>\n      <th>1059741</th>\n      <td>100</td>\n      <td>57.0</td>\n      <td>C</td>\n      <td>1.905273</td>\n      <td>-0.276855</td>\n      <td>0.322266</td>\n      <td>0.061432</td>\n      <td>0.120117</td>\n      <td>0.025070</td>\n      <td>0.947754</td>\n      <td>41.687500</td>\n    </tr>\n    <tr>\n      <th>1059742</th>\n      <td>92</td>\n      <td>132.0</td>\n      <td>P</td>\n      <td>0.149536</td>\n      <td>-0.433350</td>\n      <td>0.262695</td>\n      <td>0.054565</td>\n      <td>0.404541</td>\n      <td>0.028809</td>\n      <td>0.551758</td>\n      <td>38.375000</td>\n    </tr>\n    <tr>\n      <th>1059743</th>\n      <td>100</td>\n      <td>135.0</td>\n      <td>C</td>\n      <td>0.971680</td>\n      <td>-0.711426</td>\n      <td>0.270508</td>\n      <td>0.438477</td>\n      <td>0.483887</td>\n      <td>0.097107</td>\n      <td>0.289307</td>\n      <td>0.180542</td>\n    </tr>\n    <tr>\n      <th>1059744</th>\n      <td>100</td>\n      <td>135.0</td>\n      <td>P</td>\n      <td>1.183594</td>\n      <td>-0.541504</td>\n      <td>0.479980</td>\n      <td>0.091614</td>\n      <td>0.104736</td>\n      <td>0.095215</td>\n      <td>0.864746</td>\n      <td>32.093750</td>\n    </tr>\n  </tbody>\n</table>\n<p>1059745 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pn28_RUMBAFH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1650891685322,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "32oPe6XUBCTF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_options = pd.get_dummies(synthetic_options, prefix='', prefix_sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1650891686346,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "_Tlc7k7xBDPV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_sc = StandardScaler()\n",
    "output_sc = StandardScaler()\n",
    "input_data = input_sc.fit_transform(synthetic_options.drop('Option Price', axis=1))\n",
    "output_data = output_sc.fit_transform(synthetic_options['Option Price'].values.reshape(-1, 1))\n",
    "\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "\n",
    "last_train_idx = int(np.round(len(input_data) * train_size))\n",
    "last_val_idx = last_train_idx + int(np.round(len(input_data) * val_size))\n",
    "\n",
    "X_train = input_data[0:last_train_idx]\n",
    "X_val = input_data[last_train_idx:last_val_idx]\n",
    "X_test = input_data[last_val_idx:]\n",
    "\n",
    "y_train = output_data[0:last_train_idx]\n",
    "y_val = output_data[last_train_idx:last_val_idx]\n",
    "y_test = output_data[last_val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650891686347,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "7nEEoQGvvDpL",
    "outputId": "88b4d863-d037-439b-e625-5ebb52ad41ef",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Price', 'Strike', 'Kappa', 'Rho', 'Theta', 'Xi', 'V_0',\n       'Interest Rate', 'Time to Expiration', 'C', 'P'],\n      dtype='object')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_options.drop('Option Price', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1650891686349,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "IwMVtvfABIhR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = Variable(torch.Tensor(X_train))\n",
    "X_val = Variable(torch.Tensor(X_val))\n",
    "X_test = Variable(torch.Tensor(X_test))\n",
    "\n",
    "y_train = Variable(torch.Tensor(y_train))\n",
    "y_val = Variable(torch.Tensor(y_val))\n",
    "y_test = Variable(torch.Tensor(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ra2l5P1nBVCz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891686350,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "xTLMLFoSBWDy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "device = 'cuda:0' if CUDA else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1650891686352,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "CQ9Gl3bUBWsj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, module):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.module = module\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.module(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650891686353,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "YL3SicQPBYq0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, layer_size, act_fn):\n",
    "      super(HiddenLayer, self).__init__()\n",
    "      \n",
    "      if act_fn == 'ReLU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.ReLU())\n",
    "      elif act_fn == 'LeakyReLU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.LeakyReLU())\n",
    "      elif act_fn == 'ELU':\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(layer_size, layer_size),\n",
    "          nn.ELU())\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1650891686792,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "lHUFGf9xBawS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, output_size, hidden_size, num_layers, act_fn):\n",
    "    super(Net, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.output_size = output_size\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if act_fn == 'ReLU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ReLU())\n",
    "    elif act_fn == 'LeakyReLU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.LeakyReLU())\n",
    "    elif act_fn == 'ELU':\n",
    "      self.initial_layer = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ELU())\n",
    "\n",
    "    self.hidden_layers_list = []\n",
    "\n",
    "    for i in range(num_layers // 2):\n",
    "      self.hidden_layers_list.append(\n",
    "          ResBlock(\n",
    "            nn.Sequential(\n",
    "                HiddenLayer(self.hidden_size, act_fn),\n",
    "                HiddenLayer(self.hidden_size, act_fn)\n",
    "            )\n",
    "        )\n",
    "      )\n",
    "\n",
    "    self.hidden_layers = nn.Sequential(*self.hidden_layers_list)\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        self.initial_layer,\n",
    "        self.hidden_layers,\n",
    "        nn.Linear(self.hidden_size, self.output_size)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650891686793,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "zcq_lQrHBdH8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(m, init_m: str):\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_uniform(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.uniform_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_normal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.normal_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_xuniform(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.xavier_uniform_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_xnormal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      torch.nn.init.xavier_normal_(m.weight)\n",
    "      m.bias.data.fill_(0.01)\n",
    "\n",
    "  if init_m == 'uniform':\n",
    "    m.apply(init_uniform)\n",
    "  elif init_m == 'normal':\n",
    "    m.apply(init_normal)\n",
    "  elif init_m == 'xaiver uniform':\n",
    "    m.apply(init_xuniform)\n",
    "  elif init_m == 'xavier normal':\n",
    "    m.apply(init_xnormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuCpyycNCKEZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650891686794,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "YbCOnCSNCL25",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 11\n",
    "output_size = 1\n",
    "num_layers = 4\n",
    "hidden_size = 800\n",
    "batch_size = 1031\n",
    "epochs = 2000\n",
    "lr = 0.000059\n",
    "init_method = 'xaiver uniform'\n",
    "act_fn = 'LeakyReLU'\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers, act_fn)\n",
    "init_weights(model, init_method)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 7351,
     "status": "ok",
     "timestamp": 1650891694138,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "QqZbxrppvDpZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1650891694142,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "Q-9T0GArCMgp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class OptDataset(Dataset):\n",
    "\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X[idx], self.y[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6-hCH2ivDpb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Losses Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1650891694144,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "mVaO8TruHW4M",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def MAPELoss(output, target):\n",
    "  return torch.mean(torch.abs((target - output) / target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1650891694149,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "tVLW5dHhvDpe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, X_val, y_val):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X_val, y_val), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = loss_fn(out, batch_labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    print('\\nVal set: Average loss: {:.8f}\\n'.format(\n",
    "                losses.mean()))\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68eF5_EovDpf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Early Stopping class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650891694150,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "n2So2ffSvDpg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code took form: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, \n",
    "                 patience=10, \n",
    "                 verbose=False, \n",
    "                 delta=0, \n",
    "                 path='../models/final_heston_model.chkpt',\n",
    "                 trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL_xmnKXvDph",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1650891694152,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "DrBBTGKJvDph",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val\n",
    "):\n",
    "\n",
    "  training_losses = []\n",
    "  validation_losses = []\n",
    "\n",
    "  early_stopping = EarlyStopping(patience=20)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "\n",
    "    for batch, batch_labels in DataLoader(OptDataset(X_train, y_train), batch_size=batch_size):\n",
    "      out = model(batch.to(device))\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss = loss_fn(out, batch_labels.to(device))\n",
    "      epoch_losses.append(loss.item())\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if i > 0 and i % 50 == 0:\n",
    "        avg_loss = total_loss / 50\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| Epoch {:3d} | {:5d}/{:5d} batches | lr {:2.5f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.8f}'.format(\n",
    "              epoch, i, len(X_train) // batch_size+1, lr, elapsed * 1000 / 50,\n",
    "              avg_loss))\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "      i += 1\n",
    "\n",
    "    training_losses.append(np.array(epoch_losses).mean())\n",
    "    val_loss = evaluate(model, loss_fn, X_val, y_val)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Stopping at Epoch: {epoch}\")\n",
    "        break\n",
    "\n",
    "  return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2525796,
     "status": "ok",
     "timestamp": 1650894219916,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "hBETWoCfvDpj",
    "outputId": "30acf99f-3a1e-4d87-d1ff-13068ebe4cd4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 |    50/  823 batches | lr 0.00006 | ms/batch 29.00 | loss 0.37702600\n",
      "| Epoch   0 |   100/  823 batches | lr 0.00006 | ms/batch 21.45 | loss 0.02658004\n",
      "| Epoch   0 |   150/  823 batches | lr 0.00006 | ms/batch 23.69 | loss 0.01414761\n",
      "| Epoch   0 |   200/  823 batches | lr 0.00006 | ms/batch 21.94 | loss 0.00902883\n",
      "| Epoch   0 |   250/  823 batches | lr 0.00006 | ms/batch 25.93 | loss 0.00628480\n",
      "| Epoch   0 |   300/  823 batches | lr 0.00006 | ms/batch 23.28 | loss 0.00446198\n",
      "| Epoch   0 |   350/  823 batches | lr 0.00006 | ms/batch 22.94 | loss 0.00319103\n",
      "| Epoch   0 |   400/  823 batches | lr 0.00006 | ms/batch 22.74 | loss 0.00250326\n",
      "| Epoch   0 |   450/  823 batches | lr 0.00006 | ms/batch 24.78 | loss 0.00211067\n",
      "| Epoch   0 |   500/  823 batches | lr 0.00006 | ms/batch 21.65 | loss 0.00178866\n",
      "| Epoch   0 |   550/  823 batches | lr 0.00006 | ms/batch 24.51 | loss 0.00153596\n",
      "| Epoch   0 |   600/  823 batches | lr 0.00006 | ms/batch 22.14 | loss 0.00140145\n",
      "| Epoch   0 |   650/  823 batches | lr 0.00006 | ms/batch 24.32 | loss 0.00132173\n",
      "| Epoch   0 |   700/  823 batches | lr 0.00006 | ms/batch 22.01 | loss 0.00123144\n",
      "| Epoch   0 |   750/  823 batches | lr 0.00006 | ms/batch 25.79 | loss 0.00113333\n",
      "| Epoch   0 |   800/  823 batches | lr 0.00006 | ms/batch 22.25 | loss 0.00103119\n",
      "\n",
      "Val set: Average loss: 0.00095200\n",
      "\n",
      "| Epoch   1 |    50/  823 batches | lr 0.00006 | ms/batch 21.40 | loss 0.00096018\n",
      "| Epoch   1 |   100/  823 batches | lr 0.00006 | ms/batch 24.65 | loss 0.00091144\n",
      "| Epoch   1 |   150/  823 batches | lr 0.00006 | ms/batch 21.44 | loss 0.00085390\n",
      "| Epoch   1 |   200/  823 batches | lr 0.00006 | ms/batch 22.07 | loss 0.00088516\n",
      "| Epoch   1 |   250/  823 batches | lr 0.00006 | ms/batch 21.14 | loss 0.00080210\n",
      "| Epoch   1 |   300/  823 batches | lr 0.00006 | ms/batch 26.67 | loss 0.00075105\n",
      "| Epoch   1 |   350/  823 batches | lr 0.00006 | ms/batch 22.87 | loss 0.00077009\n",
      "| Epoch   1 |   400/  823 batches | lr 0.00006 | ms/batch 23.34 | loss 0.00077443\n",
      "| Epoch   1 |   450/  823 batches | lr 0.00006 | ms/batch 22.29 | loss 0.00072718\n",
      "| Epoch   1 |   500/  823 batches | lr 0.00006 | ms/batch 23.50 | loss 0.00077588\n",
      "| Epoch   1 |   550/  823 batches | lr 0.00006 | ms/batch 24.94 | loss 0.00070603\n",
      "| Epoch   1 |   600/  823 batches | lr 0.00006 | ms/batch 27.92 | loss 0.00069200\n",
      "| Epoch   1 |   650/  823 batches | lr 0.00006 | ms/batch 22.16 | loss 0.00065512\n",
      "| Epoch   1 |   700/  823 batches | lr 0.00006 | ms/batch 23.05 | loss 0.00064115\n",
      "| Epoch   1 |   750/  823 batches | lr 0.00006 | ms/batch 21.87 | loss 0.00061186\n",
      "| Epoch   1 |   800/  823 batches | lr 0.00006 | ms/batch 23.91 | loss 0.00060517\n",
      "\n",
      "Val set: Average loss: 0.00055279\n",
      "\n",
      "| Epoch   2 |    50/  823 batches | lr 0.00006 | ms/batch 20.66 | loss 0.00068398\n",
      "| Epoch   2 |   100/  823 batches | lr 0.00006 | ms/batch 21.87 | loss 0.00056768\n",
      "| Epoch   2 |   150/  823 batches | lr 0.00006 | ms/batch 20.10 | loss 0.00063140\n",
      "| Epoch   2 |   200/  823 batches | lr 0.00006 | ms/batch 22.67 | loss 0.00059786\n",
      "| Epoch   2 |   250/  823 batches | lr 0.00006 | ms/batch 20.90 | loss 0.00054028\n",
      "| Epoch   2 |   300/  823 batches | lr 0.00006 | ms/batch 22.63 | loss 0.00050952\n",
      "| Epoch   2 |   350/  823 batches | lr 0.00006 | ms/batch 20.82 | loss 0.00055832\n",
      "| Epoch   2 |   400/  823 batches | lr 0.00006 | ms/batch 22.04 | loss 0.00058298\n",
      "| Epoch   2 |   450/  823 batches | lr 0.00006 | ms/batch 20.37 | loss 0.00050657\n",
      "| Epoch   2 |   500/  823 batches | lr 0.00006 | ms/batch 21.93 | loss 0.00049898\n",
      "| Epoch   2 |   550/  823 batches | lr 0.00006 | ms/batch 20.48 | loss 0.00052826\n",
      "| Epoch   2 |   600/  823 batches | lr 0.00006 | ms/batch 22.70 | loss 0.00056866\n",
      "| Epoch   2 |   650/  823 batches | lr 0.00006 | ms/batch 22.67 | loss 0.00049670\n",
      "| Epoch   2 |   700/  823 batches | lr 0.00006 | ms/batch 22.88 | loss 0.00052753\n",
      "| Epoch   2 |   750/  823 batches | lr 0.00006 | ms/batch 20.89 | loss 0.00045148\n",
      "| Epoch   2 |   800/  823 batches | lr 0.00006 | ms/batch 22.70 | loss 0.00048035\n",
      "\n",
      "Val set: Average loss: 0.00043945\n",
      "\n",
      "| Epoch   3 |    50/  823 batches | lr 0.00006 | ms/batch 20.08 | loss 0.00054293\n",
      "| Epoch   3 |   100/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00044362\n",
      "| Epoch   3 |   150/  823 batches | lr 0.00006 | ms/batch 20.07 | loss 0.00052404\n",
      "| Epoch   3 |   200/  823 batches | lr 0.00006 | ms/batch 21.46 | loss 0.00048762\n",
      "| Epoch   3 |   250/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00044402\n",
      "| Epoch   3 |   300/  823 batches | lr 0.00006 | ms/batch 21.67 | loss 0.00040025\n",
      "| Epoch   3 |   350/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00047179\n",
      "| Epoch   3 |   400/  823 batches | lr 0.00006 | ms/batch 21.57 | loss 0.00050796\n",
      "| Epoch   3 |   450/  823 batches | lr 0.00006 | ms/batch 19.85 | loss 0.00039900\n",
      "| Epoch   3 |   500/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00039649\n",
      "| Epoch   3 |   550/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00044613\n",
      "| Epoch   3 |   600/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00047807\n",
      "| Epoch   3 |   650/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00042777\n",
      "| Epoch   3 |   700/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00044166\n",
      "| Epoch   3 |   750/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00036108\n",
      "| Epoch   3 |   800/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00039168\n",
      "\n",
      "Val set: Average loss: 0.00035527\n",
      "\n",
      "| Epoch   4 |    50/  823 batches | lr 0.00006 | ms/batch 19.91 | loss 0.00048507\n",
      "| Epoch   4 |   100/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00039688\n",
      "| Epoch   4 |   150/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00042210\n",
      "| Epoch   4 |   200/  823 batches | lr 0.00006 | ms/batch 21.05 | loss 0.00043688\n",
      "| Epoch   4 |   250/  823 batches | lr 0.00006 | ms/batch 19.89 | loss 0.00039155\n",
      "| Epoch   4 |   300/  823 batches | lr 0.00006 | ms/batch 21.70 | loss 0.00033398\n",
      "| Epoch   4 |   350/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00035834\n",
      "| Epoch   4 |   400/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00045937\n",
      "| Epoch   4 |   450/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00036008\n",
      "| Epoch   4 |   500/  823 batches | lr 0.00006 | ms/batch 21.50 | loss 0.00034173\n",
      "| Epoch   4 |   550/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00036268\n",
      "| Epoch   4 |   600/  823 batches | lr 0.00006 | ms/batch 21.36 | loss 0.00045055\n",
      "| Epoch   4 |   650/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00036773\n",
      "| Epoch   4 |   700/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00035822\n",
      "| Epoch   4 |   750/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00030848\n",
      "| Epoch   4 |   800/  823 batches | lr 0.00006 | ms/batch 21.42 | loss 0.00033045\n",
      "\n",
      "Val set: Average loss: 0.00031284\n",
      "\n",
      "| Epoch   5 |    50/  823 batches | lr 0.00006 | ms/batch 20.03 | loss 0.00040354\n",
      "| Epoch   5 |   100/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00034379\n",
      "| Epoch   5 |   150/  823 batches | lr 0.00006 | ms/batch 19.85 | loss 0.00039475\n",
      "| Epoch   5 |   200/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00038300\n",
      "| Epoch   5 |   250/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00033288\n",
      "| Epoch   5 |   300/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00028970\n",
      "| Epoch   5 |   350/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00028725\n",
      "| Epoch   5 |   400/  823 batches | lr 0.00006 | ms/batch 21.47 | loss 0.00037151\n",
      "| Epoch   5 |   450/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00033790\n",
      "| Epoch   5 |   500/  823 batches | lr 0.00006 | ms/batch 21.35 | loss 0.00035028\n",
      "| Epoch   5 |   550/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00031717\n",
      "| Epoch   5 |   600/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00043483\n",
      "| Epoch   5 |   650/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00034190\n",
      "| Epoch   5 |   700/  823 batches | lr 0.00006 | ms/batch 21.69 | loss 0.00030080\n",
      "| Epoch   5 |   750/  823 batches | lr 0.00006 | ms/batch 20.07 | loss 0.00026628\n",
      "| Epoch   5 |   800/  823 batches | lr 0.00006 | ms/batch 21.38 | loss 0.00028580\n",
      "\n",
      "Val set: Average loss: 0.00027440\n",
      "\n",
      "| Epoch   6 |    50/  823 batches | lr 0.00006 | ms/batch 19.97 | loss 0.00037261\n",
      "| Epoch   6 |   100/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00035782\n",
      "| Epoch   6 |   150/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00030384\n",
      "| Epoch   6 |   200/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00033666\n",
      "| Epoch   6 |   250/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00028433\n",
      "| Epoch   6 |   300/  823 batches | lr 0.00006 | ms/batch 21.52 | loss 0.00025600\n",
      "| Epoch   6 |   350/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00024994\n",
      "| Epoch   6 |   400/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00031194\n",
      "| Epoch   6 |   450/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00034669\n",
      "| Epoch   6 |   500/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00031549\n",
      "| Epoch   6 |   550/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00028271\n",
      "| Epoch   6 |   600/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00041135\n",
      "| Epoch   6 |   650/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00033019\n",
      "| Epoch   6 |   700/  823 batches | lr 0.00006 | ms/batch 22.41 | loss 0.00025414\n",
      "| Epoch   6 |   750/  823 batches | lr 0.00006 | ms/batch 20.75 | loss 0.00023035\n",
      "| Epoch   6 |   800/  823 batches | lr 0.00006 | ms/batch 23.01 | loss 0.00025248\n",
      "\n",
      "Val set: Average loss: 0.00024772\n",
      "\n",
      "| Epoch   7 |    50/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00037340\n",
      "| Epoch   7 |   100/  823 batches | lr 0.00006 | ms/batch 19.92 | loss 0.00037526\n",
      "| Epoch   7 |   150/  823 batches | lr 0.00006 | ms/batch 20.24 | loss 0.00031201\n",
      "| Epoch   7 |   200/  823 batches | lr 0.00006 | ms/batch 22.24 | loss 0.00027340\n",
      "| Epoch   7 |   250/  823 batches | lr 0.00006 | ms/batch 20.40 | loss 0.00026482\n",
      "| Epoch   7 |   300/  823 batches | lr 0.00006 | ms/batch 22.27 | loss 0.00025384\n",
      "| Epoch   7 |   350/  823 batches | lr 0.00006 | ms/batch 20.45 | loss 0.00023189\n",
      "| Epoch   7 |   400/  823 batches | lr 0.00006 | ms/batch 21.32 | loss 0.00036565\n",
      "| Epoch   7 |   450/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00024294\n",
      "| Epoch   7 |   500/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00026278\n",
      "| Epoch   7 |   550/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00024071\n",
      "| Epoch   7 |   600/  823 batches | lr 0.00006 | ms/batch 21.40 | loss 0.00037257\n",
      "| Epoch   7 |   650/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00041758\n",
      "| Epoch   7 |   700/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00022091\n",
      "| Epoch   7 |   750/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00020654\n",
      "| Epoch   7 |   800/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00023430\n",
      "\n",
      "Val set: Average loss: 0.00022104\n",
      "\n",
      "| Epoch   8 |    50/  823 batches | lr 0.00006 | ms/batch 20.46 | loss 0.00029179\n",
      "| Epoch   8 |   100/  823 batches | lr 0.00006 | ms/batch 19.52 | loss 0.00027489\n",
      "| Epoch   8 |   150/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00028235\n",
      "| Epoch   8 |   200/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00023286\n",
      "| Epoch   8 |   250/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00021354\n",
      "| Epoch   8 |   300/  823 batches | lr 0.00006 | ms/batch 21.06 | loss 0.00021501\n",
      "| Epoch   8 |   350/  823 batches | lr 0.00006 | ms/batch 19.54 | loss 0.00020297\n",
      "| Epoch   8 |   400/  823 batches | lr 0.00006 | ms/batch 21.12 | loss 0.00029891\n",
      "| Epoch   8 |   450/  823 batches | lr 0.00006 | ms/batch 19.57 | loss 0.00022094\n",
      "| Epoch   8 |   500/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00025156\n",
      "| Epoch   8 |   550/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00023206\n",
      "| Epoch   8 |   600/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00023600\n",
      "| Epoch   8 |   650/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00022107\n",
      "| Epoch   8 |   700/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00026138\n",
      "| Epoch   8 |   750/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00023309\n",
      "| Epoch   8 |   800/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00022468\n",
      "\n",
      "Val set: Average loss: 0.00030552\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch   9 |    50/  823 batches | lr 0.00006 | ms/batch 19.94 | loss 0.00030261\n",
      "| Epoch   9 |   100/  823 batches | lr 0.00006 | ms/batch 19.57 | loss 0.00027092\n",
      "| Epoch   9 |   150/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00027167\n",
      "| Epoch   9 |   200/  823 batches | lr 0.00006 | ms/batch 21.18 | loss 0.00021730\n",
      "| Epoch   9 |   250/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00018104\n",
      "| Epoch   9 |   300/  823 batches | lr 0.00006 | ms/batch 21.49 | loss 0.00019931\n",
      "| Epoch   9 |   350/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00019608\n",
      "| Epoch   9 |   400/  823 batches | lr 0.00006 | ms/batch 21.11 | loss 0.00024544\n",
      "| Epoch   9 |   450/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00023547\n",
      "| Epoch   9 |   500/  823 batches | lr 0.00006 | ms/batch 23.18 | loss 0.00022097\n",
      "| Epoch   9 |   550/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00023095\n",
      "| Epoch   9 |   600/  823 batches | lr 0.00006 | ms/batch 21.52 | loss 0.00025269\n",
      "| Epoch   9 |   650/  823 batches | lr 0.00006 | ms/batch 19.54 | loss 0.00018192\n",
      "| Epoch   9 |   700/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00023954\n",
      "| Epoch   9 |   750/  823 batches | lr 0.00006 | ms/batch 19.57 | loss 0.00023771\n",
      "| Epoch   9 |   800/  823 batches | lr 0.00006 | ms/batch 21.60 | loss 0.00030080\n",
      "\n",
      "Val set: Average loss: 0.00019213\n",
      "\n",
      "| Epoch  10 |    50/  823 batches | lr 0.00006 | ms/batch 20.17 | loss 0.00024662\n",
      "| Epoch  10 |   100/  823 batches | lr 0.00006 | ms/batch 20.51 | loss 0.00021219\n",
      "| Epoch  10 |   150/  823 batches | lr 0.00006 | ms/batch 19.50 | loss 0.00022643\n",
      "| Epoch  10 |   200/  823 batches | lr 0.00006 | ms/batch 21.41 | loss 0.00026255\n",
      "| Epoch  10 |   250/  823 batches | lr 0.00006 | ms/batch 19.96 | loss 0.00016534\n",
      "| Epoch  10 |   300/  823 batches | lr 0.00006 | ms/batch 21.09 | loss 0.00019131\n",
      "| Epoch  10 |   350/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00018177\n",
      "| Epoch  10 |   400/  823 batches | lr 0.00006 | ms/batch 21.59 | loss 0.00023053\n",
      "| Epoch  10 |   450/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00024513\n",
      "| Epoch  10 |   500/  823 batches | lr 0.00006 | ms/batch 21.55 | loss 0.00023472\n",
      "| Epoch  10 |   550/  823 batches | lr 0.00006 | ms/batch 19.50 | loss 0.00023022\n",
      "| Epoch  10 |   600/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00019752\n",
      "| Epoch  10 |   650/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00016111\n",
      "| Epoch  10 |   700/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00023414\n",
      "| Epoch  10 |   750/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00026322\n",
      "| Epoch  10 |   800/  823 batches | lr 0.00006 | ms/batch 21.43 | loss 0.00019359\n",
      "\n",
      "Val set: Average loss: 0.00018502\n",
      "\n",
      "| Epoch  11 |    50/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00019191\n",
      "| Epoch  11 |   100/  823 batches | lr 0.00006 | ms/batch 19.49 | loss 0.00015986\n",
      "| Epoch  11 |   150/  823 batches | lr 0.00006 | ms/batch 19.80 | loss 0.00023540\n",
      "| Epoch  11 |   200/  823 batches | lr 0.00006 | ms/batch 21.58 | loss 0.00019330\n",
      "| Epoch  11 |   250/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00020479\n",
      "| Epoch  11 |   300/  823 batches | lr 0.00006 | ms/batch 21.52 | loss 0.00020579\n",
      "| Epoch  11 |   350/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00020347\n",
      "| Epoch  11 |   400/  823 batches | lr 0.00006 | ms/batch 21.37 | loss 0.00025925\n",
      "| Epoch  11 |   450/  823 batches | lr 0.00006 | ms/batch 19.92 | loss 0.00021953\n",
      "| Epoch  11 |   500/  823 batches | lr 0.00006 | ms/batch 21.64 | loss 0.00022881\n",
      "| Epoch  11 |   550/  823 batches | lr 0.00006 | ms/batch 19.91 | loss 0.00028052\n",
      "| Epoch  11 |   600/  823 batches | lr 0.00006 | ms/batch 21.51 | loss 0.00028222\n",
      "| Epoch  11 |   650/  823 batches | lr 0.00006 | ms/batch 20.08 | loss 0.00032133\n",
      "| Epoch  11 |   700/  823 batches | lr 0.00006 | ms/batch 21.78 | loss 0.00015285\n",
      "| Epoch  11 |   750/  823 batches | lr 0.00006 | ms/batch 20.08 | loss 0.00014714\n",
      "| Epoch  11 |   800/  823 batches | lr 0.00006 | ms/batch 21.55 | loss 0.00018490\n",
      "\n",
      "Val set: Average loss: 0.00021343\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  12 |    50/  823 batches | lr 0.00006 | ms/batch 20.02 | loss 0.00021000\n",
      "| Epoch  12 |   100/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00018020\n",
      "| Epoch  12 |   150/  823 batches | lr 0.00006 | ms/batch 19.82 | loss 0.00023721\n",
      "| Epoch  12 |   200/  823 batches | lr 0.00006 | ms/batch 22.16 | loss 0.00018823\n",
      "| Epoch  12 |   250/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00014728\n",
      "| Epoch  12 |   300/  823 batches | lr 0.00006 | ms/batch 21.75 | loss 0.00019304\n",
      "| Epoch  12 |   350/  823 batches | lr 0.00006 | ms/batch 19.99 | loss 0.00016372\n",
      "| Epoch  12 |   400/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00033676\n",
      "| Epoch  12 |   450/  823 batches | lr 0.00006 | ms/batch 19.95 | loss 0.00017845\n",
      "| Epoch  12 |   500/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00024105\n",
      "| Epoch  12 |   550/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00017346\n",
      "| Epoch  12 |   600/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00024269\n",
      "| Epoch  12 |   650/  823 batches | lr 0.00006 | ms/batch 19.97 | loss 0.00022883\n",
      "| Epoch  12 |   700/  823 batches | lr 0.00006 | ms/batch 21.42 | loss 0.00014516\n",
      "| Epoch  12 |   750/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00013021\n",
      "| Epoch  12 |   800/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00015920\n",
      "\n",
      "Val set: Average loss: 0.00014077\n",
      "\n",
      "| Epoch  13 |    50/  823 batches | lr 0.00006 | ms/batch 19.89 | loss 0.00018361\n",
      "| Epoch  13 |   100/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00017168\n",
      "| Epoch  13 |   150/  823 batches | lr 0.00006 | ms/batch 19.48 | loss 0.00019818\n",
      "| Epoch  13 |   200/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00016817\n",
      "| Epoch  13 |   250/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00012601\n",
      "| Epoch  13 |   300/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00020032\n",
      "| Epoch  13 |   350/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00013674\n",
      "| Epoch  13 |   400/  823 batches | lr 0.00006 | ms/batch 21.41 | loss 0.00019809\n",
      "| Epoch  13 |   450/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00019415\n",
      "| Epoch  13 |   500/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00014436\n",
      "| Epoch  13 |   550/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00019754\n",
      "| Epoch  13 |   600/  823 batches | lr 0.00006 | ms/batch 21.35 | loss 0.00018013\n",
      "| Epoch  13 |   650/  823 batches | lr 0.00006 | ms/batch 19.82 | loss 0.00014003\n",
      "| Epoch  13 |   700/  823 batches | lr 0.00006 | ms/batch 21.15 | loss 0.00024624\n",
      "| Epoch  13 |   750/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00015869\n",
      "| Epoch  13 |   800/  823 batches | lr 0.00006 | ms/batch 21.43 | loss 0.00021974\n",
      "\n",
      "Val set: Average loss: 0.00015851\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  14 |    50/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00015002\n",
      "| Epoch  14 |   100/  823 batches | lr 0.00006 | ms/batch 19.44 | loss 0.00013055\n",
      "| Epoch  14 |   150/  823 batches | lr 0.00006 | ms/batch 19.52 | loss 0.00020852\n",
      "| Epoch  14 |   200/  823 batches | lr 0.00006 | ms/batch 21.56 | loss 0.00015222\n",
      "| Epoch  14 |   250/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00016306\n",
      "| Epoch  14 |   300/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00019565\n",
      "| Epoch  14 |   350/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00013287\n",
      "| Epoch  14 |   400/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00022782\n",
      "| Epoch  14 |   450/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00023423\n",
      "| Epoch  14 |   500/  823 batches | lr 0.00006 | ms/batch 21.71 | loss 0.00013106\n",
      "| Epoch  14 |   550/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00018020\n",
      "| Epoch  14 |   600/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00015686\n",
      "| Epoch  14 |   650/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00012997\n",
      "| Epoch  14 |   700/  823 batches | lr 0.00006 | ms/batch 21.54 | loss 0.00016512\n",
      "| Epoch  14 |   750/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00014511\n",
      "| Epoch  14 |   800/  823 batches | lr 0.00006 | ms/batch 21.39 | loss 0.00020462\n",
      "\n",
      "Val set: Average loss: 0.00016204\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  15 |    50/  823 batches | lr 0.00006 | ms/batch 19.99 | loss 0.00016726\n",
      "| Epoch  15 |   100/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00012356\n",
      "| Epoch  15 |   150/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00020870\n",
      "| Epoch  15 |   200/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00023387\n",
      "| Epoch  15 |   250/  823 batches | lr 0.00006 | ms/batch 19.52 | loss 0.00013111\n",
      "| Epoch  15 |   300/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00015421\n",
      "| Epoch  15 |   350/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00015529\n",
      "| Epoch  15 |   400/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00029595\n",
      "| Epoch  15 |   450/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00014843\n",
      "| Epoch  15 |   500/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00014651\n",
      "| Epoch  15 |   550/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00020276\n",
      "| Epoch  15 |   600/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00017171\n",
      "| Epoch  15 |   650/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00014827\n",
      "| Epoch  15 |   700/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00016038\n",
      "| Epoch  15 |   750/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00012319\n",
      "| Epoch  15 |   800/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00016552\n",
      "\n",
      "Val set: Average loss: 0.00017922\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  16 |    50/  823 batches | lr 0.00006 | ms/batch 19.93 | loss 0.00016611\n",
      "| Epoch  16 |   100/  823 batches | lr 0.00006 | ms/batch 19.85 | loss 0.00017759\n",
      "| Epoch  16 |   150/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00016723\n",
      "| Epoch  16 |   200/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00016706\n",
      "| Epoch  16 |   250/  823 batches | lr 0.00006 | ms/batch 19.85 | loss 0.00012482\n",
      "| Epoch  16 |   300/  823 batches | lr 0.00006 | ms/batch 21.18 | loss 0.00012683\n",
      "| Epoch  16 |   350/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00012253\n",
      "| Epoch  16 |   400/  823 batches | lr 0.00006 | ms/batch 21.45 | loss 0.00017247\n",
      "| Epoch  16 |   450/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00018767\n",
      "| Epoch  16 |   500/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00012044\n",
      "| Epoch  16 |   550/  823 batches | lr 0.00006 | ms/batch 19.79 | loss 0.00017063\n",
      "| Epoch  16 |   600/  823 batches | lr 0.00006 | ms/batch 21.39 | loss 0.00016487\n",
      "| Epoch  16 |   650/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00013376\n",
      "| Epoch  16 |   700/  823 batches | lr 0.00006 | ms/batch 21.35 | loss 0.00012158\n",
      "| Epoch  16 |   750/  823 batches | lr 0.00006 | ms/batch 19.84 | loss 0.00018076\n",
      "| Epoch  16 |   800/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00011964\n",
      "\n",
      "Val set: Average loss: 0.00021153\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  17 |    50/  823 batches | lr 0.00006 | ms/batch 19.90 | loss 0.00018794\n",
      "| Epoch  17 |   100/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00017020\n",
      "| Epoch  17 |   150/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00015684\n",
      "| Epoch  17 |   200/  823 batches | lr 0.00006 | ms/batch 21.18 | loss 0.00014118\n",
      "| Epoch  17 |   250/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00011841\n",
      "| Epoch  17 |   300/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00011907\n",
      "| Epoch  17 |   350/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00012807\n",
      "| Epoch  17 |   400/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00017216\n",
      "| Epoch  17 |   450/  823 batches | lr 0.00006 | ms/batch 19.78 | loss 0.00019507\n",
      "| Epoch  17 |   500/  823 batches | lr 0.00006 | ms/batch 21.80 | loss 0.00013543\n",
      "| Epoch  17 |   550/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00015373\n",
      "| Epoch  17 |   600/  823 batches | lr 0.00006 | ms/batch 21.37 | loss 0.00013964\n",
      "| Epoch  17 |   650/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00011387\n",
      "| Epoch  17 |   700/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00012183\n",
      "| Epoch  17 |   750/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00028330\n",
      "| Epoch  17 |   800/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00013211\n",
      "\n",
      "Val set: Average loss: 0.00011465\n",
      "\n",
      "| Epoch  18 |    50/  823 batches | lr 0.00006 | ms/batch 19.97 | loss 0.00019789\n",
      "| Epoch  18 |   100/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00010925\n",
      "| Epoch  18 |   150/  823 batches | lr 0.00006 | ms/batch 19.78 | loss 0.00012815\n",
      "| Epoch  18 |   200/  823 batches | lr 0.00006 | ms/batch 21.38 | loss 0.00010509\n",
      "| Epoch  18 |   250/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00010018\n",
      "| Epoch  18 |   300/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00023718\n",
      "| Epoch  18 |   350/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00016821\n",
      "| Epoch  18 |   400/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00026888\n",
      "| Epoch  18 |   450/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00015287\n",
      "| Epoch  18 |   500/  823 batches | lr 0.00006 | ms/batch 21.38 | loss 0.00020019\n",
      "| Epoch  18 |   550/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00012505\n",
      "| Epoch  18 |   600/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00017412\n",
      "| Epoch  18 |   650/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00011575\n",
      "| Epoch  18 |   700/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00014642\n",
      "| Epoch  18 |   750/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00011145\n",
      "| Epoch  18 |   800/  823 batches | lr 0.00006 | ms/batch 21.18 | loss 0.00014653\n",
      "\n",
      "Val set: Average loss: 0.00024962\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  19 |    50/  823 batches | lr 0.00006 | ms/batch 19.91 | loss 0.00023708\n",
      "| Epoch  19 |   100/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00011803\n",
      "| Epoch  19 |   150/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00016028\n",
      "| Epoch  19 |   200/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00015347\n",
      "| Epoch  19 |   250/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00011719\n",
      "| Epoch  19 |   300/  823 batches | lr 0.00006 | ms/batch 21.18 | loss 0.00013491\n",
      "| Epoch  19 |   350/  823 batches | lr 0.00006 | ms/batch 19.93 | loss 0.00012550\n",
      "| Epoch  19 |   400/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00016810\n",
      "| Epoch  19 |   450/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00012056\n",
      "| Epoch  19 |   500/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00011119\n",
      "| Epoch  19 |   550/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00015329\n",
      "| Epoch  19 |   600/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00012211\n",
      "| Epoch  19 |   650/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00009920\n",
      "| Epoch  19 |   700/  823 batches | lr 0.00006 | ms/batch 21.36 | loss 0.00011279\n",
      "| Epoch  19 |   750/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00015692\n",
      "| Epoch  19 |   800/  823 batches | lr 0.00006 | ms/batch 21.12 | loss 0.00016089\n",
      "\n",
      "Val set: Average loss: 0.00012410\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  20 |    50/  823 batches | lr 0.00006 | ms/batch 19.92 | loss 0.00010403\n",
      "| Epoch  20 |   100/  823 batches | lr 0.00006 | ms/batch 19.42 | loss 0.00010181\n",
      "| Epoch  20 |   150/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00014232\n",
      "| Epoch  20 |   200/  823 batches | lr 0.00006 | ms/batch 21.09 | loss 0.00011024\n",
      "| Epoch  20 |   250/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00014494\n",
      "| Epoch  20 |   300/  823 batches | lr 0.00006 | ms/batch 21.16 | loss 0.00019124\n",
      "| Epoch  20 |   350/  823 batches | lr 0.00006 | ms/batch 19.57 | loss 0.00017660\n",
      "| Epoch  20 |   400/  823 batches | lr 0.00006 | ms/batch 21.16 | loss 0.00014184\n",
      "| Epoch  20 |   450/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00008882\n",
      "| Epoch  20 |   500/  823 batches | lr 0.00006 | ms/batch 21.38 | loss 0.00010169\n",
      "| Epoch  20 |   550/  823 batches | lr 0.00006 | ms/batch 19.50 | loss 0.00012344\n",
      "| Epoch  20 |   600/  823 batches | lr 0.00006 | ms/batch 21.39 | loss 0.00017060\n",
      "| Epoch  20 |   650/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00021572\n",
      "| Epoch  20 |   700/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00015744\n",
      "| Epoch  20 |   750/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00010455\n",
      "| Epoch  20 |   800/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00013530\n",
      "\n",
      "Val set: Average loss: 0.00010379\n",
      "\n",
      "| Epoch  21 |    50/  823 batches | lr 0.00006 | ms/batch 20.02 | loss 0.00013252\n",
      "| Epoch  21 |   100/  823 batches | lr 0.00006 | ms/batch 19.50 | loss 0.00012853\n",
      "| Epoch  21 |   150/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00013632\n",
      "| Epoch  21 |   200/  823 batches | lr 0.00006 | ms/batch 21.35 | loss 0.00011866\n",
      "| Epoch  21 |   250/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00011086\n",
      "| Epoch  21 |   300/  823 batches | lr 0.00006 | ms/batch 21.14 | loss 0.00014163\n",
      "| Epoch  21 |   350/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00010911\n",
      "| Epoch  21 |   400/  823 batches | lr 0.00006 | ms/batch 21.18 | loss 0.00010611\n",
      "| Epoch  21 |   450/  823 batches | lr 0.00006 | ms/batch 19.53 | loss 0.00010692\n",
      "| Epoch  21 |   500/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00013865\n",
      "| Epoch  21 |   550/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00013325\n",
      "| Epoch  21 |   600/  823 batches | lr 0.00006 | ms/batch 21.32 | loss 0.00014563\n",
      "| Epoch  21 |   650/  823 batches | lr 0.00006 | ms/batch 19.51 | loss 0.00011896\n",
      "| Epoch  21 |   700/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00012351\n",
      "| Epoch  21 |   750/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00013883\n",
      "| Epoch  21 |   800/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00009732\n",
      "\n",
      "Val set: Average loss: 0.00010611\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  22 |    50/  823 batches | lr 0.00006 | ms/batch 19.93 | loss 0.00011783\n",
      "| Epoch  22 |   100/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00009794\n",
      "| Epoch  22 |   150/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00010512\n",
      "| Epoch  22 |   200/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00018552\n",
      "| Epoch  22 |   250/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00009798\n",
      "| Epoch  22 |   300/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00014579\n",
      "| Epoch  22 |   350/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00009504\n",
      "| Epoch  22 |   400/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00013696\n",
      "| Epoch  22 |   450/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00011939\n",
      "| Epoch  22 |   500/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00015203\n",
      "| Epoch  22 |   550/  823 batches | lr 0.00006 | ms/batch 19.53 | loss 0.00010734\n",
      "| Epoch  22 |   600/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00015697\n",
      "| Epoch  22 |   650/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00019254\n",
      "| Epoch  22 |   700/  823 batches | lr 0.00006 | ms/batch 21.11 | loss 0.00017113\n",
      "| Epoch  22 |   750/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00010192\n",
      "| Epoch  22 |   800/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00014023\n",
      "\n",
      "Val set: Average loss: 0.00028890\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  23 |    50/  823 batches | lr 0.00006 | ms/batch 19.89 | loss 0.00020341\n",
      "| Epoch  23 |   100/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00010716\n",
      "| Epoch  23 |   150/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00012353\n",
      "| Epoch  23 |   200/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00011534\n",
      "| Epoch  23 |   250/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00009612\n",
      "| Epoch  23 |   300/  823 batches | lr 0.00006 | ms/batch 21.18 | loss 0.00015430\n",
      "| Epoch  23 |   350/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00012225\n",
      "| Epoch  23 |   400/  823 batches | lr 0.00006 | ms/batch 21.35 | loss 0.00011405\n",
      "| Epoch  23 |   450/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00008686\n",
      "| Epoch  23 |   500/  823 batches | lr 0.00006 | ms/batch 21.12 | loss 0.00011181\n",
      "| Epoch  23 |   550/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00017504\n",
      "| Epoch  23 |   600/  823 batches | lr 0.00006 | ms/batch 21.13 | loss 0.00012718\n",
      "| Epoch  23 |   650/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00016028\n",
      "| Epoch  23 |   700/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00018832\n",
      "| Epoch  23 |   750/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00009104\n",
      "| Epoch  23 |   800/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00010829\n",
      "\n",
      "Val set: Average loss: 0.00019621\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  24 |    50/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00015449\n",
      "| Epoch  24 |   100/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00010298\n",
      "| Epoch  24 |   150/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00011035\n",
      "| Epoch  24 |   200/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00010259\n",
      "| Epoch  24 |   250/  823 batches | lr 0.00006 | ms/batch 19.50 | loss 0.00008502\n",
      "| Epoch  24 |   300/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00013757\n",
      "| Epoch  24 |   350/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00011356\n",
      "| Epoch  24 |   400/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00010209\n",
      "| Epoch  24 |   450/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00008314\n",
      "| Epoch  24 |   500/  823 batches | lr 0.00006 | ms/batch 21.14 | loss 0.00008952\n",
      "| Epoch  24 |   550/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00010618\n",
      "| Epoch  24 |   600/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00013778\n",
      "| Epoch  24 |   650/  823 batches | lr 0.00006 | ms/batch 19.46 | loss 0.00010367\n",
      "| Epoch  24 |   700/  823 batches | lr 0.00006 | ms/batch 21.10 | loss 0.00010358\n",
      "| Epoch  24 |   750/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00014648\n",
      "| Epoch  24 |   800/  823 batches | lr 0.00006 | ms/batch 21.11 | loss 0.00009110\n",
      "\n",
      "Val set: Average loss: 0.00013047\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  25 |    50/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00010204\n",
      "| Epoch  25 |   100/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00014696\n",
      "| Epoch  25 |   150/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00011294\n",
      "| Epoch  25 |   200/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00007714\n",
      "| Epoch  25 |   250/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00030999\n",
      "| Epoch  25 |   300/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00016626\n",
      "| Epoch  25 |   350/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00014871\n",
      "| Epoch  25 |   400/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00013381\n",
      "| Epoch  25 |   450/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00008632\n",
      "| Epoch  25 |   500/  823 batches | lr 0.00006 | ms/batch 21.11 | loss 0.00011973\n",
      "| Epoch  25 |   550/  823 batches | lr 0.00006 | ms/batch 19.45 | loss 0.00008865\n",
      "| Epoch  25 |   600/  823 batches | lr 0.00006 | ms/batch 21.07 | loss 0.00013394\n",
      "| Epoch  25 |   650/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00012309\n",
      "| Epoch  25 |   700/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00011883\n",
      "| Epoch  25 |   750/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00007813\n",
      "| Epoch  25 |   800/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00010312\n",
      "\n",
      "Val set: Average loss: 0.00008678\n",
      "\n",
      "| Epoch  26 |    50/  823 batches | lr 0.00006 | ms/batch 19.85 | loss 0.00007949\n",
      "| Epoch  26 |   100/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00011305\n",
      "| Epoch  26 |   150/  823 batches | lr 0.00006 | ms/batch 19.80 | loss 0.00010386\n",
      "| Epoch  26 |   200/  823 batches | lr 0.00006 | ms/batch 21.40 | loss 0.00009920\n",
      "| Epoch  26 |   250/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00013040\n",
      "| Epoch  26 |   300/  823 batches | lr 0.00006 | ms/batch 21.07 | loss 0.00010873\n",
      "| Epoch  26 |   350/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00008475\n",
      "| Epoch  26 |   400/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00012173\n",
      "| Epoch  26 |   450/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00012184\n",
      "| Epoch  26 |   500/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00008031\n",
      "| Epoch  26 |   550/  823 batches | lr 0.00006 | ms/batch 19.57 | loss 0.00012504\n",
      "| Epoch  26 |   600/  823 batches | lr 0.00006 | ms/batch 21.11 | loss 0.00013299\n",
      "| Epoch  26 |   650/  823 batches | lr 0.00006 | ms/batch 19.79 | loss 0.00009224\n",
      "| Epoch  26 |   700/  823 batches | lr 0.00006 | ms/batch 21.08 | loss 0.00009059\n",
      "| Epoch  26 |   750/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00014682\n",
      "| Epoch  26 |   800/  823 batches | lr 0.00006 | ms/batch 21.18 | loss 0.00014270\n",
      "\n",
      "Val set: Average loss: 0.00008826\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  27 |    50/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00007698\n",
      "| Epoch  27 |   100/  823 batches | lr 0.00006 | ms/batch 19.47 | loss 0.00007847\n",
      "| Epoch  27 |   150/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00008239\n",
      "| Epoch  27 |   200/  823 batches | lr 0.00006 | ms/batch 21.16 | loss 0.00020630\n",
      "| Epoch  27 |   250/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00010609\n",
      "| Epoch  27 |   300/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00016551\n",
      "| Epoch  27 |   350/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00009987\n",
      "| Epoch  27 |   400/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00010538\n",
      "| Epoch  27 |   450/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00009140\n",
      "| Epoch  27 |   500/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00009781\n",
      "| Epoch  27 |   550/  823 batches | lr 0.00006 | ms/batch 19.76 | loss 0.00016445\n",
      "| Epoch  27 |   600/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00009726\n",
      "| Epoch  27 |   650/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00008291\n",
      "| Epoch  27 |   700/  823 batches | lr 0.00006 | ms/batch 21.14 | loss 0.00010391\n",
      "| Epoch  27 |   750/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00011822\n",
      "| Epoch  27 |   800/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00017311\n",
      "\n",
      "Val set: Average loss: 0.00018778\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  28 |    50/  823 batches | lr 0.00006 | ms/batch 19.80 | loss 0.00012862\n",
      "| Epoch  28 |   100/  823 batches | lr 0.00006 | ms/batch 19.48 | loss 0.00008407\n",
      "| Epoch  28 |   150/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00009244\n",
      "| Epoch  28 |   200/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00009590\n",
      "| Epoch  28 |   250/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00008016\n",
      "| Epoch  28 |   300/  823 batches | lr 0.00006 | ms/batch 21.11 | loss 0.00011882\n",
      "| Epoch  28 |   350/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00007187\n",
      "| Epoch  28 |   400/  823 batches | lr 0.00006 | ms/batch 21.15 | loss 0.00007997\n",
      "| Epoch  28 |   450/  823 batches | lr 0.00006 | ms/batch 19.53 | loss 0.00007910\n",
      "| Epoch  28 |   500/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00014750\n",
      "| Epoch  28 |   550/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00015732\n",
      "| Epoch  28 |   600/  823 batches | lr 0.00006 | ms/batch 21.06 | loss 0.00008142\n",
      "| Epoch  28 |   650/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00007022\n",
      "| Epoch  28 |   700/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00007704\n",
      "| Epoch  28 |   750/  823 batches | lr 0.00006 | ms/batch 19.83 | loss 0.00015307\n",
      "| Epoch  28 |   800/  823 batches | lr 0.00006 | ms/batch 21.48 | loss 0.00014496\n",
      "\n",
      "Val set: Average loss: 0.00010812\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  29 |    50/  823 batches | lr 0.00006 | ms/batch 20.11 | loss 0.00007757\n",
      "| Epoch  29 |   100/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00008668\n",
      "| Epoch  29 |   150/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00009678\n",
      "| Epoch  29 |   200/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00011337\n",
      "| Epoch  29 |   250/  823 batches | lr 0.00006 | ms/batch 19.87 | loss 0.00007235\n",
      "| Epoch  29 |   300/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00013600\n",
      "| Epoch  29 |   350/  823 batches | lr 0.00006 | ms/batch 19.83 | loss 0.00009205\n",
      "| Epoch  29 |   400/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00010181\n",
      "| Epoch  29 |   450/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00007081\n",
      "| Epoch  29 |   500/  823 batches | lr 0.00006 | ms/batch 21.37 | loss 0.00026568\n",
      "| Epoch  29 |   550/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00009263\n",
      "| Epoch  29 |   600/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00012023\n",
      "| Epoch  29 |   650/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00012272\n",
      "| Epoch  29 |   700/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00012097\n",
      "| Epoch  29 |   750/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00007106\n",
      "| Epoch  29 |   800/  823 batches | lr 0.00006 | ms/batch 21.39 | loss 0.00010135\n",
      "\n",
      "Val set: Average loss: 0.00007903\n",
      "\n",
      "| Epoch  30 |    50/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00007798\n",
      "| Epoch  30 |   100/  823 batches | lr 0.00006 | ms/batch 20.00 | loss 0.00011318\n",
      "| Epoch  30 |   150/  823 batches | lr 0.00006 | ms/batch 20.04 | loss 0.00010541\n",
      "| Epoch  30 |   200/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00009527\n",
      "| Epoch  30 |   250/  823 batches | lr 0.00006 | ms/batch 19.96 | loss 0.00006810\n",
      "| Epoch  30 |   300/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00013535\n",
      "| Epoch  30 |   350/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00007758\n",
      "| Epoch  30 |   400/  823 batches | lr 0.00006 | ms/batch 21.41 | loss 0.00011334\n",
      "| Epoch  30 |   450/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00012909\n",
      "| Epoch  30 |   500/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00009652\n",
      "| Epoch  30 |   550/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00008034\n",
      "| Epoch  30 |   600/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00006926\n",
      "| Epoch  30 |   650/  823 batches | lr 0.00006 | ms/batch 19.52 | loss 0.00011963\n",
      "| Epoch  30 |   700/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00009124\n",
      "| Epoch  30 |   750/  823 batches | lr 0.00006 | ms/batch 19.82 | loss 0.00008744\n",
      "| Epoch  30 |   800/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00011551\n",
      "\n",
      "Val set: Average loss: 0.00007702\n",
      "\n",
      "| Epoch  31 |    50/  823 batches | lr 0.00006 | ms/batch 19.95 | loss 0.00008005\n",
      "| Epoch  31 |   100/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00011205\n",
      "| Epoch  31 |   150/  823 batches | lr 0.00006 | ms/batch 19.80 | loss 0.00009202\n",
      "| Epoch  31 |   200/  823 batches | lr 0.00006 | ms/batch 21.36 | loss 0.00008786\n",
      "| Epoch  31 |   250/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00008551\n",
      "| Epoch  31 |   300/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00015015\n",
      "| Epoch  31 |   350/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00008157\n",
      "| Epoch  31 |   400/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00009846\n",
      "| Epoch  31 |   450/  823 batches | lr 0.00006 | ms/batch 20.06 | loss 0.00008474\n",
      "| Epoch  31 |   500/  823 batches | lr 0.00006 | ms/batch 21.14 | loss 0.00007719\n",
      "| Epoch  31 |   550/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00013137\n",
      "| Epoch  31 |   600/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00009474\n",
      "| Epoch  31 |   650/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00010067\n",
      "| Epoch  31 |   700/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00010329\n",
      "| Epoch  31 |   750/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00009381\n",
      "| Epoch  31 |   800/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00012186\n",
      "\n",
      "Val set: Average loss: 0.00007547\n",
      "\n",
      "| Epoch  32 |    50/  823 batches | lr 0.00006 | ms/batch 20.05 | loss 0.00008360\n",
      "| Epoch  32 |   100/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00019374\n",
      "| Epoch  32 |   150/  823 batches | lr 0.00006 | ms/batch 19.85 | loss 0.00017570\n",
      "| Epoch  32 |   200/  823 batches | lr 0.00006 | ms/batch 21.16 | loss 0.00014826\n",
      "| Epoch  32 |   250/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00009068\n",
      "| Epoch  32 |   300/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00010685\n",
      "| Epoch  32 |   350/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00013017\n",
      "| Epoch  32 |   400/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00010926\n",
      "| Epoch  32 |   450/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00006293\n",
      "| Epoch  32 |   500/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00007894\n",
      "| Epoch  32 |   550/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00008030\n",
      "| Epoch  32 |   600/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00010297\n",
      "| Epoch  32 |   650/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00011099\n",
      "| Epoch  32 |   700/  823 batches | lr 0.00006 | ms/batch 21.48 | loss 0.00010057\n",
      "| Epoch  32 |   750/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00007026\n",
      "| Epoch  32 |   800/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00007823\n",
      "\n",
      "Val set: Average loss: 0.00008283\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  33 |    50/  823 batches | lr 0.00006 | ms/batch 19.86 | loss 0.00006425\n",
      "| Epoch  33 |   100/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00008898\n",
      "| Epoch  33 |   150/  823 batches | lr 0.00006 | ms/batch 19.85 | loss 0.00009991\n",
      "| Epoch  33 |   200/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00009980\n",
      "| Epoch  33 |   250/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00006440\n",
      "| Epoch  33 |   300/  823 batches | lr 0.00006 | ms/batch 21.35 | loss 0.00011641\n",
      "| Epoch  33 |   350/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00009324\n",
      "| Epoch  33 |   400/  823 batches | lr 0.00006 | ms/batch 21.42 | loss 0.00009817\n",
      "| Epoch  33 |   450/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00012466\n",
      "| Epoch  33 |   500/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00006555\n",
      "| Epoch  33 |   550/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00011421\n",
      "| Epoch  33 |   600/  823 batches | lr 0.00006 | ms/batch 20.97 | loss 0.00007861\n",
      "| Epoch  33 |   650/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00010963\n",
      "| Epoch  33 |   700/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00010461\n",
      "| Epoch  33 |   750/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00007663\n",
      "| Epoch  33 |   800/  823 batches | lr 0.00006 | ms/batch 21.10 | loss 0.00008218\n",
      "\n",
      "Val set: Average loss: 0.00006089\n",
      "\n",
      "| Epoch  34 |    50/  823 batches | lr 0.00006 | ms/batch 19.94 | loss 0.00014277\n",
      "| Epoch  34 |   100/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00011637\n",
      "| Epoch  34 |   150/  823 batches | lr 0.00006 | ms/batch 19.82 | loss 0.00008437\n",
      "| Epoch  34 |   200/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00011455\n",
      "| Epoch  34 |   250/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00006977\n",
      "| Epoch  34 |   300/  823 batches | lr 0.00006 | ms/batch 21.43 | loss 0.00010900\n",
      "| Epoch  34 |   350/  823 batches | lr 0.00006 | ms/batch 19.82 | loss 0.00007712\n",
      "| Epoch  34 |   400/  823 batches | lr 0.00006 | ms/batch 21.44 | loss 0.00007813\n",
      "| Epoch  34 |   450/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00019204\n",
      "| Epoch  34 |   500/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00009946\n",
      "| Epoch  34 |   550/  823 batches | lr 0.00006 | ms/batch 19.92 | loss 0.00009114\n",
      "| Epoch  34 |   600/  823 batches | lr 0.00006 | ms/batch 21.47 | loss 0.00011894\n",
      "| Epoch  34 |   650/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00010743\n",
      "| Epoch  34 |   700/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00009753\n",
      "| Epoch  34 |   750/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00006833\n",
      "| Epoch  34 |   800/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00007982\n",
      "\n",
      "Val set: Average loss: 0.00007397\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  35 |    50/  823 batches | lr 0.00006 | ms/batch 20.03 | loss 0.00008063\n",
      "| Epoch  35 |   100/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00006738\n",
      "| Epoch  35 |   150/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00006749\n",
      "| Epoch  35 |   200/  823 batches | lr 0.00006 | ms/batch 21.36 | loss 0.00007039\n",
      "| Epoch  35 |   250/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00010293\n",
      "| Epoch  35 |   300/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00011847\n",
      "| Epoch  35 |   350/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00005920\n",
      "| Epoch  35 |   400/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00012904\n",
      "| Epoch  35 |   450/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00013351\n",
      "| Epoch  35 |   500/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00006763\n",
      "| Epoch  35 |   550/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00005672\n",
      "| Epoch  35 |   600/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00008603\n",
      "| Epoch  35 |   650/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00009025\n",
      "| Epoch  35 |   700/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00007359\n",
      "| Epoch  35 |   750/  823 batches | lr 0.00006 | ms/batch 19.78 | loss 0.00016018\n",
      "| Epoch  35 |   800/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00007864\n",
      "\n",
      "Val set: Average loss: 0.00006899\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  36 |    50/  823 batches | lr 0.00006 | ms/batch 19.85 | loss 0.00007961\n",
      "| Epoch  36 |   100/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00006202\n",
      "| Epoch  36 |   150/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00009600\n",
      "| Epoch  36 |   200/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00011631\n",
      "| Epoch  36 |   250/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00007027\n",
      "| Epoch  36 |   300/  823 batches | lr 0.00006 | ms/batch 21.40 | loss 0.00011143\n",
      "| Epoch  36 |   350/  823 batches | lr 0.00006 | ms/batch 19.57 | loss 0.00009523\n",
      "| Epoch  36 |   400/  823 batches | lr 0.00006 | ms/batch 21.39 | loss 0.00009621\n",
      "| Epoch  36 |   450/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00011284\n",
      "| Epoch  36 |   500/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00006742\n",
      "| Epoch  36 |   550/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00006023\n",
      "| Epoch  36 |   600/  823 batches | lr 0.00006 | ms/batch 21.42 | loss 0.00014399\n",
      "| Epoch  36 |   650/  823 batches | lr 0.00006 | ms/batch 19.57 | loss 0.00009798\n",
      "| Epoch  36 |   700/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00008143\n",
      "| Epoch  36 |   750/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00007566\n",
      "| Epoch  36 |   800/  823 batches | lr 0.00006 | ms/batch 21.37 | loss 0.00009797\n",
      "\n",
      "Val set: Average loss: 0.00006891\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  37 |    50/  823 batches | lr 0.00006 | ms/batch 19.83 | loss 0.00008030\n",
      "| Epoch  37 |   100/  823 batches | lr 0.00006 | ms/batch 19.43 | loss 0.00006132\n",
      "| Epoch  37 |   150/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00009896\n",
      "| Epoch  37 |   200/  823 batches | lr 0.00006 | ms/batch 21.41 | loss 0.00012958\n",
      "| Epoch  37 |   250/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00006874\n",
      "| Epoch  37 |   300/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00011521\n",
      "| Epoch  37 |   350/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00016550\n",
      "| Epoch  37 |   400/  823 batches | lr 0.00006 | ms/batch 21.32 | loss 0.00007956\n",
      "| Epoch  37 |   450/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00008507\n",
      "| Epoch  37 |   500/  823 batches | lr 0.00006 | ms/batch 21.12 | loss 0.00006013\n",
      "| Epoch  37 |   550/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00007066\n",
      "| Epoch  37 |   600/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00011583\n",
      "| Epoch  37 |   650/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00006096\n",
      "| Epoch  37 |   700/  823 batches | lr 0.00006 | ms/batch 21.13 | loss 0.00006784\n",
      "| Epoch  37 |   750/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00008291\n",
      "| Epoch  37 |   800/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00008114\n",
      "\n",
      "Val set: Average loss: 0.00007641\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  38 |    50/  823 batches | lr 0.00006 | ms/batch 19.95 | loss 0.00008884\n",
      "| Epoch  38 |   100/  823 batches | lr 0.00006 | ms/batch 19.50 | loss 0.00008277\n",
      "| Epoch  38 |   150/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00016614\n",
      "| Epoch  38 |   200/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00015487\n",
      "| Epoch  38 |   250/  823 batches | lr 0.00006 | ms/batch 19.51 | loss 0.00009897\n",
      "| Epoch  38 |   300/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00009681\n",
      "| Epoch  38 |   350/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00013579\n",
      "| Epoch  38 |   400/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00011722\n",
      "| Epoch  38 |   450/  823 batches | lr 0.00006 | ms/batch 19.57 | loss 0.00005664\n",
      "| Epoch  38 |   500/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00008015\n",
      "| Epoch  38 |   550/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00008812\n",
      "| Epoch  38 |   600/  823 batches | lr 0.00006 | ms/batch 21.37 | loss 0.00015559\n",
      "| Epoch  38 |   650/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00009363\n",
      "| Epoch  38 |   700/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00008059\n",
      "| Epoch  38 |   750/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00005706\n",
      "| Epoch  38 |   800/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00010287\n",
      "\n",
      "Val set: Average loss: 0.00006634\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  39 |    50/  823 batches | lr 0.00006 | ms/batch 19.95 | loss 0.00007614\n",
      "| Epoch  39 |   100/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00007567\n",
      "| Epoch  39 |   150/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00008631\n",
      "| Epoch  39 |   200/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00011175\n",
      "| Epoch  39 |   250/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00006030\n",
      "| Epoch  39 |   300/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00006681\n",
      "| Epoch  39 |   350/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00006998\n",
      "| Epoch  39 |   400/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00007944\n",
      "| Epoch  39 |   450/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00008526\n",
      "| Epoch  39 |   500/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00005372\n",
      "| Epoch  39 |   550/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00006119\n",
      "| Epoch  39 |   600/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00008735\n",
      "| Epoch  39 |   650/  823 batches | lr 0.00006 | ms/batch 19.84 | loss 0.00010265\n",
      "| Epoch  39 |   700/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00010744\n",
      "| Epoch  39 |   750/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00005769\n",
      "| Epoch  39 |   800/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00006730\n",
      "\n",
      "Val set: Average loss: 0.00006363\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch  40 |    50/  823 batches | lr 0.00006 | ms/batch 19.86 | loss 0.00008585\n",
      "| Epoch  40 |   100/  823 batches | lr 0.00006 | ms/batch 19.52 | loss 0.00009192\n",
      "| Epoch  40 |   150/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00011710\n",
      "| Epoch  40 |   200/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00015886\n",
      "| Epoch  40 |   250/  823 batches | lr 0.00006 | ms/batch 19.83 | loss 0.00005756\n",
      "| Epoch  40 |   300/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00005638\n",
      "| Epoch  40 |   350/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00005669\n",
      "| Epoch  40 |   400/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00006174\n",
      "| Epoch  40 |   450/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00019964\n",
      "| Epoch  40 |   500/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00007257\n",
      "| Epoch  40 |   550/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00007913\n",
      "| Epoch  40 |   600/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00007777\n",
      "| Epoch  40 |   650/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00007767\n",
      "| Epoch  40 |   700/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00007047\n",
      "| Epoch  40 |   750/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00006569\n",
      "| Epoch  40 |   800/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00006970\n",
      "\n",
      "Val set: Average loss: 0.00011842\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch  41 |    50/  823 batches | lr 0.00006 | ms/batch 19.92 | loss 0.00012943\n",
      "| Epoch  41 |   100/  823 batches | lr 0.00006 | ms/batch 19.52 | loss 0.00008666\n",
      "| Epoch  41 |   150/  823 batches | lr 0.00006 | ms/batch 19.76 | loss 0.00008743\n",
      "| Epoch  41 |   200/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00010723\n",
      "| Epoch  41 |   250/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00005253\n",
      "| Epoch  41 |   300/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00006526\n",
      "| Epoch  41 |   350/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00006877\n",
      "| Epoch  41 |   400/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00009545\n",
      "| Epoch  41 |   450/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00009226\n",
      "| Epoch  41 |   500/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00004977\n",
      "| Epoch  41 |   550/  823 batches | lr 0.00006 | ms/batch 19.83 | loss 0.00010041\n",
      "| Epoch  41 |   600/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00008953\n",
      "| Epoch  41 |   650/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00006033\n",
      "| Epoch  41 |   700/  823 batches | lr 0.00006 | ms/batch 21.38 | loss 0.00013031\n",
      "| Epoch  41 |   750/  823 batches | lr 0.00006 | ms/batch 20.05 | loss 0.00006230\n",
      "| Epoch  41 |   800/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00010417\n",
      "\n",
      "Val set: Average loss: 0.00006413\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch  42 |    50/  823 batches | lr 0.00006 | ms/batch 19.85 | loss 0.00007993\n",
      "| Epoch  42 |   100/  823 batches | lr 0.00006 | ms/batch 19.53 | loss 0.00006792\n",
      "| Epoch  42 |   150/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00007700\n",
      "| Epoch  42 |   200/  823 batches | lr 0.00006 | ms/batch 21.37 | loss 0.00007114\n",
      "| Epoch  42 |   250/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00008090\n",
      "| Epoch  42 |   300/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00007912\n",
      "| Epoch  42 |   350/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00009972\n",
      "| Epoch  42 |   400/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00016532\n",
      "| Epoch  42 |   450/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00016104\n",
      "| Epoch  42 |   500/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00010863\n",
      "| Epoch  42 |   550/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00005977\n",
      "| Epoch  42 |   600/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00015645\n",
      "| Epoch  42 |   650/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00006226\n",
      "| Epoch  42 |   700/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00006426\n",
      "| Epoch  42 |   750/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00005647\n",
      "| Epoch  42 |   800/  823 batches | lr 0.00006 | ms/batch 21.40 | loss 0.00008828\n",
      "\n",
      "Val set: Average loss: 0.00007003\n",
      "\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch  43 |    50/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00007389\n",
      "| Epoch  43 |   100/  823 batches | lr 0.00006 | ms/batch 19.51 | loss 0.00006958\n",
      "| Epoch  43 |   150/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00008420\n",
      "| Epoch  43 |   200/  823 batches | lr 0.00006 | ms/batch 21.37 | loss 0.00007665\n",
      "| Epoch  43 |   250/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00006646\n",
      "| Epoch  43 |   300/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00006863\n",
      "| Epoch  43 |   350/  823 batches | lr 0.00006 | ms/batch 19.53 | loss 0.00007865\n",
      "| Epoch  43 |   400/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00007370\n",
      "| Epoch  43 |   450/  823 batches | lr 0.00006 | ms/batch 19.76 | loss 0.00005116\n",
      "| Epoch  43 |   500/  823 batches | lr 0.00006 | ms/batch 21.15 | loss 0.00005092\n",
      "| Epoch  43 |   550/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00008585\n",
      "| Epoch  43 |   600/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00007085\n",
      "| Epoch  43 |   650/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00011384\n",
      "| Epoch  43 |   700/  823 batches | lr 0.00006 | ms/batch 21.38 | loss 0.00005930\n",
      "| Epoch  43 |   750/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00008246\n",
      "| Epoch  43 |   800/  823 batches | lr 0.00006 | ms/batch 21.15 | loss 0.00007917\n",
      "\n",
      "Val set: Average loss: 0.00005663\n",
      "\n",
      "| Epoch  44 |    50/  823 batches | lr 0.00006 | ms/batch 19.95 | loss 0.00007641\n",
      "| Epoch  44 |   100/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00008121\n",
      "| Epoch  44 |   150/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00008464\n",
      "| Epoch  44 |   200/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00007318\n",
      "| Epoch  44 |   250/  823 batches | lr 0.00006 | ms/batch 19.57 | loss 0.00005686\n",
      "| Epoch  44 |   300/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00012255\n",
      "| Epoch  44 |   350/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00009407\n",
      "| Epoch  44 |   400/  823 batches | lr 0.00006 | ms/batch 21.09 | loss 0.00007869\n",
      "| Epoch  44 |   450/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00005287\n",
      "| Epoch  44 |   500/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00009712\n",
      "| Epoch  44 |   550/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00006770\n",
      "| Epoch  44 |   600/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00013096\n",
      "| Epoch  44 |   650/  823 batches | lr 0.00006 | ms/batch 19.79 | loss 0.00016099\n",
      "| Epoch  44 |   700/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00007434\n",
      "| Epoch  44 |   750/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00005639\n",
      "| Epoch  44 |   800/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00006413\n",
      "\n",
      "Val set: Average loss: 0.00006437\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  45 |    50/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00007791\n",
      "| Epoch  45 |   100/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00006786\n",
      "| Epoch  45 |   150/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00008036\n",
      "| Epoch  45 |   200/  823 batches | lr 0.00006 | ms/batch 21.09 | loss 0.00007413\n",
      "| Epoch  45 |   250/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00005672\n",
      "| Epoch  45 |   300/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00012119\n",
      "| Epoch  45 |   350/  823 batches | lr 0.00006 | ms/batch 19.85 | loss 0.00017284\n",
      "| Epoch  45 |   400/  823 batches | lr 0.00006 | ms/batch 21.41 | loss 0.00010638\n",
      "| Epoch  45 |   450/  823 batches | lr 0.00006 | ms/batch 19.86 | loss 0.00005118\n",
      "| Epoch  45 |   500/  823 batches | lr 0.00006 | ms/batch 21.53 | loss 0.00006103\n",
      "| Epoch  45 |   550/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00006974\n",
      "| Epoch  45 |   600/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00009401\n",
      "| Epoch  45 |   650/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00007436\n",
      "| Epoch  45 |   700/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00005031\n",
      "| Epoch  45 |   750/  823 batches | lr 0.00006 | ms/batch 19.85 | loss 0.00004905\n",
      "| Epoch  45 |   800/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00005244\n",
      "\n",
      "Val set: Average loss: 0.00006561\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  46 |    50/  823 batches | lr 0.00006 | ms/batch 19.93 | loss 0.00006915\n",
      "| Epoch  46 |   100/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00007360\n",
      "| Epoch  46 |   150/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00006462\n",
      "| Epoch  46 |   200/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00011123\n",
      "| Epoch  46 |   250/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00004911\n",
      "| Epoch  46 |   300/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00006095\n",
      "| Epoch  46 |   350/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00010462\n",
      "| Epoch  46 |   400/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00007917\n",
      "| Epoch  46 |   450/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00007077\n",
      "| Epoch  46 |   500/  823 batches | lr 0.00006 | ms/batch 21.44 | loss 0.00010734\n",
      "| Epoch  46 |   550/  823 batches | lr 0.00006 | ms/batch 19.82 | loss 0.00011979\n",
      "| Epoch  46 |   600/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00007876\n",
      "| Epoch  46 |   650/  823 batches | lr 0.00006 | ms/batch 19.93 | loss 0.00005612\n",
      "| Epoch  46 |   700/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00005101\n",
      "| Epoch  46 |   750/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00005627\n",
      "| Epoch  46 |   800/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00008432\n",
      "\n",
      "Val set: Average loss: 0.00015063\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  47 |    50/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00008971\n",
      "| Epoch  47 |   100/  823 batches | lr 0.00006 | ms/batch 19.42 | loss 0.00006796\n",
      "| Epoch  47 |   150/  823 batches | lr 0.00006 | ms/batch 19.49 | loss 0.00008719\n",
      "| Epoch  47 |   200/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00008629\n",
      "| Epoch  47 |   250/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00004659\n",
      "| Epoch  47 |   300/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00013441\n",
      "| Epoch  47 |   350/  823 batches | lr 0.00006 | ms/batch 19.78 | loss 0.00028447\n",
      "| Epoch  47 |   400/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00010221\n",
      "| Epoch  47 |   450/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00005390\n",
      "| Epoch  47 |   500/  823 batches | lr 0.00006 | ms/batch 21.16 | loss 0.00006634\n",
      "| Epoch  47 |   550/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00006311\n",
      "| Epoch  47 |   600/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00010302\n",
      "| Epoch  47 |   650/  823 batches | lr 0.00006 | ms/batch 19.54 | loss 0.00004979\n",
      "| Epoch  47 |   700/  823 batches | lr 0.00006 | ms/batch 21.13 | loss 0.00005870\n",
      "| Epoch  47 |   750/  823 batches | lr 0.00006 | ms/batch 19.94 | loss 0.00004827\n",
      "| Epoch  47 |   800/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00006518\n",
      "\n",
      "Val set: Average loss: 0.00011277\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  48 |    50/  823 batches | lr 0.00006 | ms/batch 19.83 | loss 0.00009083\n",
      "| Epoch  48 |   100/  823 batches | lr 0.00006 | ms/batch 19.53 | loss 0.00005449\n",
      "| Epoch  48 |   150/  823 batches | lr 0.00006 | ms/batch 19.80 | loss 0.00008936\n",
      "| Epoch  48 |   200/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00007988\n",
      "| Epoch  48 |   250/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00008739\n",
      "| Epoch  48 |   300/  823 batches | lr 0.00006 | ms/batch 21.13 | loss 0.00012988\n",
      "| Epoch  48 |   350/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00016689\n",
      "| Epoch  48 |   400/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00006903\n",
      "| Epoch  48 |   450/  823 batches | lr 0.00006 | ms/batch 19.54 | loss 0.00004886\n",
      "| Epoch  48 |   500/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00006482\n",
      "| Epoch  48 |   550/  823 batches | lr 0.00006 | ms/batch 19.50 | loss 0.00004977\n",
      "| Epoch  48 |   600/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00011855\n",
      "| Epoch  48 |   650/  823 batches | lr 0.00006 | ms/batch 19.83 | loss 0.00006785\n",
      "| Epoch  48 |   700/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00004868\n",
      "| Epoch  48 |   750/  823 batches | lr 0.00006 | ms/batch 19.47 | loss 0.00004487\n",
      "| Epoch  48 |   800/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00005581\n",
      "\n",
      "Val set: Average loss: 0.00007493\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  49 |    50/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00006056\n",
      "| Epoch  49 |   100/  823 batches | lr 0.00006 | ms/batch 19.46 | loss 0.00004626\n",
      "| Epoch  49 |   150/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00007017\n",
      "| Epoch  49 |   200/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00004964\n",
      "| Epoch  49 |   250/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00014262\n",
      "| Epoch  49 |   300/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00008199\n",
      "| Epoch  49 |   350/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00012473\n",
      "| Epoch  49 |   400/  823 batches | lr 0.00006 | ms/batch 21.12 | loss 0.00005535\n",
      "| Epoch  49 |   450/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00004593\n",
      "| Epoch  49 |   500/  823 batches | lr 0.00006 | ms/batch 20.96 | loss 0.00004392\n",
      "| Epoch  49 |   550/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00007235\n",
      "| Epoch  49 |   600/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00005374\n",
      "| Epoch  49 |   650/  823 batches | lr 0.00006 | ms/batch 19.48 | loss 0.00006551\n",
      "| Epoch  49 |   700/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00008946\n",
      "| Epoch  49 |   750/  823 batches | lr 0.00006 | ms/batch 19.54 | loss 0.00006749\n",
      "| Epoch  49 |   800/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00007841\n",
      "\n",
      "Val set: Average loss: 0.00004913\n",
      "\n",
      "| Epoch  50 |    50/  823 batches | lr 0.00006 | ms/batch 19.82 | loss 0.00007118\n",
      "| Epoch  50 |   100/  823 batches | lr 0.00006 | ms/batch 19.46 | loss 0.00007249\n",
      "| Epoch  50 |   150/  823 batches | lr 0.00006 | ms/batch 19.79 | loss 0.00008010\n",
      "| Epoch  50 |   200/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00009890\n",
      "| Epoch  50 |   250/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00008195\n",
      "| Epoch  50 |   300/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00007767\n",
      "| Epoch  50 |   350/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00007790\n",
      "| Epoch  50 |   400/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00005481\n",
      "| Epoch  50 |   450/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00004680\n",
      "| Epoch  50 |   500/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00011409\n",
      "| Epoch  50 |   550/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00006415\n",
      "| Epoch  50 |   600/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00011413\n",
      "| Epoch  50 |   650/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00013250\n",
      "| Epoch  50 |   700/  823 batches | lr 0.00006 | ms/batch 21.15 | loss 0.00004926\n",
      "| Epoch  50 |   750/  823 batches | lr 0.00006 | ms/batch 19.57 | loss 0.00004662\n",
      "| Epoch  50 |   800/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00005645\n",
      "\n",
      "Val set: Average loss: 0.00005301\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  51 |    50/  823 batches | lr 0.00006 | ms/batch 19.91 | loss 0.00005713\n",
      "| Epoch  51 |   100/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00008747\n",
      "| Epoch  51 |   150/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00006022\n",
      "| Epoch  51 |   200/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00009496\n",
      "| Epoch  51 |   250/  823 batches | lr 0.00006 | ms/batch 19.78 | loss 0.00005972\n",
      "| Epoch  51 |   300/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00006998\n",
      "| Epoch  51 |   350/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00005958\n",
      "| Epoch  51 |   400/  823 batches | lr 0.00006 | ms/batch 21.16 | loss 0.00005372\n",
      "| Epoch  51 |   450/  823 batches | lr 0.00006 | ms/batch 20.54 | loss 0.00011147\n",
      "| Epoch  51 |   500/  823 batches | lr 0.00006 | ms/batch 22.08 | loss 0.00005926\n",
      "| Epoch  51 |   550/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00006196\n",
      "| Epoch  51 |   600/  823 batches | lr 0.00006 | ms/batch 21.16 | loss 0.00010067\n",
      "| Epoch  51 |   650/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00011831\n",
      "| Epoch  51 |   700/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00006021\n",
      "| Epoch  51 |   750/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00004551\n",
      "| Epoch  51 |   800/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00005088\n",
      "\n",
      "Val set: Average loss: 0.00005845\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  52 |    50/  823 batches | lr 0.00006 | ms/batch 19.94 | loss 0.00009512\n",
      "| Epoch  52 |   100/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00013071\n",
      "| Epoch  52 |   150/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00009470\n",
      "| Epoch  52 |   200/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00010942\n",
      "| Epoch  52 |   250/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00007725\n",
      "| Epoch  52 |   300/  823 batches | lr 0.00006 | ms/batch 21.44 | loss 0.00008649\n",
      "| Epoch  52 |   350/  823 batches | lr 0.00006 | ms/batch 19.51 | loss 0.00015774\n",
      "| Epoch  52 |   400/  823 batches | lr 0.00006 | ms/batch 21.10 | loss 0.00005637\n",
      "| Epoch  52 |   450/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00004432\n",
      "| Epoch  52 |   500/  823 batches | lr 0.00006 | ms/batch 21.09 | loss 0.00006092\n",
      "| Epoch  52 |   550/  823 batches | lr 0.00006 | ms/batch 19.45 | loss 0.00004657\n",
      "| Epoch  52 |   600/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00012536\n",
      "| Epoch  52 |   650/  823 batches | lr 0.00006 | ms/batch 19.52 | loss 0.00008221\n",
      "| Epoch  52 |   700/  823 batches | lr 0.00006 | ms/batch 21.02 | loss 0.00004102\n",
      "| Epoch  52 |   750/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00004699\n",
      "| Epoch  52 |   800/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00008244\n",
      "\n",
      "Val set: Average loss: 0.00005600\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  53 |    50/  823 batches | lr 0.00006 | ms/batch 19.99 | loss 0.00007108\n",
      "| Epoch  53 |   100/  823 batches | lr 0.00006 | ms/batch 19.51 | loss 0.00008328\n",
      "| Epoch  53 |   150/  823 batches | lr 0.00006 | ms/batch 19.52 | loss 0.00009719\n",
      "| Epoch  53 |   200/  823 batches | lr 0.00006 | ms/batch 21.39 | loss 0.00022790\n",
      "| Epoch  53 |   250/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00005392\n",
      "| Epoch  53 |   300/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00006359\n",
      "| Epoch  53 |   350/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00009156\n",
      "| Epoch  53 |   400/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00005927\n",
      "| Epoch  53 |   450/  823 batches | lr 0.00006 | ms/batch 19.86 | loss 0.00004577\n",
      "| Epoch  53 |   500/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00006213\n",
      "| Epoch  53 |   550/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00004776\n",
      "| Epoch  53 |   600/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00012195\n",
      "| Epoch  53 |   650/  823 batches | lr 0.00006 | ms/batch 19.76 | loss 0.00008284\n",
      "| Epoch  53 |   700/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00004355\n",
      "| Epoch  53 |   750/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00004881\n",
      "| Epoch  53 |   800/  823 batches | lr 0.00006 | ms/batch 22.02 | loss 0.00010555\n",
      "\n",
      "Val set: Average loss: 0.00017130\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  54 |    50/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00007672\n",
      "| Epoch  54 |   100/  823 batches | lr 0.00006 | ms/batch 19.52 | loss 0.00006474\n",
      "| Epoch  54 |   150/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00006322\n",
      "| Epoch  54 |   200/  823 batches | lr 0.00006 | ms/batch 21.03 | loss 0.00010803\n",
      "| Epoch  54 |   250/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00005375\n",
      "| Epoch  54 |   300/  823 batches | lr 0.00006 | ms/batch 21.32 | loss 0.00006398\n",
      "| Epoch  54 |   350/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00009129\n",
      "| Epoch  54 |   400/  823 batches | lr 0.00006 | ms/batch 21.35 | loss 0.00007033\n",
      "| Epoch  54 |   450/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00003870\n",
      "| Epoch  54 |   500/  823 batches | lr 0.00006 | ms/batch 21.15 | loss 0.00004003\n",
      "| Epoch  54 |   550/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00004422\n",
      "| Epoch  54 |   600/  823 batches | lr 0.00006 | ms/batch 21.15 | loss 0.00005104\n",
      "| Epoch  54 |   650/  823 batches | lr 0.00006 | ms/batch 19.50 | loss 0.00005065\n",
      "| Epoch  54 |   700/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00007378\n",
      "| Epoch  54 |   750/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00005527\n",
      "| Epoch  54 |   800/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00005827\n",
      "\n",
      "Val set: Average loss: 0.00005050\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  55 |    50/  823 batches | lr 0.00006 | ms/batch 19.85 | loss 0.00005051\n",
      "| Epoch  55 |   100/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00010415\n",
      "| Epoch  55 |   150/  823 batches | lr 0.00006 | ms/batch 19.54 | loss 0.00004566\n",
      "| Epoch  55 |   200/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00006814\n",
      "| Epoch  55 |   250/  823 batches | lr 0.00006 | ms/batch 19.48 | loss 0.00006280\n",
      "| Epoch  55 |   300/  823 batches | lr 0.00006 | ms/batch 21.10 | loss 0.00006259\n",
      "| Epoch  55 |   350/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00010759\n",
      "| Epoch  55 |   400/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00008406\n",
      "| Epoch  55 |   450/  823 batches | lr 0.00006 | ms/batch 19.41 | loss 0.00005839\n",
      "| Epoch  55 |   500/  823 batches | lr 0.00006 | ms/batch 21.11 | loss 0.00004777\n",
      "| Epoch  55 |   550/  823 batches | lr 0.00006 | ms/batch 19.50 | loss 0.00005625\n",
      "| Epoch  55 |   600/  823 batches | lr 0.00006 | ms/batch 21.12 | loss 0.00008957\n",
      "| Epoch  55 |   650/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00007013\n",
      "| Epoch  55 |   700/  823 batches | lr 0.00006 | ms/batch 21.50 | loss 0.00006145\n",
      "| Epoch  55 |   750/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00006199\n",
      "| Epoch  55 |   800/  823 batches | lr 0.00006 | ms/batch 21.03 | loss 0.00006872\n",
      "\n",
      "Val set: Average loss: 0.00013480\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch  56 |    50/  823 batches | lr 0.00006 | ms/batch 19.95 | loss 0.00006311\n",
      "| Epoch  56 |   100/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00006189\n",
      "| Epoch  56 |   150/  823 batches | lr 0.00006 | ms/batch 19.53 | loss 0.00005144\n",
      "| Epoch  56 |   200/  823 batches | lr 0.00006 | ms/batch 21.32 | loss 0.00009454\n",
      "| Epoch  56 |   250/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00006370\n",
      "| Epoch  56 |   300/  823 batches | lr 0.00006 | ms/batch 21.16 | loss 0.00009166\n",
      "| Epoch  56 |   350/  823 batches | lr 0.00006 | ms/batch 19.57 | loss 0.00006857\n",
      "| Epoch  56 |   400/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00005386\n",
      "| Epoch  56 |   450/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00005302\n",
      "| Epoch  56 |   500/  823 batches | lr 0.00006 | ms/batch 21.14 | loss 0.00004008\n",
      "| Epoch  56 |   550/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00007384\n",
      "| Epoch  56 |   600/  823 batches | lr 0.00006 | ms/batch 21.16 | loss 0.00008428\n",
      "| Epoch  56 |   650/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00006654\n",
      "| Epoch  56 |   700/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00005809\n",
      "| Epoch  56 |   750/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00011461\n",
      "| Epoch  56 |   800/  823 batches | lr 0.00006 | ms/batch 21.41 | loss 0.00008415\n",
      "\n",
      "Val set: Average loss: 0.00008096\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch  57 |    50/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00007017\n",
      "| Epoch  57 |   100/  823 batches | lr 0.00006 | ms/batch 19.47 | loss 0.00005458\n",
      "| Epoch  57 |   150/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00004273\n",
      "| Epoch  57 |   200/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00009194\n",
      "| Epoch  57 |   250/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00005197\n",
      "| Epoch  57 |   300/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00008217\n",
      "| Epoch  57 |   350/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00006995\n",
      "| Epoch  57 |   400/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00007676\n",
      "| Epoch  57 |   450/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00006741\n",
      "| Epoch  57 |   500/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00005392\n",
      "| Epoch  57 |   550/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00006034\n",
      "| Epoch  57 |   600/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00007300\n",
      "| Epoch  57 |   650/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00013576\n",
      "| Epoch  57 |   700/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00005533\n",
      "| Epoch  57 |   750/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00005184\n",
      "| Epoch  57 |   800/  823 batches | lr 0.00006 | ms/batch 21.11 | loss 0.00009281\n",
      "\n",
      "Val set: Average loss: 0.00004894\n",
      "\n",
      "| Epoch  58 |    50/  823 batches | lr 0.00006 | ms/batch 19.76 | loss 0.00006607\n",
      "| Epoch  58 |   100/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00005306\n",
      "| Epoch  58 |   150/  823 batches | lr 0.00006 | ms/batch 19.57 | loss 0.00005799\n",
      "| Epoch  58 |   200/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00005417\n",
      "| Epoch  58 |   250/  823 batches | lr 0.00006 | ms/batch 19.52 | loss 0.00004049\n",
      "| Epoch  58 |   300/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00006945\n",
      "| Epoch  58 |   350/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00006414\n",
      "| Epoch  58 |   400/  823 batches | lr 0.00006 | ms/batch 21.14 | loss 0.00006876\n",
      "| Epoch  58 |   450/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00005103\n",
      "| Epoch  58 |   500/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00006137\n",
      "| Epoch  58 |   550/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00010789\n",
      "| Epoch  58 |   600/  823 batches | lr 0.00006 | ms/batch 21.15 | loss 0.00009334\n",
      "| Epoch  58 |   650/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00016505\n",
      "| Epoch  58 |   700/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00005272\n",
      "| Epoch  58 |   750/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00005573\n",
      "| Epoch  58 |   800/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00009594\n",
      "\n",
      "Val set: Average loss: 0.00020379\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  59 |    50/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00007360\n",
      "| Epoch  59 |   100/  823 batches | lr 0.00006 | ms/batch 19.48 | loss 0.00005248\n",
      "| Epoch  59 |   150/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00005556\n",
      "| Epoch  59 |   200/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00007422\n",
      "| Epoch  59 |   250/  823 batches | lr 0.00006 | ms/batch 19.57 | loss 0.00005861\n",
      "| Epoch  59 |   300/  823 batches | lr 0.00006 | ms/batch 21.32 | loss 0.00007133\n",
      "| Epoch  59 |   350/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00010719\n",
      "| Epoch  59 |   400/  823 batches | lr 0.00006 | ms/batch 21.16 | loss 0.00008441\n",
      "| Epoch  59 |   450/  823 batches | lr 0.00006 | ms/batch 19.84 | loss 0.00004234\n",
      "| Epoch  59 |   500/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00003919\n",
      "| Epoch  59 |   550/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00005559\n",
      "| Epoch  59 |   600/  823 batches | lr 0.00006 | ms/batch 21.38 | loss 0.00007747\n",
      "| Epoch  59 |   650/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00004242\n",
      "| Epoch  59 |   700/  823 batches | lr 0.00006 | ms/batch 21.08 | loss 0.00004361\n",
      "| Epoch  59 |   750/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00003875\n",
      "| Epoch  59 |   800/  823 batches | lr 0.00006 | ms/batch 21.01 | loss 0.00004136\n",
      "\n",
      "Val set: Average loss: 0.00014427\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  60 |    50/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00006108\n",
      "| Epoch  60 |   100/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00005155\n",
      "| Epoch  60 |   150/  823 batches | lr 0.00006 | ms/batch 19.52 | loss 0.00007751\n",
      "| Epoch  60 |   200/  823 batches | lr 0.00006 | ms/batch 21.05 | loss 0.00004112\n",
      "| Epoch  60 |   250/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00008689\n",
      "| Epoch  60 |   300/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00007763\n",
      "| Epoch  60 |   350/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00004598\n",
      "| Epoch  60 |   400/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00005469\n",
      "| Epoch  60 |   450/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00006598\n",
      "| Epoch  60 |   500/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00005611\n",
      "| Epoch  60 |   550/  823 batches | lr 0.00006 | ms/batch 19.47 | loss 0.00010188\n",
      "| Epoch  60 |   600/  823 batches | lr 0.00006 | ms/batch 21.06 | loss 0.00007241\n",
      "| Epoch  60 |   650/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00004731\n",
      "| Epoch  60 |   700/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00004163\n",
      "| Epoch  60 |   750/  823 batches | lr 0.00006 | ms/batch 19.47 | loss 0.00012687\n",
      "| Epoch  60 |   800/  823 batches | lr 0.00006 | ms/batch 21.16 | loss 0.00010325\n",
      "\n",
      "Val set: Average loss: 0.00010321\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  61 |    50/  823 batches | lr 0.00006 | ms/batch 19.89 | loss 0.00006987\n",
      "| Epoch  61 |   100/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00006967\n",
      "| Epoch  61 |   150/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00005464\n",
      "| Epoch  61 |   200/  823 batches | lr 0.00006 | ms/batch 21.43 | loss 0.00005361\n",
      "| Epoch  61 |   250/  823 batches | lr 0.00006 | ms/batch 19.86 | loss 0.00005539\n",
      "| Epoch  61 |   300/  823 batches | lr 0.00006 | ms/batch 21.44 | loss 0.00008354\n",
      "| Epoch  61 |   350/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00008485\n",
      "| Epoch  61 |   400/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00005746\n",
      "| Epoch  61 |   450/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00004868\n",
      "| Epoch  61 |   500/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00004518\n",
      "| Epoch  61 |   550/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00005378\n",
      "| Epoch  61 |   600/  823 batches | lr 0.00006 | ms/batch 21.43 | loss 0.00009647\n",
      "| Epoch  61 |   650/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00003973\n",
      "| Epoch  61 |   700/  823 batches | lr 0.00006 | ms/batch 21.35 | loss 0.00005696\n",
      "| Epoch  61 |   750/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00005795\n",
      "| Epoch  61 |   800/  823 batches | lr 0.00006 | ms/batch 21.44 | loss 0.00006929\n",
      "\n",
      "Val set: Average loss: 0.00004633\n",
      "\n",
      "| Epoch  62 |    50/  823 batches | lr 0.00006 | ms/batch 20.09 | loss 0.00004652\n",
      "| Epoch  62 |   100/  823 batches | lr 0.00006 | ms/batch 19.88 | loss 0.00004398\n",
      "| Epoch  62 |   150/  823 batches | lr 0.00006 | ms/batch 20.01 | loss 0.00009241\n",
      "| Epoch  62 |   200/  823 batches | lr 0.00006 | ms/batch 21.41 | loss 0.00004435\n",
      "| Epoch  62 |   250/  823 batches | lr 0.00006 | ms/batch 19.87 | loss 0.00004733\n",
      "| Epoch  62 |   300/  823 batches | lr 0.00006 | ms/batch 21.42 | loss 0.00009202\n",
      "| Epoch  62 |   350/  823 batches | lr 0.00006 | ms/batch 19.93 | loss 0.00007844\n",
      "| Epoch  62 |   400/  823 batches | lr 0.00006 | ms/batch 21.49 | loss 0.00006400\n",
      "| Epoch  62 |   450/  823 batches | lr 0.00006 | ms/batch 19.83 | loss 0.00004821\n",
      "| Epoch  62 |   500/  823 batches | lr 0.00006 | ms/batch 21.46 | loss 0.00005385\n",
      "| Epoch  62 |   550/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00012892\n",
      "| Epoch  62 |   600/  823 batches | lr 0.00006 | ms/batch 21.39 | loss 0.00010300\n",
      "| Epoch  62 |   650/  823 batches | lr 0.00006 | ms/batch 19.99 | loss 0.00006956\n",
      "| Epoch  62 |   700/  823 batches | lr 0.00006 | ms/batch 21.63 | loss 0.00004889\n",
      "| Epoch  62 |   750/  823 batches | lr 0.00006 | ms/batch 19.94 | loss 0.00005259\n",
      "| Epoch  62 |   800/  823 batches | lr 0.00006 | ms/batch 21.57 | loss 0.00009976\n",
      "\n",
      "Val set: Average loss: 0.00009287\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  63 |    50/  823 batches | lr 0.00006 | ms/batch 20.01 | loss 0.00008139\n",
      "| Epoch  63 |   100/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00005087\n",
      "| Epoch  63 |   150/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00004830\n",
      "| Epoch  63 |   200/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00005765\n",
      "| Epoch  63 |   250/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00007551\n",
      "| Epoch  63 |   300/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00007569\n",
      "| Epoch  63 |   350/  823 batches | lr 0.00006 | ms/batch 19.76 | loss 0.00012450\n",
      "| Epoch  63 |   400/  823 batches | lr 0.00006 | ms/batch 21.42 | loss 0.00008582\n",
      "| Epoch  63 |   450/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00004300\n",
      "| Epoch  63 |   500/  823 batches | lr 0.00006 | ms/batch 21.52 | loss 0.00005094\n",
      "| Epoch  63 |   550/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00004539\n",
      "| Epoch  63 |   600/  823 batches | lr 0.00006 | ms/batch 21.35 | loss 0.00012489\n",
      "| Epoch  63 |   650/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00007760\n",
      "| Epoch  63 |   700/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00004439\n",
      "| Epoch  63 |   750/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00006091\n",
      "| Epoch  63 |   800/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00011022\n",
      "\n",
      "Val set: Average loss: 0.00014839\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  64 |    50/  823 batches | lr 0.00006 | ms/batch 20.17 | loss 0.00006093\n",
      "| Epoch  64 |   100/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00005575\n",
      "| Epoch  64 |   150/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00007471\n",
      "| Epoch  64 |   200/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00005233\n",
      "| Epoch  64 |   250/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00006195\n",
      "| Epoch  64 |   300/  823 batches | lr 0.00006 | ms/batch 21.38 | loss 0.00008037\n",
      "| Epoch  64 |   350/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00010429\n",
      "| Epoch  64 |   400/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00004821\n",
      "| Epoch  64 |   450/  823 batches | lr 0.00006 | ms/batch 19.78 | loss 0.00003741\n",
      "| Epoch  64 |   500/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00004981\n",
      "| Epoch  64 |   550/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00004507\n",
      "| Epoch  64 |   600/  823 batches | lr 0.00006 | ms/batch 21.48 | loss 0.00011691\n",
      "| Epoch  64 |   650/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00008011\n",
      "| Epoch  64 |   700/  823 batches | lr 0.00006 | ms/batch 21.32 | loss 0.00004821\n",
      "| Epoch  64 |   750/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00006300\n",
      "| Epoch  64 |   800/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00009737\n",
      "\n",
      "Val set: Average loss: 0.00007071\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  65 |    50/  823 batches | lr 0.00006 | ms/batch 20.02 | loss 0.00005089\n",
      "| Epoch  65 |   100/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00004709\n",
      "| Epoch  65 |   150/  823 batches | lr 0.00006 | ms/batch 19.89 | loss 0.00004278\n",
      "| Epoch  65 |   200/  823 batches | lr 0.00006 | ms/batch 21.60 | loss 0.00005250\n",
      "| Epoch  65 |   250/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00004762\n",
      "| Epoch  65 |   300/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00004434\n",
      "| Epoch  65 |   350/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00003998\n",
      "| Epoch  65 |   400/  823 batches | lr 0.00006 | ms/batch 21.35 | loss 0.00004890\n",
      "| Epoch  65 |   450/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00005324\n",
      "| Epoch  65 |   500/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00004545\n",
      "| Epoch  65 |   550/  823 batches | lr 0.00006 | ms/batch 19.79 | loss 0.00006104\n",
      "| Epoch  65 |   600/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00004653\n",
      "| Epoch  65 |   650/  823 batches | lr 0.00006 | ms/batch 19.96 | loss 0.00004153\n",
      "| Epoch  65 |   700/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00015985\n",
      "| Epoch  65 |   750/  823 batches | lr 0.00006 | ms/batch 19.52 | loss 0.00005944\n",
      "| Epoch  65 |   800/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00004774\n",
      "\n",
      "Val set: Average loss: 0.00004817\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  66 |    50/  823 batches | lr 0.00006 | ms/batch 19.76 | loss 0.00003919\n",
      "| Epoch  66 |   100/  823 batches | lr 0.00006 | ms/batch 19.47 | loss 0.00004045\n",
      "| Epoch  66 |   150/  823 batches | lr 0.00006 | ms/batch 19.44 | loss 0.00006733\n",
      "| Epoch  66 |   200/  823 batches | lr 0.00006 | ms/batch 21.01 | loss 0.00004910\n",
      "| Epoch  66 |   250/  823 batches | lr 0.00006 | ms/batch 19.53 | loss 0.00006581\n",
      "| Epoch  66 |   300/  823 batches | lr 0.00006 | ms/batch 21.14 | loss 0.00006684\n",
      "| Epoch  66 |   350/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00008776\n",
      "| Epoch  66 |   400/  823 batches | lr 0.00006 | ms/batch 21.18 | loss 0.00008707\n",
      "| Epoch  66 |   450/  823 batches | lr 0.00006 | ms/batch 19.50 | loss 0.00003804\n",
      "| Epoch  66 |   500/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00006075\n",
      "| Epoch  66 |   550/  823 batches | lr 0.00006 | ms/batch 19.52 | loss 0.00007000\n",
      "| Epoch  66 |   600/  823 batches | lr 0.00006 | ms/batch 21.14 | loss 0.00004791\n",
      "| Epoch  66 |   650/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00004598\n",
      "| Epoch  66 |   700/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00011131\n",
      "| Epoch  66 |   750/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00004925\n",
      "| Epoch  66 |   800/  823 batches | lr 0.00006 | ms/batch 21.14 | loss 0.00004341\n",
      "\n",
      "Val set: Average loss: 0.00003625\n",
      "\n",
      "| Epoch  67 |    50/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00003999\n",
      "| Epoch  67 |   100/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00007387\n",
      "| Epoch  67 |   150/  823 batches | lr 0.00006 | ms/batch 19.49 | loss 0.00004582\n",
      "| Epoch  67 |   200/  823 batches | lr 0.00006 | ms/batch 21.08 | loss 0.00009186\n",
      "| Epoch  67 |   250/  823 batches | lr 0.00006 | ms/batch 19.50 | loss 0.00005074\n",
      "| Epoch  67 |   300/  823 batches | lr 0.00006 | ms/batch 21.15 | loss 0.00006685\n",
      "| Epoch  67 |   350/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00007376\n",
      "| Epoch  67 |   400/  823 batches | lr 0.00006 | ms/batch 21.11 | loss 0.00005510\n",
      "| Epoch  67 |   450/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00003647\n",
      "| Epoch  67 |   500/  823 batches | lr 0.00006 | ms/batch 21.04 | loss 0.00006967\n",
      "| Epoch  67 |   550/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00007147\n",
      "| Epoch  67 |   600/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00004355\n",
      "| Epoch  67 |   650/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00004577\n",
      "| Epoch  67 |   700/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00009435\n",
      "| Epoch  67 |   750/  823 batches | lr 0.00006 | ms/batch 19.49 | loss 0.00004575\n",
      "| Epoch  67 |   800/  823 batches | lr 0.00006 | ms/batch 21.15 | loss 0.00004452\n",
      "\n",
      "Val set: Average loss: 0.00003663\n",
      "\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch  68 |    50/  823 batches | lr 0.00006 | ms/batch 19.86 | loss 0.00010749\n",
      "| Epoch  68 |   100/  823 batches | lr 0.00006 | ms/batch 19.37 | loss 0.00006926\n",
      "| Epoch  68 |   150/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00003508\n",
      "| Epoch  68 |   200/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00005993\n",
      "| Epoch  68 |   250/  823 batches | lr 0.00006 | ms/batch 19.54 | loss 0.00008096\n",
      "| Epoch  68 |   300/  823 batches | lr 0.00006 | ms/batch 21.13 | loss 0.00004909\n",
      "| Epoch  68 |   350/  823 batches | lr 0.00006 | ms/batch 19.53 | loss 0.00007920\n",
      "| Epoch  68 |   400/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00008514\n",
      "| Epoch  68 |   450/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00005337\n",
      "| Epoch  68 |   500/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00003513\n",
      "| Epoch  68 |   550/  823 batches | lr 0.00006 | ms/batch 19.42 | loss 0.00007348\n",
      "| Epoch  68 |   600/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00004473\n",
      "| Epoch  68 |   650/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00004791\n",
      "| Epoch  68 |   700/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00006328\n",
      "| Epoch  68 |   750/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00006359\n",
      "| Epoch  68 |   800/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00005546\n",
      "\n",
      "Val set: Average loss: 0.00005782\n",
      "\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch  69 |    50/  823 batches | lr 0.00006 | ms/batch 19.80 | loss 0.00007769\n",
      "| Epoch  69 |   100/  823 batches | lr 0.00006 | ms/batch 19.44 | loss 0.00005064\n",
      "| Epoch  69 |   150/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00005061\n",
      "| Epoch  69 |   200/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00007273\n",
      "| Epoch  69 |   250/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00009281\n",
      "| Epoch  69 |   300/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00006388\n",
      "| Epoch  69 |   350/  823 batches | lr 0.00006 | ms/batch 19.43 | loss 0.00007365\n",
      "| Epoch  69 |   400/  823 batches | lr 0.00006 | ms/batch 21.10 | loss 0.00007084\n",
      "| Epoch  69 |   450/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00005303\n",
      "| Epoch  69 |   500/  823 batches | lr 0.00006 | ms/batch 21.16 | loss 0.00003727\n",
      "| Epoch  69 |   550/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00009537\n",
      "| Epoch  69 |   600/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00011208\n",
      "| Epoch  69 |   650/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00005742\n",
      "| Epoch  69 |   700/  823 batches | lr 0.00006 | ms/batch 21.11 | loss 0.00004179\n",
      "| Epoch  69 |   750/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00005678\n",
      "| Epoch  69 |   800/  823 batches | lr 0.00006 | ms/batch 21.18 | loss 0.00005977\n",
      "\n",
      "Val set: Average loss: 0.00003909\n",
      "\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch  70 |    50/  823 batches | lr 0.00006 | ms/batch 19.91 | loss 0.00004613\n",
      "| Epoch  70 |   100/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00004823\n",
      "| Epoch  70 |   150/  823 batches | lr 0.00006 | ms/batch 19.53 | loss 0.00003895\n",
      "| Epoch  70 |   200/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00006664\n",
      "| Epoch  70 |   250/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00013345\n",
      "| Epoch  70 |   300/  823 batches | lr 0.00006 | ms/batch 21.11 | loss 0.00009631\n",
      "| Epoch  70 |   350/  823 batches | lr 0.00006 | ms/batch 19.49 | loss 0.00009971\n",
      "| Epoch  70 |   400/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00004384\n",
      "| Epoch  70 |   450/  823 batches | lr 0.00006 | ms/batch 19.48 | loss 0.00004313\n",
      "| Epoch  70 |   500/  823 batches | lr 0.00006 | ms/batch 21.43 | loss 0.00004274\n",
      "| Epoch  70 |   550/  823 batches | lr 0.00006 | ms/batch 19.52 | loss 0.00006132\n",
      "| Epoch  70 |   600/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00006926\n",
      "| Epoch  70 |   650/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00004770\n",
      "| Epoch  70 |   700/  823 batches | lr 0.00006 | ms/batch 21.37 | loss 0.00003665\n",
      "| Epoch  70 |   750/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00004115\n",
      "| Epoch  70 |   800/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00004413\n",
      "\n",
      "Val set: Average loss: 0.00004275\n",
      "\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch  71 |    50/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00004022\n",
      "| Epoch  71 |   100/  823 batches | lr 0.00006 | ms/batch 19.54 | loss 0.00004180\n",
      "| Epoch  71 |   150/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00007814\n",
      "| Epoch  71 |   200/  823 batches | lr 0.00006 | ms/batch 21.35 | loss 0.00006463\n",
      "| Epoch  71 |   250/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00008592\n",
      "| Epoch  71 |   300/  823 batches | lr 0.00006 | ms/batch 21.42 | loss 0.00004527\n",
      "| Epoch  71 |   350/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00003994\n",
      "| Epoch  71 |   400/  823 batches | lr 0.00006 | ms/batch 21.12 | loss 0.00004235\n",
      "| Epoch  71 |   450/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00009295\n",
      "| Epoch  71 |   500/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00003550\n",
      "| Epoch  71 |   550/  823 batches | lr 0.00006 | ms/batch 19.76 | loss 0.00003952\n",
      "| Epoch  71 |   600/  823 batches | lr 0.00006 | ms/batch 21.42 | loss 0.00007755\n",
      "| Epoch  71 |   650/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00006013\n",
      "| Epoch  71 |   700/  823 batches | lr 0.00006 | ms/batch 21.14 | loss 0.00003522\n",
      "| Epoch  71 |   750/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00007389\n",
      "| Epoch  71 |   800/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00004520\n",
      "\n",
      "Val set: Average loss: 0.00003930\n",
      "\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch  72 |    50/  823 batches | lr 0.00006 | ms/batch 19.96 | loss 0.00005702\n",
      "| Epoch  72 |   100/  823 batches | lr 0.00006 | ms/batch 19.54 | loss 0.00006688\n",
      "| Epoch  72 |   150/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00004549\n",
      "| Epoch  72 |   200/  823 batches | lr 0.00006 | ms/batch 21.13 | loss 0.00005083\n",
      "| Epoch  72 |   250/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00005184\n",
      "| Epoch  72 |   300/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00008310\n",
      "| Epoch  72 |   350/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00006238\n",
      "| Epoch  72 |   400/  823 batches | lr 0.00006 | ms/batch 21.40 | loss 0.00004385\n",
      "| Epoch  72 |   450/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00011353\n",
      "| Epoch  72 |   500/  823 batches | lr 0.00006 | ms/batch 21.49 | loss 0.00004717\n",
      "| Epoch  72 |   550/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00006474\n",
      "| Epoch  72 |   600/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00005809\n",
      "| Epoch  72 |   650/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00005871\n",
      "| Epoch  72 |   700/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00004338\n",
      "| Epoch  72 |   750/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00005899\n",
      "| Epoch  72 |   800/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00004181\n",
      "\n",
      "Val set: Average loss: 0.00004761\n",
      "\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch  73 |    50/  823 batches | lr 0.00006 | ms/batch 19.80 | loss 0.00006321\n",
      "| Epoch  73 |   100/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00009746\n",
      "| Epoch  73 |   150/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00006784\n",
      "| Epoch  73 |   200/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00004533\n",
      "| Epoch  73 |   250/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00007125\n",
      "| Epoch  73 |   300/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00005508\n",
      "| Epoch  73 |   350/  823 batches | lr 0.00006 | ms/batch 19.76 | loss 0.00005941\n",
      "| Epoch  73 |   400/  823 batches | lr 0.00006 | ms/batch 21.39 | loss 0.00006076\n",
      "| Epoch  73 |   450/  823 batches | lr 0.00006 | ms/batch 20.02 | loss 0.00005029\n",
      "| Epoch  73 |   500/  823 batches | lr 0.00006 | ms/batch 21.47 | loss 0.00003884\n",
      "| Epoch  73 |   550/  823 batches | lr 0.00006 | ms/batch 19.64 | loss 0.00004073\n",
      "| Epoch  73 |   600/  823 batches | lr 0.00006 | ms/batch 21.40 | loss 0.00006536\n",
      "| Epoch  73 |   650/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00004248\n",
      "| Epoch  73 |   700/  823 batches | lr 0.00006 | ms/batch 21.12 | loss 0.00003592\n",
      "| Epoch  73 |   750/  823 batches | lr 0.00006 | ms/batch 19.86 | loss 0.00005579\n",
      "| Epoch  73 |   800/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00008062\n",
      "\n",
      "Val set: Average loss: 0.00005480\n",
      "\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch  74 |    50/  823 batches | lr 0.00006 | ms/batch 19.89 | loss 0.00006291\n",
      "| Epoch  74 |   100/  823 batches | lr 0.00006 | ms/batch 19.40 | loss 0.00008258\n",
      "| Epoch  74 |   150/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00007724\n",
      "| Epoch  74 |   200/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00004106\n",
      "| Epoch  74 |   250/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00007277\n",
      "| Epoch  74 |   300/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00006299\n",
      "| Epoch  74 |   350/  823 batches | lr 0.00006 | ms/batch 19.76 | loss 0.00006627\n",
      "| Epoch  74 |   400/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00006290\n",
      "| Epoch  74 |   450/  823 batches | lr 0.00006 | ms/batch 19.78 | loss 0.00005191\n",
      "| Epoch  74 |   500/  823 batches | lr 0.00006 | ms/batch 21.40 | loss 0.00004071\n",
      "| Epoch  74 |   550/  823 batches | lr 0.00006 | ms/batch 19.54 | loss 0.00004590\n",
      "| Epoch  74 |   600/  823 batches | lr 0.00006 | ms/batch 21.40 | loss 0.00006202\n",
      "| Epoch  74 |   650/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00008740\n",
      "| Epoch  74 |   700/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00004534\n",
      "| Epoch  74 |   750/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00004300\n",
      "| Epoch  74 |   800/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00004735\n",
      "\n",
      "Val set: Average loss: 0.00003960\n",
      "\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch  75 |    50/  823 batches | lr 0.00006 | ms/batch 19.79 | loss 0.00005114\n",
      "| Epoch  75 |   100/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00004202\n",
      "| Epoch  75 |   150/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00007165\n",
      "| Epoch  75 |   200/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00006649\n",
      "| Epoch  75 |   250/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00004580\n",
      "| Epoch  75 |   300/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00003965\n",
      "| Epoch  75 |   350/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00006902\n",
      "| Epoch  75 |   400/  823 batches | lr 0.00006 | ms/batch 21.16 | loss 0.00004737\n",
      "| Epoch  75 |   450/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00006397\n",
      "| Epoch  75 |   500/  823 batches | lr 0.00006 | ms/batch 21.18 | loss 0.00004044\n",
      "| Epoch  75 |   550/  823 batches | lr 0.00006 | ms/batch 19.49 | loss 0.00009934\n",
      "| Epoch  75 |   600/  823 batches | lr 0.00006 | ms/batch 21.20 | loss 0.00012958\n",
      "| Epoch  75 |   650/  823 batches | lr 0.00006 | ms/batch 20.19 | loss 0.00004557\n",
      "| Epoch  75 |   700/  823 batches | lr 0.00006 | ms/batch 21.36 | loss 0.00004172\n",
      "| Epoch  75 |   750/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00004910\n",
      "| Epoch  75 |   800/  823 batches | lr 0.00006 | ms/batch 21.19 | loss 0.00008001\n",
      "\n",
      "Val set: Average loss: 0.00004696\n",
      "\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch  76 |    50/  823 batches | lr 0.00006 | ms/batch 19.90 | loss 0.00006234\n",
      "| Epoch  76 |   100/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00005785\n",
      "| Epoch  76 |   150/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00007119\n",
      "| Epoch  76 |   200/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00008030\n",
      "| Epoch  76 |   250/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00006694\n",
      "| Epoch  76 |   300/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00010105\n",
      "| Epoch  76 |   350/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00009089\n",
      "| Epoch  76 |   400/  823 batches | lr 0.00006 | ms/batch 21.18 | loss 0.00004318\n",
      "| Epoch  76 |   450/  823 batches | lr 0.00006 | ms/batch 19.47 | loss 0.00003871\n",
      "| Epoch  76 |   500/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00004554\n",
      "| Epoch  76 |   550/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00005116\n",
      "| Epoch  76 |   600/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00009095\n",
      "| Epoch  76 |   650/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00006767\n",
      "| Epoch  76 |   700/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00004155\n",
      "| Epoch  76 |   750/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00007183\n",
      "| Epoch  76 |   800/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00009917\n",
      "\n",
      "Val set: Average loss: 0.00005402\n",
      "\n",
      "EarlyStopping counter: 10 out of 20\n",
      "| Epoch  77 |    50/  823 batches | lr 0.00006 | ms/batch 19.86 | loss 0.00005125\n",
      "| Epoch  77 |   100/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00004612\n",
      "| Epoch  77 |   150/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00004981\n",
      "| Epoch  77 |   200/  823 batches | lr 0.00006 | ms/batch 21.35 | loss 0.00007017\n",
      "| Epoch  77 |   250/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00006886\n",
      "| Epoch  77 |   300/  823 batches | lr 0.00006 | ms/batch 21.47 | loss 0.00007026\n",
      "| Epoch  77 |   350/  823 batches | lr 0.00006 | ms/batch 19.84 | loss 0.00007930\n",
      "| Epoch  77 |   400/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00004026\n",
      "| Epoch  77 |   450/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00003433\n",
      "| Epoch  77 |   500/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00003687\n",
      "| Epoch  77 |   550/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00004340\n",
      "| Epoch  77 |   600/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00005253\n",
      "| Epoch  77 |   650/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00003567\n",
      "| Epoch  77 |   700/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00003360\n",
      "| Epoch  77 |   750/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00003678\n",
      "| Epoch  77 |   800/  823 batches | lr 0.00006 | ms/batch 21.14 | loss 0.00004101\n",
      "\n",
      "Val set: Average loss: 0.00004539\n",
      "\n",
      "EarlyStopping counter: 11 out of 20\n",
      "| Epoch  78 |    50/  823 batches | lr 0.00006 | ms/batch 19.92 | loss 0.00006203\n",
      "| Epoch  78 |   100/  823 batches | lr 0.00006 | ms/batch 19.62 | loss 0.00006682\n",
      "| Epoch  78 |   150/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00005343\n",
      "| Epoch  78 |   200/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00004698\n",
      "| Epoch  78 |   250/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00003481\n",
      "| Epoch  78 |   300/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00014016\n",
      "| Epoch  78 |   350/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00009401\n",
      "| Epoch  78 |   400/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00006392\n",
      "| Epoch  78 |   450/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00003457\n",
      "| Epoch  78 |   500/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00003159\n",
      "| Epoch  78 |   550/  823 batches | lr 0.00006 | ms/batch 19.82 | loss 0.00003245\n",
      "| Epoch  78 |   600/  823 batches | lr 0.00006 | ms/batch 21.38 | loss 0.00003650\n",
      "| Epoch  78 |   650/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00004209\n",
      "| Epoch  78 |   700/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00003993\n",
      "| Epoch  78 |   750/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00012672\n",
      "| Epoch  78 |   800/  823 batches | lr 0.00006 | ms/batch 21.15 | loss 0.00007499\n",
      "\n",
      "Val set: Average loss: 0.00006413\n",
      "\n",
      "EarlyStopping counter: 12 out of 20\n",
      "| Epoch  79 |    50/  823 batches | lr 0.00006 | ms/batch 19.99 | loss 0.00004606\n",
      "| Epoch  79 |   100/  823 batches | lr 0.00006 | ms/batch 19.58 | loss 0.00004256\n",
      "| Epoch  79 |   150/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00004280\n",
      "| Epoch  79 |   200/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00006811\n",
      "| Epoch  79 |   250/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00004196\n",
      "| Epoch  79 |   300/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00006840\n",
      "| Epoch  79 |   350/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00004545\n",
      "| Epoch  79 |   400/  823 batches | lr 0.00006 | ms/batch 21.15 | loss 0.00003440\n",
      "| Epoch  79 |   450/  823 batches | lr 0.00006 | ms/batch 19.74 | loss 0.00003499\n",
      "| Epoch  79 |   500/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00005573\n",
      "| Epoch  79 |   550/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00010941\n",
      "| Epoch  79 |   600/  823 batches | lr 0.00006 | ms/batch 21.41 | loss 0.00005077\n",
      "| Epoch  79 |   650/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00003974\n",
      "| Epoch  79 |   700/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00004886\n",
      "| Epoch  79 |   750/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00006225\n",
      "| Epoch  79 |   800/  823 batches | lr 0.00006 | ms/batch 21.37 | loss 0.00004559\n",
      "\n",
      "Val set: Average loss: 0.00004829\n",
      "\n",
      "EarlyStopping counter: 13 out of 20\n",
      "| Epoch  80 |    50/  823 batches | lr 0.00006 | ms/batch 19.83 | loss 0.00005138\n",
      "| Epoch  80 |   100/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00003862\n",
      "| Epoch  80 |   150/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00006745\n",
      "| Epoch  80 |   200/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00006222\n",
      "| Epoch  80 |   250/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00004160\n",
      "| Epoch  80 |   300/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00004319\n",
      "| Epoch  80 |   350/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00007722\n",
      "| Epoch  80 |   400/  823 batches | lr 0.00006 | ms/batch 21.43 | loss 0.00010360\n",
      "| Epoch  80 |   450/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00005303\n",
      "| Epoch  80 |   500/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00004019\n",
      "| Epoch  80 |   550/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00005072\n",
      "| Epoch  80 |   600/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00005284\n",
      "| Epoch  80 |   650/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00004140\n",
      "| Epoch  80 |   700/  823 batches | lr 0.00006 | ms/batch 21.21 | loss 0.00003434\n",
      "| Epoch  80 |   750/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00005219\n",
      "| Epoch  80 |   800/  823 batches | lr 0.00006 | ms/batch 21.25 | loss 0.00006374\n",
      "\n",
      "Val set: Average loss: 0.00004845\n",
      "\n",
      "EarlyStopping counter: 14 out of 20\n",
      "| Epoch  81 |    50/  823 batches | lr 0.00006 | ms/batch 20.02 | loss 0.00006470\n",
      "| Epoch  81 |   100/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00004659\n",
      "| Epoch  81 |   150/  823 batches | lr 0.00006 | ms/batch 19.79 | loss 0.00004743\n",
      "| Epoch  81 |   200/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00006132\n",
      "| Epoch  81 |   250/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00007904\n",
      "| Epoch  81 |   300/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00007632\n",
      "| Epoch  81 |   350/  823 batches | lr 0.00006 | ms/batch 19.78 | loss 0.00004897\n",
      "| Epoch  81 |   400/  823 batches | lr 0.00006 | ms/batch 21.48 | loss 0.00004397\n",
      "| Epoch  81 |   450/  823 batches | lr 0.00006 | ms/batch 19.79 | loss 0.00004440\n",
      "| Epoch  81 |   500/  823 batches | lr 0.00006 | ms/batch 21.45 | loss 0.00004166\n",
      "| Epoch  81 |   550/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00005596\n",
      "| Epoch  81 |   600/  823 batches | lr 0.00006 | ms/batch 21.35 | loss 0.00005002\n",
      "| Epoch  81 |   650/  823 batches | lr 0.00006 | ms/batch 20.00 | loss 0.00003810\n",
      "| Epoch  81 |   700/  823 batches | lr 0.00006 | ms/batch 21.48 | loss 0.00007955\n",
      "| Epoch  81 |   750/  823 batches | lr 0.00006 | ms/batch 19.86 | loss 0.00008738\n",
      "| Epoch  81 |   800/  823 batches | lr 0.00006 | ms/batch 21.45 | loss 0.00006775\n",
      "\n",
      "Val set: Average loss: 0.00004449\n",
      "\n",
      "EarlyStopping counter: 15 out of 20\n",
      "| Epoch  82 |    50/  823 batches | lr 0.00006 | ms/batch 20.18 | loss 0.00004850\n",
      "| Epoch  82 |   100/  823 batches | lr 0.00006 | ms/batch 19.90 | loss 0.00004458\n",
      "| Epoch  82 |   150/  823 batches | lr 0.00006 | ms/batch 20.03 | loss 0.00004902\n",
      "| Epoch  82 |   200/  823 batches | lr 0.00006 | ms/batch 21.69 | loss 0.00008745\n",
      "| Epoch  82 |   250/  823 batches | lr 0.00006 | ms/batch 20.12 | loss 0.00005658\n",
      "| Epoch  82 |   300/  823 batches | lr 0.00006 | ms/batch 21.61 | loss 0.00006958\n",
      "| Epoch  82 |   350/  823 batches | lr 0.00006 | ms/batch 19.82 | loss 0.00009993\n",
      "| Epoch  82 |   400/  823 batches | lr 0.00006 | ms/batch 21.45 | loss 0.00005503\n",
      "| Epoch  82 |   450/  823 batches | lr 0.00006 | ms/batch 19.78 | loss 0.00003550\n",
      "| Epoch  82 |   500/  823 batches | lr 0.00006 | ms/batch 21.50 | loss 0.00003455\n",
      "| Epoch  82 |   550/  823 batches | lr 0.00006 | ms/batch 19.81 | loss 0.00006384\n",
      "| Epoch  82 |   600/  823 batches | lr 0.00006 | ms/batch 21.77 | loss 0.00006504\n",
      "| Epoch  82 |   650/  823 batches | lr 0.00006 | ms/batch 19.87 | loss 0.00005744\n",
      "| Epoch  82 |   700/  823 batches | lr 0.00006 | ms/batch 21.16 | loss 0.00003970\n",
      "| Epoch  82 |   750/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00007635\n",
      "| Epoch  82 |   800/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00007578\n",
      "\n",
      "Val set: Average loss: 0.00005730\n",
      "\n",
      "EarlyStopping counter: 16 out of 20\n",
      "| Epoch  83 |    50/  823 batches | lr 0.00006 | ms/batch 19.82 | loss 0.00006979\n",
      "| Epoch  83 |   100/  823 batches | lr 0.00006 | ms/batch 19.45 | loss 0.00004427\n",
      "| Epoch  83 |   150/  823 batches | lr 0.00006 | ms/batch 19.70 | loss 0.00004151\n",
      "| Epoch  83 |   200/  823 batches | lr 0.00006 | ms/batch 21.42 | loss 0.00005888\n",
      "| Epoch  83 |   250/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00004930\n",
      "| Epoch  83 |   300/  823 batches | lr 0.00006 | ms/batch 21.37 | loss 0.00009566\n",
      "| Epoch  83 |   350/  823 batches | lr 0.00006 | ms/batch 19.72 | loss 0.00010627\n",
      "| Epoch  83 |   400/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00005013\n",
      "| Epoch  83 |   450/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00003538\n",
      "| Epoch  83 |   500/  823 batches | lr 0.00006 | ms/batch 21.28 | loss 0.00004884\n",
      "| Epoch  83 |   550/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00006709\n",
      "| Epoch  83 |   600/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00006558\n",
      "| Epoch  83 |   650/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00004909\n",
      "| Epoch  83 |   700/  823 batches | lr 0.00006 | ms/batch 21.29 | loss 0.00004245\n",
      "| Epoch  83 |   750/  823 batches | lr 0.00006 | ms/batch 19.90 | loss 0.00008082\n",
      "| Epoch  83 |   800/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00005790\n",
      "\n",
      "Val set: Average loss: 0.00005707\n",
      "\n",
      "EarlyStopping counter: 17 out of 20\n",
      "| Epoch  84 |    50/  823 batches | lr 0.00006 | ms/batch 20.01 | loss 0.00004634\n",
      "| Epoch  84 |   100/  823 batches | lr 0.00006 | ms/batch 19.61 | loss 0.00003933\n",
      "| Epoch  84 |   150/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00003835\n",
      "| Epoch  84 |   200/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00004679\n",
      "| Epoch  84 |   250/  823 batches | lr 0.00006 | ms/batch 19.60 | loss 0.00004136\n",
      "| Epoch  84 |   300/  823 batches | lr 0.00006 | ms/batch 21.33 | loss 0.00004040\n",
      "| Epoch  84 |   350/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00003337\n",
      "| Epoch  84 |   400/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00003295\n",
      "| Epoch  84 |   450/  823 batches | lr 0.00006 | ms/batch 19.56 | loss 0.00003692\n",
      "| Epoch  84 |   500/  823 batches | lr 0.00006 | ms/batch 21.37 | loss 0.00003177\n",
      "| Epoch  84 |   550/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00006362\n",
      "| Epoch  84 |   600/  823 batches | lr 0.00006 | ms/batch 21.32 | loss 0.00004699\n",
      "| Epoch  84 |   650/  823 batches | lr 0.00006 | ms/batch 19.71 | loss 0.00006175\n",
      "| Epoch  84 |   700/  823 batches | lr 0.00006 | ms/batch 21.30 | loss 0.00006025\n",
      "| Epoch  84 |   750/  823 batches | lr 0.00006 | ms/batch 19.67 | loss 0.00003479\n",
      "| Epoch  84 |   800/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00004288\n",
      "\n",
      "Val set: Average loss: 0.00012568\n",
      "\n",
      "EarlyStopping counter: 18 out of 20\n",
      "| Epoch  85 |    50/  823 batches | lr 0.00006 | ms/batch 19.83 | loss 0.00006622\n",
      "| Epoch  85 |   100/  823 batches | lr 0.00006 | ms/batch 19.78 | loss 0.00004936\n",
      "| Epoch  85 |   150/  823 batches | lr 0.00006 | ms/batch 19.75 | loss 0.00003293\n",
      "| Epoch  85 |   200/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00003581\n",
      "| Epoch  85 |   250/  823 batches | lr 0.00006 | ms/batch 19.59 | loss 0.00009202\n",
      "| Epoch  85 |   300/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00003949\n",
      "| Epoch  85 |   350/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00003648\n",
      "| Epoch  85 |   400/  823 batches | lr 0.00006 | ms/batch 21.34 | loss 0.00003770\n",
      "| Epoch  85 |   450/  823 batches | lr 0.00006 | ms/batch 19.65 | loss 0.00007364\n",
      "| Epoch  85 |   500/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00006089\n",
      "| Epoch  85 |   550/  823 batches | lr 0.00006 | ms/batch 19.73 | loss 0.00003681\n",
      "| Epoch  85 |   600/  823 batches | lr 0.00006 | ms/batch 21.27 | loss 0.00010787\n",
      "| Epoch  85 |   650/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00005652\n",
      "| Epoch  85 |   700/  823 batches | lr 0.00006 | ms/batch 21.36 | loss 0.00004536\n",
      "| Epoch  85 |   750/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00004244\n",
      "| Epoch  85 |   800/  823 batches | lr 0.00006 | ms/batch 21.17 | loss 0.00004909\n",
      "\n",
      "Val set: Average loss: 0.00007710\n",
      "\n",
      "EarlyStopping counter: 19 out of 20\n",
      "| Epoch  86 |    50/  823 batches | lr 0.00006 | ms/batch 19.92 | loss 0.00004598\n",
      "| Epoch  86 |   100/  823 batches | lr 0.00006 | ms/batch 19.63 | loss 0.00004087\n",
      "| Epoch  86 |   150/  823 batches | lr 0.00006 | ms/batch 19.77 | loss 0.00005759\n",
      "| Epoch  86 |   200/  823 batches | lr 0.00006 | ms/batch 21.26 | loss 0.00008801\n",
      "| Epoch  86 |   250/  823 batches | lr 0.00006 | ms/batch 19.69 | loss 0.00005351\n",
      "| Epoch  86 |   300/  823 batches | lr 0.00006 | ms/batch 21.31 | loss 0.00003537\n",
      "| Epoch  86 |   350/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00003218\n",
      "| Epoch  86 |   400/  823 batches | lr 0.00006 | ms/batch 21.24 | loss 0.00004930\n",
      "| Epoch  86 |   450/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00004944\n",
      "| Epoch  86 |   500/  823 batches | lr 0.00006 | ms/batch 21.32 | loss 0.00006932\n",
      "| Epoch  86 |   550/  823 batches | lr 0.00006 | ms/batch 19.68 | loss 0.00007926\n",
      "| Epoch  86 |   600/  823 batches | lr 0.00006 | ms/batch 21.45 | loss 0.00005203\n",
      "| Epoch  86 |   650/  823 batches | lr 0.00006 | ms/batch 19.66 | loss 0.00003713\n",
      "| Epoch  86 |   700/  823 batches | lr 0.00006 | ms/batch 21.22 | loss 0.00003472\n",
      "| Epoch  86 |   750/  823 batches | lr 0.00006 | ms/batch 19.55 | loss 0.00003779\n",
      "| Epoch  86 |   800/  823 batches | lr 0.00006 | ms/batch 21.23 | loss 0.00004038\n",
      "\n",
      "Val set: Average loss: 0.00006382\n",
      "\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Stopping at Epoch: 86\n"
     ]
    }
   ],
   "source": [
    "load = False\n",
    "save_model_path = '../models/final_heston_model.chkpt'\n",
    "val_err_df_path = '../results/val_final_heston_model.csv'\n",
    "\n",
    "if not load:\n",
    "  train_losses, val_losses = train(\n",
    "      epochs,\n",
    "      batch_size,\n",
    "      model,\n",
    "      optimizer,\n",
    "      loss_fn,\n",
    "      X_train,\n",
    "      y_train,\n",
    "      X_val,\n",
    "      y_val)\n",
    "  val_err_df = pd.DataFrame({\n",
    "      'Training': train_losses,\n",
    "      'Validation': val_losses})\n",
    "  val_err_df.to_csv(val_err_df_path)\n",
    "  torch.save(model.state_dict(), save_model_path)\n",
    "else:\n",
    "  model = Net(input_size, output_size, hidden_size, num_layers, act_fn)\n",
    "  model.load_state_dict(torch.load(save_model_path, map_location=device))\n",
    "  model = model.to(device)\n",
    "  val_err_df = pd.read_csv(val_err_df_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650894219921,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "KkenWSRYvDpl",
    "outputId": "37746aa5-0b46-4a80-d14d-397868e89361",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 864x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "val_err_df[1:].plot(xlabel='epoch', ylabel='MSE', ax=ax)\n",
    "plt.title('Losses during training')\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlG5Ky25vDpm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1650894219922,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "TBZ4eHAdvDpn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 30\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_out = model(X_test[0:test_size].to(device))\n",
    "\n",
    "test_out = output_sc.inverse_transform(test_out.cpu().detach().numpy())\n",
    "real_out = output_sc.inverse_transform(y_test[0:test_size].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1650894219926,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "wK1WeGIZvDpo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['Price', 'Strike', 'Kappa', 'Rho', 'Theta', 'Xi', 'V_0',\n",
    "       'Interest Rate', 'Time to Expiration', 'C', 'P']\n",
    "test_options = pd.DataFrame(input_sc.inverse_transform(X_test[0:test_size].detach().cpu().numpy()), columns=cols)\n",
    "test_options['Prediction'] = test_out\n",
    "test_options['Real'] = real_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1650894220545,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "TNw5-1jgvDpr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_options['Moneyness'] = test_options.Price / test_options.Strike\n",
    "test_options['Abs Error'] = np.abs(test_options.Prediction - test_options.Real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1650894220546,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "lAC6AdeVvDpr",
    "outputId": "78bd13cc-c5a0-4746-f402-9e84d8b78696",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         Price      Strike     Kappa       Rho     Theta        Xi       V_0  \\\n8   100.000237   79.008804  1.810506 -0.489986  0.033022  0.424320  0.236206   \n27  100.000237  123.994934  0.853535 -0.363494  0.375735  0.181882  0.495855   \n9   100.000237  148.018738  0.100272 -0.180037  0.220708  0.111194  0.437267   \n7    98.001068   53.015518  1.452399 -0.810086  0.039505  0.261969  0.259040   \n24   99.000420   81.992279  1.697242 -0.454589  0.002021  0.099283  0.378426   \n22  101.000053  100.994873  1.306733 -0.282550  0.063884  0.056293  0.345962   \n5   119.000381  177.970490  1.700059 -0.272527  0.273446  0.499447  0.381089   \n19  100.000237   92.999939  1.198259 -0.110510  0.057824  0.484365  0.309789   \n18  100.000237   79.008804  1.886015 -0.080568  0.268567  0.037546  0.365491   \n11  100.000237   83.006271  1.107394 -0.298662  0.219369  0.441657  0.458826   \n28  110.998192  179.998474  0.894530 -0.789025  0.498756  0.010624  0.216169   \n26  100.000237   62.024445  0.214663 -0.574230  0.423929  0.022183  0.388191   \n3   115.996803   91.001205  0.809019 -0.214292  0.486074  0.305426  0.465927   \n17  101.000053  124.999176  0.079422 -0.249436  0.191908  0.210460  0.278809   \n14  101.000053  160.030640  0.441473 -0.495600  0.282729  0.419668  0.053532   \n12  111.999382  157.027664  0.162257 -0.346176  0.005966  0.096042  0.340065   \n1   100.000237   90.006714  1.781204 -0.265676  0.151816  0.362019  0.332520   \n0    91.997589  165.022598  1.689353 -0.744619  0.207761  0.196682  0.179774   \n10   86.998978  173.017532  1.793037 -0.898135  0.433934  0.209367  0.486851   \n21  100.000237   83.006271  1.073231 -0.038701  0.463104  0.392042  0.085235   \n25  100.000237  112.000099  1.618351 -0.427248  0.311758  0.265387  0.082825   \n15  100.000237   57.032486  0.881358 -0.379417  0.128283  0.063763  0.155870   \n29  100.000237  150.007721  1.003907 -0.005206  0.144630  0.167716  0.423445   \n16  100.000237   98.996140  1.843753 -0.362289  0.378694  0.173918  0.479750   \n4   100.000237   64.013428  0.520082 -0.871746  0.435766  0.169760  0.252438   \n23   89.995193   54.029510  0.506840 -0.728633  0.369394  0.112181  0.247809   \n2   119.000381  118.998100  0.743793 -0.631957  0.020763  0.349615  0.302988   \n13  100.000237   62.024445  0.899884 -0.772531  0.293228  0.100975  0.173560   \n6   100.000237  101.999115  1.182622 -0.249436  0.069802  0.078282  0.147880   \n20  100.000237   80.997787  0.331308 -0.006729  0.322010  0.444758  0.355917   \n\n    Interest Rate  Time to Expiration         C         P  Prediction  \\\n8        0.034151            0.187983 -0.000003  1.000003    0.243559   \n27       0.065431            0.112466 -0.000003  1.000003   20.824448   \n9        0.079532            0.365224 -0.000003  1.000003   44.848103   \n7        0.057435            0.341413  0.999997  0.000003   42.796741   \n24       0.062373            0.476528  0.999997  0.000003   15.589758   \n22       0.093031            0.107676  0.999997  0.000003    2.806886   \n5        0.035153            0.197282  0.999997  0.000003    0.218376   \n19       0.062862            0.422144  0.999997  0.000003    6.954056   \n18       0.098157            0.492871 -0.000003  1.000003    0.118558   \n11       0.067987            1.057843 -0.000003  1.000003    0.553665   \n28       0.059782            0.159805 -0.000003  1.000003   65.100052   \n26       0.087424            0.768031 -0.000003  1.000003    0.111786   \n3        0.075435            0.799731 -0.000003  1.000003    0.440553   \n17       0.081613            0.361983 -0.000003  1.000003   22.788265   \n14       0.067258            0.938086  0.999997  0.000003    0.123036   \n12       0.040101            0.373818  0.999997  0.000003    0.572855   \n1        0.010186            0.874121 -0.000003  1.000003    5.352197   \n0        0.091636            0.966828 -0.000003  1.000003   66.905067   \n10       0.027186            0.135572  0.999997  0.000003    0.165599   \n21       0.053101            0.103731 -0.000003  1.000003    0.208346   \n25       0.031588            0.560076 -0.000003  1.000003   11.251267   \n15       0.052979            0.476809 -0.000003  1.000003    0.173993   \n29       0.069522            0.455535  0.999997  0.000003    0.328752   \n16       0.090164            0.920052  0.999997  0.000003    4.512918   \n4        0.041319            0.708434  0.999997  0.000003   34.977501   \n23       0.053009            0.530735  0.999997  0.000003   34.839134   \n2        0.066769            0.478711  0.999997  0.000003    4.502014   \n13       0.062805            0.999514  0.999997  0.000003   35.585201   \n6        0.025334            0.582037  0.999997  0.000003    2.852960   \n20       0.016986            0.419467  0.999997  0.000003   20.427147   \n\n         Real  Moneyness  Abs Error  \n8    0.241661   1.265685   0.001898  \n27  20.829142   0.806486   0.004694  \n9   44.842133   0.675592   0.005970  \n7   42.787884   1.848536   0.008858  \n24  15.578036   1.207436   0.011722  \n22   2.761820   1.000051   0.045066  \n5    0.156950   0.668652   0.061426  \n19   6.880905   1.075272   0.073152  \n18   0.029883   1.265685   0.088675  \n11   0.464028   1.204731   0.089637  \n28  65.003410   0.616662   0.096642  \n26   0.008705   1.612271   0.103081  \n3    0.336961   1.274673   0.103592  \n17  22.674259   0.808006   0.114006  \n14  -0.001884   0.631130   0.124920  \n12   0.432261   0.713246   0.140594  \n1    5.499053   1.111031   0.146855  \n0   66.739990   0.557485   0.165077  \n10  -0.001884   0.502833   0.167483  \n21   0.040472   1.204731   0.167874  \n25  11.079407   0.892858   0.171861  \n15  -0.001884   1.753391   0.175877  \n29   0.104005   0.666634   0.224747  \n16   4.286623   1.010143   0.226295  \n4   34.719139   1.562176   0.258362  \n23  34.560307   1.665667   0.278828  \n2    4.794890   1.000019   0.292876  \n13  35.280350   1.612271   0.304852  \n6    3.164198   0.980403   0.311238  \n20  19.984676   1.234605   0.442471  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>Strike</th>\n      <th>Kappa</th>\n      <th>Rho</th>\n      <th>Theta</th>\n      <th>Xi</th>\n      <th>V_0</th>\n      <th>Interest Rate</th>\n      <th>Time to Expiration</th>\n      <th>C</th>\n      <th>P</th>\n      <th>Prediction</th>\n      <th>Real</th>\n      <th>Moneyness</th>\n      <th>Abs Error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>100.000237</td>\n      <td>79.008804</td>\n      <td>1.810506</td>\n      <td>-0.489986</td>\n      <td>0.033022</td>\n      <td>0.424320</td>\n      <td>0.236206</td>\n      <td>0.034151</td>\n      <td>0.187983</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.243559</td>\n      <td>0.241661</td>\n      <td>1.265685</td>\n      <td>0.001898</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>100.000237</td>\n      <td>123.994934</td>\n      <td>0.853535</td>\n      <td>-0.363494</td>\n      <td>0.375735</td>\n      <td>0.181882</td>\n      <td>0.495855</td>\n      <td>0.065431</td>\n      <td>0.112466</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>20.824448</td>\n      <td>20.829142</td>\n      <td>0.806486</td>\n      <td>0.004694</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>100.000237</td>\n      <td>148.018738</td>\n      <td>0.100272</td>\n      <td>-0.180037</td>\n      <td>0.220708</td>\n      <td>0.111194</td>\n      <td>0.437267</td>\n      <td>0.079532</td>\n      <td>0.365224</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>44.848103</td>\n      <td>44.842133</td>\n      <td>0.675592</td>\n      <td>0.005970</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>98.001068</td>\n      <td>53.015518</td>\n      <td>1.452399</td>\n      <td>-0.810086</td>\n      <td>0.039505</td>\n      <td>0.261969</td>\n      <td>0.259040</td>\n      <td>0.057435</td>\n      <td>0.341413</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>42.796741</td>\n      <td>42.787884</td>\n      <td>1.848536</td>\n      <td>0.008858</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>99.000420</td>\n      <td>81.992279</td>\n      <td>1.697242</td>\n      <td>-0.454589</td>\n      <td>0.002021</td>\n      <td>0.099283</td>\n      <td>0.378426</td>\n      <td>0.062373</td>\n      <td>0.476528</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>15.589758</td>\n      <td>15.578036</td>\n      <td>1.207436</td>\n      <td>0.011722</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>101.000053</td>\n      <td>100.994873</td>\n      <td>1.306733</td>\n      <td>-0.282550</td>\n      <td>0.063884</td>\n      <td>0.056293</td>\n      <td>0.345962</td>\n      <td>0.093031</td>\n      <td>0.107676</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>2.806886</td>\n      <td>2.761820</td>\n      <td>1.000051</td>\n      <td>0.045066</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>119.000381</td>\n      <td>177.970490</td>\n      <td>1.700059</td>\n      <td>-0.272527</td>\n      <td>0.273446</td>\n      <td>0.499447</td>\n      <td>0.381089</td>\n      <td>0.035153</td>\n      <td>0.197282</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>0.218376</td>\n      <td>0.156950</td>\n      <td>0.668652</td>\n      <td>0.061426</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>100.000237</td>\n      <td>92.999939</td>\n      <td>1.198259</td>\n      <td>-0.110510</td>\n      <td>0.057824</td>\n      <td>0.484365</td>\n      <td>0.309789</td>\n      <td>0.062862</td>\n      <td>0.422144</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>6.954056</td>\n      <td>6.880905</td>\n      <td>1.075272</td>\n      <td>0.073152</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>100.000237</td>\n      <td>79.008804</td>\n      <td>1.886015</td>\n      <td>-0.080568</td>\n      <td>0.268567</td>\n      <td>0.037546</td>\n      <td>0.365491</td>\n      <td>0.098157</td>\n      <td>0.492871</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.118558</td>\n      <td>0.029883</td>\n      <td>1.265685</td>\n      <td>0.088675</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>100.000237</td>\n      <td>83.006271</td>\n      <td>1.107394</td>\n      <td>-0.298662</td>\n      <td>0.219369</td>\n      <td>0.441657</td>\n      <td>0.458826</td>\n      <td>0.067987</td>\n      <td>1.057843</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.553665</td>\n      <td>0.464028</td>\n      <td>1.204731</td>\n      <td>0.089637</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>110.998192</td>\n      <td>179.998474</td>\n      <td>0.894530</td>\n      <td>-0.789025</td>\n      <td>0.498756</td>\n      <td>0.010624</td>\n      <td>0.216169</td>\n      <td>0.059782</td>\n      <td>0.159805</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>65.100052</td>\n      <td>65.003410</td>\n      <td>0.616662</td>\n      <td>0.096642</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>100.000237</td>\n      <td>62.024445</td>\n      <td>0.214663</td>\n      <td>-0.574230</td>\n      <td>0.423929</td>\n      <td>0.022183</td>\n      <td>0.388191</td>\n      <td>0.087424</td>\n      <td>0.768031</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.111786</td>\n      <td>0.008705</td>\n      <td>1.612271</td>\n      <td>0.103081</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>115.996803</td>\n      <td>91.001205</td>\n      <td>0.809019</td>\n      <td>-0.214292</td>\n      <td>0.486074</td>\n      <td>0.305426</td>\n      <td>0.465927</td>\n      <td>0.075435</td>\n      <td>0.799731</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.440553</td>\n      <td>0.336961</td>\n      <td>1.274673</td>\n      <td>0.103592</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>101.000053</td>\n      <td>124.999176</td>\n      <td>0.079422</td>\n      <td>-0.249436</td>\n      <td>0.191908</td>\n      <td>0.210460</td>\n      <td>0.278809</td>\n      <td>0.081613</td>\n      <td>0.361983</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>22.788265</td>\n      <td>22.674259</td>\n      <td>0.808006</td>\n      <td>0.114006</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>101.000053</td>\n      <td>160.030640</td>\n      <td>0.441473</td>\n      <td>-0.495600</td>\n      <td>0.282729</td>\n      <td>0.419668</td>\n      <td>0.053532</td>\n      <td>0.067258</td>\n      <td>0.938086</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>0.123036</td>\n      <td>-0.001884</td>\n      <td>0.631130</td>\n      <td>0.124920</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>111.999382</td>\n      <td>157.027664</td>\n      <td>0.162257</td>\n      <td>-0.346176</td>\n      <td>0.005966</td>\n      <td>0.096042</td>\n      <td>0.340065</td>\n      <td>0.040101</td>\n      <td>0.373818</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>0.572855</td>\n      <td>0.432261</td>\n      <td>0.713246</td>\n      <td>0.140594</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100.000237</td>\n      <td>90.006714</td>\n      <td>1.781204</td>\n      <td>-0.265676</td>\n      <td>0.151816</td>\n      <td>0.362019</td>\n      <td>0.332520</td>\n      <td>0.010186</td>\n      <td>0.874121</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>5.352197</td>\n      <td>5.499053</td>\n      <td>1.111031</td>\n      <td>0.146855</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>91.997589</td>\n      <td>165.022598</td>\n      <td>1.689353</td>\n      <td>-0.744619</td>\n      <td>0.207761</td>\n      <td>0.196682</td>\n      <td>0.179774</td>\n      <td>0.091636</td>\n      <td>0.966828</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>66.905067</td>\n      <td>66.739990</td>\n      <td>0.557485</td>\n      <td>0.165077</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>86.998978</td>\n      <td>173.017532</td>\n      <td>1.793037</td>\n      <td>-0.898135</td>\n      <td>0.433934</td>\n      <td>0.209367</td>\n      <td>0.486851</td>\n      <td>0.027186</td>\n      <td>0.135572</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>0.165599</td>\n      <td>-0.001884</td>\n      <td>0.502833</td>\n      <td>0.167483</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>100.000237</td>\n      <td>83.006271</td>\n      <td>1.073231</td>\n      <td>-0.038701</td>\n      <td>0.463104</td>\n      <td>0.392042</td>\n      <td>0.085235</td>\n      <td>0.053101</td>\n      <td>0.103731</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.208346</td>\n      <td>0.040472</td>\n      <td>1.204731</td>\n      <td>0.167874</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>100.000237</td>\n      <td>112.000099</td>\n      <td>1.618351</td>\n      <td>-0.427248</td>\n      <td>0.311758</td>\n      <td>0.265387</td>\n      <td>0.082825</td>\n      <td>0.031588</td>\n      <td>0.560076</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>11.251267</td>\n      <td>11.079407</td>\n      <td>0.892858</td>\n      <td>0.171861</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>100.000237</td>\n      <td>57.032486</td>\n      <td>0.881358</td>\n      <td>-0.379417</td>\n      <td>0.128283</td>\n      <td>0.063763</td>\n      <td>0.155870</td>\n      <td>0.052979</td>\n      <td>0.476809</td>\n      <td>-0.000003</td>\n      <td>1.000003</td>\n      <td>0.173993</td>\n      <td>-0.001884</td>\n      <td>1.753391</td>\n      <td>0.175877</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>100.000237</td>\n      <td>150.007721</td>\n      <td>1.003907</td>\n      <td>-0.005206</td>\n      <td>0.144630</td>\n      <td>0.167716</td>\n      <td>0.423445</td>\n      <td>0.069522</td>\n      <td>0.455535</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>0.328752</td>\n      <td>0.104005</td>\n      <td>0.666634</td>\n      <td>0.224747</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>100.000237</td>\n      <td>98.996140</td>\n      <td>1.843753</td>\n      <td>-0.362289</td>\n      <td>0.378694</td>\n      <td>0.173918</td>\n      <td>0.479750</td>\n      <td>0.090164</td>\n      <td>0.920052</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>4.512918</td>\n      <td>4.286623</td>\n      <td>1.010143</td>\n      <td>0.226295</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100.000237</td>\n      <td>64.013428</td>\n      <td>0.520082</td>\n      <td>-0.871746</td>\n      <td>0.435766</td>\n      <td>0.169760</td>\n      <td>0.252438</td>\n      <td>0.041319</td>\n      <td>0.708434</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>34.977501</td>\n      <td>34.719139</td>\n      <td>1.562176</td>\n      <td>0.258362</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>89.995193</td>\n      <td>54.029510</td>\n      <td>0.506840</td>\n      <td>-0.728633</td>\n      <td>0.369394</td>\n      <td>0.112181</td>\n      <td>0.247809</td>\n      <td>0.053009</td>\n      <td>0.530735</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>34.839134</td>\n      <td>34.560307</td>\n      <td>1.665667</td>\n      <td>0.278828</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>119.000381</td>\n      <td>118.998100</td>\n      <td>0.743793</td>\n      <td>-0.631957</td>\n      <td>0.020763</td>\n      <td>0.349615</td>\n      <td>0.302988</td>\n      <td>0.066769</td>\n      <td>0.478711</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>4.502014</td>\n      <td>4.794890</td>\n      <td>1.000019</td>\n      <td>0.292876</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>100.000237</td>\n      <td>62.024445</td>\n      <td>0.899884</td>\n      <td>-0.772531</td>\n      <td>0.293228</td>\n      <td>0.100975</td>\n      <td>0.173560</td>\n      <td>0.062805</td>\n      <td>0.999514</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>35.585201</td>\n      <td>35.280350</td>\n      <td>1.612271</td>\n      <td>0.304852</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>100.000237</td>\n      <td>101.999115</td>\n      <td>1.182622</td>\n      <td>-0.249436</td>\n      <td>0.069802</td>\n      <td>0.078282</td>\n      <td>0.147880</td>\n      <td>0.025334</td>\n      <td>0.582037</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>2.852960</td>\n      <td>3.164198</td>\n      <td>0.980403</td>\n      <td>0.311238</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>100.000237</td>\n      <td>80.997787</td>\n      <td>0.331308</td>\n      <td>-0.006729</td>\n      <td>0.322010</td>\n      <td>0.444758</td>\n      <td>0.355917</td>\n      <td>0.016986</td>\n      <td>0.419467</td>\n      <td>0.999997</td>\n      <td>0.000003</td>\n      <td>20.427147</td>\n      <td>19.984676</td>\n      <td>1.234605</td>\n      <td>0.442471</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_options.sort_values('Abs Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDJ-hkrSAqVh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MSE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def get_mse(model, X, y, batch_size):\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X, y), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = loss_fn(out, batch_labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1650895213041,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "GG64Xp3vAib-",
    "outputId": "f846ee65-029d-4a76-b006-d65e4ac07a48",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE on the train set is:  5.912004478484701e-05\n",
      "The MSE on the val set is:  6.381503521217241e-05\n",
      "The MSE on the test set is:  6.378291909994323e-05\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The MSE on the train set is: ', get_mse(model, X_train, y_train, batch_size).mean())\n",
    "print('The MSE on the val set is: ', get_mse(model, X_val, y_val, batch_size).mean())\n",
    "print('The MSE on the test set is: ', get_mse(model, X_test, y_test, batch_size).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCLf2KQJAvoE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MAE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1650895213494,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "mZohsEyLAu7S",
    "outputId": "4d6bfba3-5751-478d-d53e-73751702414e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_mae(model, X, y, batch_size):\n",
    "    mae_loss = nn.L1Loss()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X, y), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = mae_loss(out, batch_labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE on the train set is:  0.006119975909581668\n",
      "The MAE on the val set is:  0.006309957772457194\n",
      "The MAE on the test set is:  0.006288769772475206\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The MAE on the train set is: ', get_mae(model, X_train, y_train, batch_size).mean())\n",
    "print('The MAE on the val set is: ', get_mae(model, X_val, y_val, batch_size).mean())\n",
    "print('The MAE on the test set is: ', get_mae(model, X_test, y_test, batch_size).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmsZ4aPaA1SJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### RSME on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the train set is:  0.007685031609541051\n",
      "The RMSE on the val set is:  0.007984763253136126\n",
      "The RMSE on the test set is:  0.007982534404318177\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The RMSE on the train set is: ', np.sqrt(get_mse(model, X_train, y_train, batch_size)).mean())\n",
    "print('The RMSE on the val set is: ', np.sqrt(get_mse(model, X_val, y_val, batch_size)).mean())\n",
    "print('The RMSE on the test set is: ', np.sqrt(get_mse(model, X_test, y_test, batch_size)).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8Jxp0L4A5zb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MAPE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def get_mape(model, X, y, batch_size):\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_labels in DataLoader(OptDataset(X, y), batch_size=batch_size):\n",
    "            out = model(batch.to(device))\n",
    "            loss = MAPELoss(out, batch_labels.to(device)).detach().cpu().item()\n",
    "            losses.append(loss)\n",
    "\n",
    "    return np.array(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAPE on the train set is:  0.03522749650739088\n",
      "The MAPE on the val set is:  0.03732553258248903\n",
      "The MAPE on the test set is:  0.040648341034222574\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('The MAPE on the train set is: ', get_mape(model, X_train, y_train, batch_size).mean())\n",
    "print('The MAPE on the val set is: ', get_mape(model, X_val, y_val, batch_size).mean())\n",
    "print('The MAPE on the test set is: ', get_mape(model, X_test, y_test, batch_size).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLLLqkX6BEle",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650895213820,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "sf8FmLxxBGcu",
    "outputId": "764313a9-78fc-46ca-dd08-9fb8834e4374",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the R^2 score is:  0.9999279146131453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(X_test[0:batch_size].to(device)).squeeze().cpu().detach().numpy()\n",
    "\n",
    "y_true = y_test[0:batch_size].cpu().squeeze().detach().numpy()\n",
    "\n",
    "r2 = r2_score(y_pred=out, y_true=y_true)\n",
    "\n",
    "print('the R^2 score is: ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1650895214553,
     "user": {
      "displayName": "Paolo D'Elia",
      "userId": "06873635760880783531"
     },
     "user_tz": -120
    },
    "id": "awvWu7WxBIHB",
    "outputId": "1b3dccd3-90ef-47d5-9895-267fdf93020c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 648x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.scatter(\n",
    "    y=output_sc.inverse_transform(out.reshape(-1, 1)),\n",
    "    x=output_sc.inverse_transform(y_true.squeeze().reshape(-1, 1))\n",
    ")\n",
    "ax.set_xlabel('Actual Value')\n",
    "ax.set_ylabel('Predicted Value')\n",
    "\n",
    "ax.text(20, 80, f'$R^2$ = {np.round(r2, 6)}', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model-Testing-Heston.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}